{"cells":[{"cell_type":"markdown","metadata":{"id":"av22_gwc_hOd"},"source":["# Setup and install for SVHN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l247Lv_MYR8O"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bxt_WMJPvnIc"},"outputs":[],"source":["%pip install progressbar"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yqquQY0B65AH"},"outputs":[],"source":["%pip install gpflow\n","%pip install plotnine"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EfIU_eNp3Zio"},"outputs":[],"source":["from plotnine import *\n","from plotnine.themes import *"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZmUjYbArAuQT"},"outputs":[],"source":["import tensorflow as tf\n","from scipy.io import loadmat\n","import random\n","import math\n","import tensorflow_probability as tfp"]},{"cell_type":"markdown","metadata":{"id":"PieVKPfHHYQ6"},"source":["_paper_name_ establishes the reusable name of the paper, it represents the directory under data_papers on the google drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BI4p7ZKb0Qz2"},"outputs":[],"source":["paper_name = \"SVHN\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"433z6V3T2rB2"},"outputs":[],"source":["import os, sys\n","import errno\n","\n","# make a directory if it does not exist\n","def make_dir_if_not_exist(used_path):\n","    if not os.path.isdir(used_path):\n","        try:\n","            os.mkdir(used_path)\n","        except OSError as exc:\n","            if exc.errno != errno.EEXIST:\n","                raise exc\n","            else:\n","                raise ValueError(f'{used_path} directoy cannot be created because its parent directory does not exist.')\n","\n","# make directories if they do not exist\n","\n","make_dir_if_not_exist(\"/content/drive/MyDrive/data_papers/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_history/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/gp_collab/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_predictions/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_ccs/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/temp/\")"]},{"cell_type":"markdown","metadata":{"id":"IufmBnPeB50P"},"source":["\n","This will use the [SVHN dataset](http://ufldl.stanford.edu/housenumbers/). This is an  image dataset of over 600,000 digit images in all, and is a harder dataset than MNIST as the numbers appear in the context of natural scene images. SVHN is obtained from house numbers in Google Street View images. \n","\n","* Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu and A. Y. Ng. \"Reading Digits in Natural Images with Unsupervised Feature Learning\". NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011.\n","\n","[Here](https://github.com/aditya9211/SVHN-CNN/blob/master/svhn_model.ipynb) is a github project that uses the same data.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0yCKQpti4HWJ"},"outputs":[],"source":["data_location = '/content/drive/MyDrive/mlpapers/GP_Collab_SVHN'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M7--mKqbEhZZ"},"outputs":[],"source":["# Run this cell to load the dataset\n","train = loadmat(f'{data_location}/train_32x32.mat')\n","test = loadmat(f'{data_location}/test_32x32.mat')\n","extra = loadmat(f'{data_location}/extra_32x32.mat')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OBiOGiseZmQc"},"outputs":[],"source":["# we have  png files with house numbers for the extra as well, in case of other ideas\n","# https://stackoverflow.com/questions/15612373/convert-image-png-to-matrix-and-then-to-1d-array\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OfOH52uQ0myu"},"outputs":[],"source":["# Set up the imports\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.models import Sequential\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from tensorflow.keras import regularizers\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Flatten, Dense, Conv2D, MaxPooling2D, BatchNormalization, Dropout, Concatenate, Add, Activation\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","import pandas as pd\n","import numpy as np\n","\n","import site\n","import os\n","import tensorflow as tf\n","import pandas as pd\n","import h5py as h5\n","import matplotlib.pyplot as plt\n","import errno\n","import numpy as np\n","import itertools\n","import multiprocessing\n","import json\n","import datetime\n","import random\n","from collections import defaultdict\n","from sklearn.model_selection import train_test_split\n","\n","from tensorflow.keras.models import Model, Sequential, load_model\n","from tensorflow.keras.layers import Bidirectional\n","from tensorflow.keras.layers import  Dense, Flatten, Activation, Dropout, Embedding, Conv1D, Conv2D, MaxPooling2D, MaxPooling1D, Concatenate, BatchNormalization, GaussianNoise, AveragePooling2D\n","from tensorflow.keras.layers import LSTM, TimeDistributed, Permute, Reshape, Lambda, RepeatVector, Input, Multiply, SimpleRNN, GRU, LeakyReLU, Add\n","from tensorflow.keras.regularizers import L2\n","from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.summary import create_file_writer\n","\n","pd.set_option('display.width', 400)\n","pd.set_option('display.max_columns', 40)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YJ0GxmKnj56g"},"outputs":[],"source":["print(np.unique(test['y'][:12000])) # just checking that there is no order to the test data\n","print(train['X'].shape)\n","print(test['X'].shape)\n","print(extra['X'].shape)\n","print(extra['y'].shape)"]},{"cell_type":"markdown","metadata":{"id":"s1B6X2mB0myt"},"source":["## Inspect and Preprocess the dataset\n","* Extract the training and testing images and labels separately from the train and test dictionaries loaded for you.\n","* turn test into validation and slice some extra for test\n","* Select a random sample of images and corresponding labels from the dataset (at least 10), and display them in a figure.\n","* Select a random sample of the images and corresponding labels from the dataset (at least 10), and display them in a figure.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VXyZZbP_0myu"},"outputs":[],"source":["train_data = train['X']\n","validation_data = test['X']\n","test_data = extra['X'][:,:,:,:30000]\n","train_targets = train['y']\n","validation_targets = test['y']\n","test_targets = extra['y'][:30000]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YXfIEqzV0myv"},"outputs":[],"source":["def plot_images(images, nrows, ncols, cls_true, cls_pred=None):\n","    fig, axes = plt.subplots(nrows, ncols, figsize=(16, 2*nrows))\n","    for i, ax in enumerate(axes.flat): \n","        # Pretty string with actual label\n","        true_number = ''.join(str(x) for x in cls_true[i] if x != 10)\n","        title = \"Label: {0}\".format(true_number)\n","            \n","        ax.imshow(images[:,:,:,i])\n","        ax.set_title(title)   \n","        ax.set_xticks([]); ax.set_yticks([])\n","\n","def plot_sample(num_sample=10):\n","    idxs = sorted(random.sample(range(train_targets.shape[0]),num_sample))\n","    plot_images(train_data[:,:,:,idxs],2,math.ceil(num_sample/2),train_targets[idxs])    \n","    print(idxs)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QfkGWAj8D8-f"},"outputs":[],"source":["plot_sample(10)"]},{"cell_type":"markdown","metadata":{"id":"JtZNyab1GfBr"},"source":["Convert the train, test and validation data to greyscale"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0tE_Uv1R0myw"},"outputs":[],"source":["# train_data_grey = train_data.mean(axis=2)\n","# train_data_grey = np.expand_dims(train_data_grey,axis=2)\n","# train_data_grey = np.moveaxis(train_data_grey, 3, 0)\n","\n","train_data = np.moveaxis(train_data, 3, 0)\n","test_data = np.moveaxis(test_data, 3, 0)\n","validation_data = np.moveaxis(validation_data, 3, 0)\n","\n","print(train_data.shape)\n","print(validation_data.shape)\n","print(test_data.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"5x_7ZufGGoAa"},"source":["Set the 10th class to zero (represents '0' digit)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fdN10zRS0myx"},"outputs":[],"source":["train_targets[train_targets == 10] = 0\n","validation_targets[validation_targets == 10] = 0\n","test_targets[test_targets == 10] = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SA4FUPc8ke1I"},"outputs":[],"source":["# extracts the layer from a model using the name\n","def get_layer_by_name(layers, name, return_first=True):\n","    matching_named_layers = [l for l in layers if l.name == name]\n","    if not matching_named_layers:\n","        return None\n","    return matching_named_layers[0] if return_first else matching_named_layers\n"]},{"cell_type":"markdown","metadata":{"id":"LM8K14_1HBws"},"source":["Let us do a dumb sequential and see how far this can go  => 88% validation is the best.. + 85% test (depends on initialization!)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6xqL0RbypQUF"},"outputs":[],"source":["# plotting utilities for the history of the fit\n","def plot_history(history):\n","    acc_keys = [k for k in history.history.keys() if 'accuracy' in k]\n","    loss_keys = [k for k in history.history.keys() if not k in acc_keys]\n","    for k, v in history.history.items():\n","        if k in acc_keys:\n","            plt.figure(1)\n","            plt.plot(v)\n","        else:\n","            plt.figure(2)\n","            plt.plot(v)\n","    plt.figure(1)\n","    plt.title('Accuracy vs. epochs')\n","    plt.ylabel('Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.legend(acc_keys, loc='lower right')\n","    plt.figure(2)\n","    plt.title('Loss vs. epochs')\n","    plt.ylabel('Loss')\n","    plt.xlabel('Epoch')\n","    plt.legend(loss_keys, loc='upper right')\n","    plt.show()\n","\n","def plot_history_df(history):\n","    acc_keys = [k for k in history.columns.values if 'accuracy' in k]\n","    loss_keys = [k for k in history.columns.values if not k in acc_keys and not k in ['epoch']]\n","    for k, v in history.items():\n","        if k in acc_keys:\n","            plt.figure(1)\n","            plt.plot(v)\n","        if k in loss_keys:\n","            plt.figure(2)\n","            plt.plot(v)\n","    plt.figure(1)\n","    plt.title('Accuracy vs. epochs')\n","    plt.ylabel('Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.legend(acc_keys, loc='lower right')\n","    plt.figure(2)\n","    plt.title('Loss vs. epochs')\n","    plt.ylabel('Loss')\n","    plt.xlabel('Epoch')\n","    plt.legend(loss_keys, loc='upper right')\n","    plt.show()   \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0juaJ1ELRdSA"},"outputs":[],"source":["# # let's do a dumb sequential and see how far this can go  => 88% validation is the best.. + 85% test ?!\n","\n","def get_model_seq(input_shape):\n","    model = Sequential([\n","                Flatten(name='F1',input_shape=input_shape),\n","                Dense(128, activation='relu', \n","                      name = 'D1'),\n","                Dense(128, activation='relu', name = 'D2'),\n","                Dense(48, activation='relu', kernel_regularizer=tf.keras.regularizers.l1(0.005), name = 'D4R'),                \n","                Dense(10,  activation='softmax', name = 'SFTMX1')\n","    ])    \n","    return model\n","\n","# model_seq = get_model_seq(train_data[0,:,:,:].shape)\n","\n","# model_seq.compile(optimizer=tf.keras.optimizers.Adam(),\n","#               loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n","#               metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n","\n","# callbacks_seq = [ EarlyStopping(monitor='val_sparse_categorical_accuracy',\n","#                            mode='max',\n","#                            patience=60)\n","#                 ]\n","\n","# history_seq = model_seq.fit(  train_data, \n","#                       train_targets,\n","#                       epochs=500, \n","#                       validation_data=(validation_data, validation_targets),\n","#                       callbacks=callbacks_seq,\n","#                       batch_size=512)\n","\n","# plot_history(history_seq)\n","# model_seq.evaluate(test_data,test_targets)\n","\n","# model_seq.summary()"]},{"cell_type":"markdown","metadata":{"id":"oNo_peRzfXNP"},"source":["A function that will take a model construction function (with _model_name_ and _input_shape_ arguments), train and fit it using the supplied data and callbacks. The _kwargs_ are supplied to the model fit function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P_dqIA0noEoo"},"outputs":[],"source":["from tensorflow.keras.callbacks import CSVLogger\n","import datetime\n","\n","def compile_and_fit_model_basic(  model_func,\n","                                  model_name,\n","                                  input_shape,\n","                                  X_train,\n","                                  Y_train,\n","                                  save_max_epoch=True,\n","                                  save_final=False,\n","                                  patience_count = None,\n","                                  early_stopping_obs = 'val_sparse_categorical_accuracy',\n","                                  log_history = True,\n","                                  verbose_level = 0,\n","                                  **kwargs):\n","    m = None\n","    if isinstance(model_func, tf.keras.models.Model):\n","        m = model_func\n","        m._name = model_name\n","    else:\n","        m = model_func(model_name, input_shape)\n","      \n","    if 'validation_data' not in kwargs.keys() and 'val_' in early_stopping_obs:\n","        early_stopping_obs = early_stopping_obs.replace('val_','')\n","\n","    callbacks_used = []\n","    if save_max_epoch:\n","        callbacks_used.append(ModelCheckpoint(f'/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/{m.name}' + '_model_{epoch:03d}_{accuracy:0.3f}',\n","                                              save_weights_only=False,\n","                                              monitor = early_stopping_obs,\n","                                              mode='max',\n","                                              save_best_only=True))\n","    if patience_count is not None:\n","        callbacks_used.append(tf.keras.callbacks.EarlyStopping(monitor=early_stopping_obs, patience=patience_count))\n","\n","    if log_history:\n","        callbacks_used.append(tf.keras.callbacks.CSVLogger(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_history/history_log_{model_name}_{datetime.date.today().strftime('%Y%m%d')}.csv\", append=True))\n","\n","    m.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), \n","              optimizer=tf.keras.optimizers.Adam(), \n","              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n","    \n","    history = m.fit(X_train, Y_train, callbacks=callbacks_used, verbose=verbose_level, **kwargs)\n","    if save_final:\n","        make_dir_if_not_exist(model_name)\n","        m.save(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{m.name}_saved_model_after_fit\")  # Save the model\n","    return (m, history)"]},{"cell_type":"markdown","metadata":{"id":"J212e-sSgQEs"},"source":["A function that given a model or model directory create a new model up to the _layer_name_, then write the features matching the supplied _X_ and _Y_ as numpy arrays to google drive."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DxMillEBxs5H"},"outputs":[],"source":["def write_features_from_models(\n","        model_entry,\n","        layer_name,\n","        X_input, Y_input,\n","        reverse_one_hot=False,\n","        normalize_X_func=None,\n","        dataset_id = \"NA\",\n","        **kwargs):\n","  \n","    Y_new = None\n","    X_new = None\n","    if reverse_one_hot:\n","        Y_new = np.apply_along_axis(np.argmax, 1, Y_input) + 1\n","    else:\n","        Y_new = Y_input.copy()\n","\n","    model_here = None\n","    if isinstance(model_entry, tf.keras.models.Model):\n","        model_here = model_entry\n","        model_file_name = model_here.name\n","    else:\n","        model_here = tf.keras.models.load_model(model_entry,**kwargs) \n","\n","    features_model = Model(model_here.input,\n","                            get_layer_by_name(model_here.layers, layer_name).output)\n","    if normalize_X_func is None:\n","        X_new = np.array(features_model.predict(X_input), dtype='float64')\n","    else:\n","        X_new = np.array(normalize_X_func(features_model.predict(X_input)), dtype='float64')\n","\n","    np.save(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features/{model_here.name}_features_{layer_name}_{dataset_id}_X\", X_new, \n","               allow_pickle=True, \n","               fix_imports=True)\n","    np.save(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features/{model_here.name}_features_{layer_name}_{dataset_id}_Y\", Y_new, \n","               allow_pickle=True, \n","               fix_imports=True)"]},{"cell_type":"markdown","metadata":{"id":"t2O5XMTr2s5m"},"source":["Some functions to get scores on the results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bhPJFtuzujdC"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n","import re\n","\n","def get_confusion_matrix_classification(model, X, Y_true):\n","    y_pred = model.predict(X)\n","    y_true = np.apply_along_axis(np.argmax, 1, Y_true)\n","    y_pred = np.apply_along_axis(np.argmax, 1, y_pred)\n","    return (confusion_matrix(y_true, y_pred), y_pred, y_true)\n","\n","def construct_confusion_matrix(X, Y_true, Y_pred):\n","    y_true = Y_true\n","    y_pred = np.apply_along_axis(np.argmax, 1, Y_pred)\n","    return (confusion_matrix(y_true, y_pred), y_pred, y_true)\n","\n","def pr_rc_f1_acc_from_supplied(y_pred, y_true):  \n","    pr, rc, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")   \n","    acc = accuracy_score(y_true, y_pred)\n","    return pr, rc, f1, acc\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mVz_etHKO89T"},"outputs":[],"source":["import re\n","import os\n","\n","def dir_has_file_with_regex(dir_name, regex_string):\n","  filenames = [ f\"{dir_name}/{dir_entry.name}\" for dir_entry in os.scandir(dir_name) if os.path.isfile(f\"{dir_name}/{dir_entry.name}\") ]   \n","  filenames = [ fn for fn in filenames if re.match(regex_string, fn, re.IGNORECASE) ]\n","  return filenames\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PZU3TT6VZwLU"},"outputs":[],"source":["# dir_has_file_with_regex(\"/content/drive/MyDrive/data_papers/gpSVHN/model_features\", \"^.*DNN_A_.*D3R_Test_X.*\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IiW-RCSEtkeR"},"outputs":[],"source":["!nvidia-smi -L"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rLjgQs2buLYd"},"outputs":[],"source":["!pip install ipython-autotime"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FY393TrFuNod"},"outputs":[],"source":["%load_ext autotime"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T1RHIyWyuxrh"},"outputs":[],"source":["import timeit"]},{"cell_type":"markdown","metadata":{"id":"UrA1SY4CHTOS"},"source":["# A basic DNN to fit SVHN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GHdJJrIbn3DR"},"outputs":[],"source":["def basic_DNNTemplate_A(model_name, inshape, num_classes = 10):\n","\n","  base_input = Input(shape=inshape, name='base_input')\n","  f1_output = Flatten(name='F1')(base_input)\n","  d1_output = Dense(128, activation='relu', name = 'D1')(f1_output)\n","  d2_output = Dense(128, activation='relu', name = 'D2')(d1_output)\n","  d3r_output = Dense(48, activation='relu', kernel_regularizer=tf.keras.regularizers.l1(0.005), name = 'D3R')(d2_output)\n","  model_output = Dense(10,  activation='softmax', name = 'SFTMX1')(d3r_output)\n","  model = Model(base_input, model_output, name = model_name)\n","\n","  return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d7Q4E6VW3QaW"},"outputs":[],"source":["# # saving 100 DNNs\n","for model_count in [i+1 for i in range(100)]:\n","  start_time = timeit.default_timer()\n","  m1, h1 = compile_and_fit_model_basic( basic_DNNTemplate_A,  \n","                    f\"DNN_A_{str(model_count)}_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","                    train_data[0,:,:,:].shape, \n","                    train_data,\n","                    train_targets,\n","                    save_max_epoch=False,\n","                    save_final=True,\n","                    patience_count = 35,\n","                    early_stopping_obs = 'val_sparse_categorical_accuracy',\n","                    log_history = True,                             \n","                    batch_size=512, \n","                    epochs=250, \n","                    class_weight=None, \n","                    validation_data=(validation_data, validation_targets))\n","  print(timeit.default_timer()-start_time)\n","# # /content/drive/MyDrive/data_papers/gpSVHN/model_finals/DNN_A_6_20210923210545_saved_model_after_fit/assets\n","# plot_history(h1)"]},{"cell_type":"code","source":["paper_name"],"metadata":{"id":"MXYn0_5beT61"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8tP4KtdkyPrM"},"outputs":[],"source":["# saving the features of 100 DNNs for the train data\n","\n","check_model_string = \"DNN_A_\"\n","not_check_model_string = [\"arallel\",\"Collab_\"]\n","for dir_entry in os.scandir(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/\"):\n","  if os.path.isdir(dir_entry):\n","    if check_model_string in str(dir_entry) and all([(not ncs in dir_entry.name) for ncs in not_check_model_string]):\n","      # f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features/{model_here.name}_features_{layer_name}_{dataset_id}_X\"\n","      # f\"DNN_A_{str(model_count)}_{datetime.datetime.now():%Y%m%d%H%M%S}\"\n","      if not dir_has_file_with_regex(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features\",f\".*{dir_entry.name.replace('_saved_model_after_fit','')}.*_features_D3R_Train.*$\"):\n","        print(dir_entry.name)\n","        write_features_from_models(\n","          f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{dir_entry.name}\",\n","          \"D3R\",\n","          train_data, train_targets,\n","          reverse_one_hot=False,\n","          normalize_X_func=None,\n","          dataset_id=\"Train\")\n","      if not dir_has_file_with_regex(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features\",f\".*{dir_entry.name.replace('_saved_model_after_fit','')}.*_features_SFTMX1_Train.*$\"):\n","        print(dir_entry.name)\n","        write_features_from_models(\n","          f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{dir_entry.name}\",\n","          \"SFTMX1\",\n","          train_data, train_targets,\n","          reverse_one_hot=False,\n","          normalize_X_func=None,\n","          dataset_id=\"Train\")      "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r_HsBg4cIfIa"},"outputs":[],"source":["# saving the features of 100 DNNs for the validation data\n","check_model_string = \"DNN_A_\"\n","not_check_model_string = [\"arallel\",\"Collab_\"]\n","for dir_entry in os.scandir(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/\"):\n","  if os.path.isdir(dir_entry):\n","    if check_model_string in str(dir_entry) and all([(not ncs in dir_entry.name) for ncs in not_check_model_string]):\n","      # f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features/{model_here.name}_features_{layer_name}_{dataset_id}_X\"\n","      if not dir_has_file_with_regex(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features\",f\".*{dir_entry.name.replace('_saved_model_after_fit','')}.*_features_D3R_Validation.*$\"):\n","        print(dir_entry.name)\n","        write_features_from_models(\n","          f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{dir_entry.name}\",\n","          \"D3R\",\n","          validation_data, validation_targets,\n","          reverse_one_hot=False,\n","          normalize_X_func=None,\n","          dataset_id=\"Validation\")\n","      if not dir_has_file_with_regex(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features\",f\".*{dir_entry.name.replace('_saved_model_after_fit','')}.*_features_SFTMX1_Validation.*$\"):\n","        print(dir_entry.name)\n","        write_features_from_models(\n","          f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{dir_entry.name}\",\n","          \"SFTMX1\",\n","          validation_data, validation_targets,\n","          reverse_one_hot=False,\n","          normalize_X_func=None,\n","          dataset_id=\"Validation\")      "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t_NjgPWEIqXy"},"outputs":[],"source":["# saving the features of 100 DNNs for the test data\n","check_model_string = \"DNN_A_\"\n","not_check_model_string = [\"arallel\",\"Collab_\"]\n","for dir_entry in os.scandir(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/\"):\n","  if os.path.isdir(dir_entry):\n","    if check_model_string in str(dir_entry) and all([(not ncs in dir_entry.name) for ncs in not_check_model_string]):\n","      if not dir_has_file_with_regex(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features\",f\".*{dir_entry.name}.*_features_D3R_Test.*$\"):\n","        print(dir_entry.name)\n","        write_features_from_models(\n","          f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{dir_entry.name}\",\n","          \"D3R\",\n","          test_data, test_targets,\n","          reverse_one_hot=False,\n","          normalize_X_func=None,\n","          dataset_id=\"Test\")\n","      if not dir_has_file_with_regex(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features\",f\".*{dir_entry.name}.*_features_SFTMX1_Test.*$\"):\n","        print(dir_entry.name)\n","        write_features_from_models(\n","          f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{dir_entry.name}\",\n","          \"SFTMX1\",\n","          test_data, test_targets,\n","          reverse_one_hot=False,\n","          normalize_X_func=None,\n","          dataset_id=\"Test\")      "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gnFbW-5mqeVl"},"outputs":[],"source":["# getting the scores for the individual 100 DNNs on the test data set\n","scores_dnn_simple = []\n","check_model_string = \"DNN_A_\"\n","not_check_model_string = [\"arallel\",\"Collab_\"]\n","for dir_entry in os.scandir(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/\"):\n","  if os.path.isdir(dir_entry):\n","    if check_model_string in str(dir_entry) and all([ (not ncs in dir_entry.name) for ncs in not_check_model_string]):\n","      print(dir_entry.name)\n","      model_here = tf.keras.models.load_model(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{dir_entry.name}\")  \n","      y_predict_here = np.array(model_here.predict(test_data), dtype='float64')\n","      y_predict_here = np.apply_along_axis(np.argmax, 1, y_predict_here)\n","      scores_dnn_simple.append(pr_rc_f1_acc_from_supplied(y_predict_here, test_targets))\n","\n","np.save(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/individual_dnn_summary_{datetime.datetime.now():%Y%m%d%H%M%S}\", np.array(scores_dnn_simple), \n","               allow_pickle=True, \n","               fix_imports=True)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NUnTfSfY2pZK"},"source":["# A basic CNN to fit SVHN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UjivSGDyT0xA"},"outputs":[],"source":["def basic_CNNTemplate_A(model_name, inshape, num_classes = 10):\n","  # Input Layer\n","  base_input = Input(shape=inshape, name='base_input')\n","  # Convolutional Layer #1\n","  c1_output = Conv2D(filters=32,kernel_size=[5, 5],padding=\"same\",activation=\"relu\", name=\"C1\")(base_input)\n","\n","  # Pooling Layer #1\n","  mxp1_output = MaxPooling2D(pool_size=[2, 2], strides=2, name=\"MXP1\")(c1_output)\n","  c2_output = Conv2D(filters=64,kernel_size=[5, 5],padding=\"same\",activation=\"relu\",name=\"C2\")(mxp1_output)\n","    \n","  #with tf.name_scope('Pool2 Layer'):\n","  mxp2_output = MaxPooling2D(pool_size=[2, 2], strides=2, name=\"MXP2\")(c2_output)\n","  f1_output = Flatten(name='F1')(mxp2_output)\n","\n","  # Dense Layer\n","  d1_output = Dense(units=256, activation=\"relu\", name=\"D1\")(f1_output)\n","  drp1_output = Dropout(rate=0.5, name=\"DRP1\")(d1_output)\n","\n","  model_output = Dense(num_classes,  activation='softmax', name = 'SFTMX1')(drp1_output)\n","  model = Model(base_input, model_output, name = model_name)\n","  return model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GqC4AbkNH2ne"},"outputs":[],"source":["# saving 100 CNNs\n","for model_count in [i+1 for i in range(100)]:\n","  start_time = timeit.default_timer()\n","  m1, h1 = compile_and_fit_model_basic( basic_CNNTemplate_A,  \n","                    f\"CNN_A_{str(model_count)}_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","                    train_data[0,:,:,:].shape, \n","                    train_data, train_targets,\n","                    save_max_epoch=False,\n","                    save_final=True,\n","                    patience_count = 35,\n","                    early_stopping_obs = 'val_sparse_categorical_accuracy',\n","                    log_history = True,                             \n","                    batch_size=512, \n","                    epochs=250, \n","                    class_weight=None, \n","                    validation_data=(validation_data, validation_targets))\n","  print(timeit.default_timer()-start_time)\n","\n","# # /content/drive/MyDrive/data_papers/gpSVHN/model_finals/DNN_A_6_20210923210545_saved_model_after_fit/assets\n","# plot_history(h1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0rhK968KxRy3"},"outputs":[],"source":["# # saving the features of 100 CNNs for the training data\n","\n","check_model_string = \"CNN_A_\"\n","not_check_model_string = [\"arallel\",\"Collab_\"]\n","for dir_entry in os.scandir(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/\"):\n","  if os.path.isdir(dir_entry):\n","    if check_model_string in str(dir_entry) and all([ (not ncs in dir_entry.name) for ncs in not_check_model_string]):\n","      if not dir_has_file_with_regex(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features\",f\".*{dir_entry.name.replace('_saved_model_after_fit','')}.*_features_DRP1_Train.*$\"):\n","        print(dir_entry.name)\n","        write_features_from_models(\n","          f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{dir_entry.name}\",\n","          \"DRP1\",\n","          train_data, train_targets,\n","          reverse_one_hot=False,\n","          normalize_X_func=None,\n","          dataset_id=\"Train\")\n","      if not dir_has_file_with_regex(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features\",f\".*{dir_entry.name.replace('_saved_model_after_fit','')}.*_features_SFTMX1_Train.*$\"):\n","        print(dir_entry.name)\n","        write_features_from_models(\n","          f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{dir_entry.name}\",\n","          \"SFTMX1\",\n","          train_data, train_targets,\n","          reverse_one_hot=False,\n","          normalize_X_func=None,\n","          dataset_id=\"Train\")      "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kk8HZeqOI5IZ"},"outputs":[],"source":["# # saving the features of 100 CNNs for the Validation data\n","\n","# # /content/drive/MyDrive/data_papers/SVHN/model_finals/CNN_A_\n","check_model_string = \"CNN_A_\"\n","not_check_model_string = [\"arallel\",\"Collab_\"]\n","for dir_entry in os.scandir(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/\"):\n","  if os.path.isdir(dir_entry):\n","    if check_model_string in str(dir_entry) and all([ (not ncs in dir_entry.name) for ncs in not_check_model_string]):\n","      if not dir_has_file_with_regex(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features\",f\".*{dir_entry.name.replace('_saved_model_after_fit','')}.*_features_DRP1_Validation.*$\"):\n","        print(dir_entry.name)\n","        write_features_from_models(\n","          f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{dir_entry.name}\",\n","          \"DRP1\",\n","          validation_data, validation_targets,\n","          reverse_one_hot=False,\n","          normalize_X_func=None,\n","          dataset_id=\"Validation\")\n","      if not dir_has_file_with_regex(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features\",f\".*{dir_entry.name.replace('_saved_model_after_fit','')}.*_features_SFTMX1_Validation.*$\"):\n","        print(dir_entry.name)\n","        write_features_from_models(\n","          f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{dir_entry.name}\",\n","          \"SFTMX1\",\n","          validation_data, validation_targets,\n","          reverse_one_hot=False,\n","          normalize_X_func=None,\n","          dataset_id=\"Validation\")      "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XIjoa4OnJAih"},"outputs":[],"source":["# # saving the features of 100 CNNs for the Test data\n","\n","# # /content/drive/MyDrive/data_papers/SVHN/model_finals/CNN_A_\n","check_model_string = \"CNN_A_\"\n","not_check_model_string = [\"arallel\",\"Collab_\"]\n","for dir_entry in os.scandir(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/\"):\n","  if os.path.isdir(dir_entry):\n","    if check_model_string in str(dir_entry) and all([ (not ncs in dir_entry.name) for ncs in not_check_model_string]):\n","      if not dir_has_file_with_regex(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features\",f\".*{dir_entry.name.replace('_saved_model_after_fit','')}.*_features_DRP1_Test.*$\"):\n","        print(dir_entry.name)\n","        write_features_from_models(\n","          f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{dir_entry.name}\",\n","          \"DRP1\",\n","          test_data, test_targets,\n","          reverse_one_hot=False,\n","          normalize_X_func=None,\n","          dataset_id=\"Test\")\n","      if not dir_has_file_with_regex(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features\",f\".*{dir_entry.name.replace('_saved_model_after_fit','')}.*_features_SFTMX1_Test.*$\"):\n","        print(dir_entry.name)\n","        write_features_from_models(\n","          f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{dir_entry.name}\",\n","          \"SFTMX1\",\n","          test_data, test_targets,\n","          reverse_one_hot=False,\n","          normalize_X_func=None,\n","          dataset_id=\"Test\")      "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xZsCz3bYySRB"},"outputs":[],"source":["# getting the scores for the individual 100 CNNs on the test data set\n","scores_cnn_simple = []\n","check_model_string = \"CNN_A_\"\n","not_check_model_string = [\"arallel\",\"Collab_\"]\n","for dir_entry in os.scandir(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/\"):\n","  if os.path.isdir(dir_entry):\n","    if check_model_string in str(dir_entry) and all([ (not ncs in dir_entry.name) for ncs in not_check_model_string]):\n","      print(dir_entry.name)\n","      model_here = tf.keras.models.load_model(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{dir_entry.name}\")  \n","      y_predict_here = np.array(model_here.predict(test_data), dtype='float64')\n","      y_predict_here = np.apply_along_axis(np.argmax, 1, y_predict_here)\n","      scores_cnn_simple.append(pr_rc_f1_acc_from_supplied(y_predict_here, test_targets))\n","\n","np.save(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/individual_cnn_summary_{datetime.datetime.now():%Y%m%d%H%M%S}\", np.array(scores_cnn_simple), \n","               allow_pickle=True, \n","               fix_imports=True)\n","\n"]},{"cell_type":"code","source":["scores_cnn_simple"],"metadata":{"id":"mFecea2NmYe7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"boCHy50O_Z-m"},"source":["# ResNet50 fit SVHN grey"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zMFmsZHu_s9S"},"outputs":[],"source":["# from tensorflow.keras.applications.vgg16 import VGG16\n","from tensorflow.keras.applications.vgg19 import VGG19\n","from tensorflow.keras.applications import ResNet50\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D, BatchNormalization\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0hG4HIud_Z-n"},"outputs":[],"source":["def basic_ResNet50Template_A(model_name, inshape, num_classes = 10):\n","  model = Sequential(name=model_name)\n","  model.add(ResNet50(include_top=True, pooling='avg', weights=None,input_shape=inshape, classes=num_classes))\n","  model.compile(loss='sparse_categorical_crossentropy',\n","                optimizer='adam',      \n","                metrics=['acc'])\n","  return(model)\n","\n","def basic_VGG19Template_A(model_name, inshape, num_classes = 10):\n","  model = Sequential(name=model_name)\n","  model.add(VGG19(include_top=True, pooling='avg', weights=None,input_shape=inshape, classes=num_classes))\n","  model.compile(loss='sparse_categorical_crossentropy',\n","                optimizer='adam',      \n","                metrics=['acc'])\n","  return(model)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mlIRttQrBd_x"},"outputs":[],"source":["# # saving 100 RestNet50\n","for model_count in [i+1 for i in range(40)]:\n","  m1, h1 = compile_and_fit_model_basic( basic_ResNet50Template_A,  \n","                    f\"RestNet50_A_{str(model_count)}_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","                    train_data[0,:,:,:].shape, \n","                    train_data, train_targets,\n","                    save_max_epoch=False,\n","                    save_final=True,\n","                    patience_count = 35,\n","                    early_stopping_obs = 'val_sparse_categorical_accuracy',\n","                    log_history = True,                             \n","                    batch_size=512, \n","                    epochs=250, \n","                    class_weight=None, \n","                    verbose_level = 1,\n","                    validation_data=(validation_data, validation_targets))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AUtudPCgMrN7"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.utils import plot_model\n","# plot_model(m1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mA5PSzi6NvjP"},"outputs":[],"source":["#  ResNets do NOT allow feature extractions!\n","# model_entry = m1\n","# X_input = train_data_grey.copy()\n","# Y_input = train_targets.copy()\n","# layer_name = \"resnet50\"\n","# m1.input\n","# get_layer_by_name(m1.layers, \"resnet50\").output\n","# Model(m1.input, get_layer_by_name(m1.layers, layer_name).output)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wXQ_hD5xP5yh"},"outputs":[],"source":["# getting the scores for the individual 40 ResNets on the test data set\n","scores_resnets_simple = []\n","check_model_string = \"RestNet50_A\"\n","not_check_model_string = [\"arallel\",\"Collab_\"]\n","for dir_entry in os.scandir(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/\"):\n","  if os.path.isdir(dir_entry):\n","    if check_model_string in str(dir_entry) and all([ (not ncs in dir_entry.name) for ncs in not_check_model_string]):\n","      print(dir_entry.name)\n","      model_here = tf.keras.models.load_model(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{dir_entry.name}\")  \n","      y_predict_here = np.array(model_here.predict(test_data), dtype='float64')\n","      y_predict_here = np.apply_along_axis(np.argmax, 1, y_predict_here)\n","      scores_resnets_simple.append(pr_rc_f1_acc_from_supplied(y_predict_here, test_targets))\n","\n","np.save(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/individual_resnets_summary_{datetime.datetime.now():%Y%m%d%H%M%S}\", np.array(scores_resnets_simple), \n","               allow_pickle=True, \n","               fix_imports=True)"]},{"cell_type":"markdown","metadata":{"id":"WS7USmAlAZ_Q"},"source":["# WideResNet fit SVHN grey"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FkC-qxwi5U1f"},"outputs":[],"source":["import uuid\n","# uuid.uuid4()\n","# str(uuid.uuid4()).split(\"-\")[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kSq59Zr_AZBE"},"outputs":[],"source":["# https://github.com/asmith26/wide_resnets_keras\n","\n","# Wide residual network http://arxiv.org/abs/1605.07146\n","def _wide_basic(n_input_plane, n_output_plane, stride, identifier, \n","                channel_axis = -1,\n","                weight_decay = 0.0005,\n","                weight_init=\"he_normal\",\n","                use_bias = False,\n","                dropout_probability = 0.0\n","                ):\n","    def f(net):\n","        # format of conv_params:\n","        #               [ [nb_col=\"kernel width\", nb_row=\"kernel height\",\n","        #               subsample=\"(stride_vertical,stride_horizontal)\",\n","        #               border_mode=\"same\" or \"valid\"] ]\n","        # B(3,3): orignal <<basic>> block\n","        conv_params = [ [3,3,stride,\"same\"],\n","                        [3,3,(1,1),\"same\"] ]\n","        \n","        n_bottleneck_plane = n_output_plane\n","\n","        # Residual block\n","        for i, v in enumerate(conv_params):\n","            if i == 0:\n","                if n_input_plane != n_output_plane:\n","                    net = BatchNormalization(axis=channel_axis, name=f\"BN{str(i)}_{identifier}_{uuid.uuid4()}\")(net)\n","                    net = Activation(\"relu\")(net)\n","                    convs = net\n","                else:\n","                    convs = BatchNormalization(axis=channel_axis, name=f\"BN{str(i)}_{identifier}_{uuid.uuid4()}\")(net)\n","                    convs = Activation(\"relu\")(convs)\n","                convs = Conv2D(n_bottleneck_plane, \n","                               (v[0],v[1]),\n","                                strides=v[2],\n","                                padding=v[3],\n","                                kernel_initializer=weight_init,\n","                                kernel_regularizer=L2(weight_decay),\n","                                use_bias=use_bias,\n","                                name = f\"CONV0_{identifier}_{uuid.uuid4()}\")(convs)\n","            else:\n","                convs = BatchNormalization(axis=channel_axis, name=f\"BN{str(i)}_{identifier}_{uuid.uuid4()}\")(convs)\n","                convs = Activation(\"relu\")(convs)\n","                if dropout_probability > 0:\n","                   convs = Dropout(dropout_probability, name=f\"DRP{str(i)}_{identifier}_{uuid.uuid4()}\")(convs)\n","                convs = Conv2D(n_bottleneck_plane, \n","                               (v[0],v[1]),\n","                                strides=v[2],\n","                                padding=v[3],\n","                                kernel_initializer=weight_init,\n","                                kernel_regularizer=L2(weight_decay),\n","                                use_bias=use_bias,\n","                                name=f\"CONV{str(i)}_{identifier}_{uuid.uuid4()}\")(convs)\n","\n","        # Shortcut Conntection: identity function or 1x1 convolutional\n","        #  (depends on difference between input & output shape - this\n","        #   corresponds to whether we are using the first block in each\n","        #   group; see _layer() ).\n","        if n_input_plane != n_output_plane:\n","            shortcut = Conv2D(n_output_plane, \n","                              (1,1),\n","                              strides=stride,\n","                              padding=\"same\",\n","                              kernel_initializer=weight_init,\n","                              kernel_regularizer=L2(weight_decay),\n","                              use_bias=use_bias,\n","                              name=f\"CONVSHORTCUT_{identifier}_{uuid.uuid4()}\")(net)\n","        else:\n","            shortcut = net\n","\n","        return Add()([convs, shortcut])\n","    \n","    return f\n","\n","\n","# \"Stacking Residual Units on the same stage\"\n","def _layer(block, n_input_plane, n_output_plane, count, stride):\n","    def f(net):\n","        net = block(n_input_plane, n_output_plane, stride)(net)\n","        for i in range(2,int(count+1)):\n","            net = block(n_output_plane, n_output_plane, stride=(1,1))(net)\n","        return net\n","    \n","    return f\n"]},{"cell_type":"markdown","metadata":{"id":"n14_FVnPRzTA"},"source":["This will be WRN-28-10 WideResNet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2zWw-kt6RxYk"},"outputs":[],"source":["input_shape=train_data[0,:,:,:].shape\n","weight_decay = 0.0005\n","weight_init=\"he_normal\"\n","use_bias = False\n","k = 10\n","depth = 28             \n","n = (depth - 4) / 6\n","dropout_probability = 0.0\n","# batch_size = 128      \n","# nb_epochs = 200\n","channel_axis = -1\n","\n","num_wrn_models = 10\n","no_classes=10\n","import functools"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QuHzXzVpVkmd"},"outputs":[],"source":["# set up 40 WRN-28-10 models \n","model_wrns = []\n","\n","for i in range(num_wrn_models):\n","  inputs_wrn = Input(shape=input_shape,name=f\"M{str(i)}_INPUT\")\n","  n_stages=[16, 16*k, 32*k, 64*k]\n","  conv1_wrn = Conv2D(16, \n","                  (3, 3), \n","                  strides=1,\n","                  padding=\"same\",\n","                  kernel_initializer=weight_init,\n","                  kernel_regularizer=L2(weight_decay),\n","                  use_bias=use_bias,\n","                  name=\"C1BLOCK\")(inputs_wrn) # \"One conv at the beginning (spatial size: 32x32)\"\n","  # Add wide residual blocks\n","  block_fn = _wide_basic\n","  conv2_wrn = _layer(functools.partial(block_fn,identifier=f\"C2BLOCK\"), n_input_plane=n_stages[0], n_output_plane=n_stages[1], count=n, stride=(1,1))(conv1_wrn)# \"Stage 1 (spatial size: 32x32)\"\n","  conv3_wrn = _layer(functools.partial(block_fn,identifier=f\"C3BLOCK\"), n_input_plane=n_stages[1], n_output_plane=n_stages[2], count=n, stride=(2,2))(conv2_wrn)# \"Stage 2 (spatial size: 16x16)\"\n","  conv4_wrn = _layer(functools.partial(block_fn,identifier=f\"C4BLOCK\"), n_input_plane=n_stages[2], n_output_plane=n_stages[3], count=n, stride=(2,2))(conv3_wrn)# \"Stage 3 (spatial size: 8x8)\"\n","\n","  batch_norm_wrn = BatchNormalization(axis=channel_axis,name=f\"M{str(i)}_BN\")(conv4_wrn)\n","  relu_wrn = Activation(\"relu\")(batch_norm_wrn)\n","                                          \n","  # Classifier block\n","  pool_wrn = AveragePooling2D(pool_size=(8, 8), strides=(1, 1), padding=\"same\", name=f\"CLASSIFIER_AVPL\")(relu_wrn)\n","  flatten_wrn = Flatten(name=f\"CLASSIFIER_FL\")(pool_wrn)\n","  predictions_wrn = Dense(units=no_classes, kernel_initializer=weight_init, use_bias=use_bias,\n","                      kernel_regularizer=L2(weight_decay), activation=\"softmax\", name=\"CLASSIFIER_D1\")(flatten_wrn)\n","  model_wrn = Model(inputs=inputs_wrn, outputs=predictions_wrn)\n","  model_wrns.append(model_wrn)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lq0CzhUGVIg_"},"outputs":[],"source":["for model_count in [i for i in range(len(model_wrns))]:\n","  mwrn, hwrn = compile_and_fit_model_basic(  model_wrns[model_count], \n","                                           f\"WideResNet28-10_ID{str(uuid.uuid4()).split('-')[0]}_{str(model_count+11)}_{datetime.datetime.now():%Y%m%d%H%M%S}\",\n","                                          train_data[0,:,:,:].shape,\n","                                          train_data,\n","                                          train_targets,\n","                                          save_max_epoch=False,\n","                                          save_final=True,\n","                                          patience_count = 35,\n","                                          early_stopping_obs = 'val_sparse_categorical_accuracy',\n","                                          log_history = True,\n","                                          verbose_level = 1,\n","                                          batch_size=256, \n","                                          epochs=250, \n","                                          class_weight=None,\n","                                          validation_data=(validation_data, validation_targets))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"brTi7688Dx78"},"outputs":[],"source":["# # saving the features of WideResNets for the training data\n","\n","check_model_string = \"WideResNet28-10_\"\n","not_check_model_string = [\"arallel\",\"Collab_\"]\n","\n","acceptable_chunk = 5000\n","idxs_for_train = np.unique(list(range(0,train_data.shape[0],acceptable_chunk)) + [train_data.shape[0]]).tolist()\n","\n","for dir_entry in os.scandir(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/\"):\n","  if os.path.isdir(dir_entry):\n","    if check_model_string in str(dir_entry) and all([ (not ncs in dir_entry.name) for ncs in not_check_model_string]):\n","      for i in range(len(idxs_for_train[:-1])):\n","        print(f\"{dir_entry.name}_{idxs_for_train[i]}-{idxs_for_train[i+1]}\")\n","        write_features_from_models(\n","          f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{dir_entry.name}\",\n","          \"CLASSIFIER_FL\",\n","          train_data[idxs_for_train[i]:idxs_for_train[i+1],:,:,:], train_targets[idxs_for_train[i]:idxs_for_train[i+1],:],\n","          reverse_one_hot=False,\n","          normalize_X_func=None,\n","          dataset_id=f\"Train{str(i)}_{idxs_for_train[i]}-{idxs_for_train[i+1]}\")\n","        write_features_from_models(\n","          f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{dir_entry.name}\",\n","          \"CLASSIFIER_D1\",\n","          train_data[idxs_for_train[i]:idxs_for_train[i+1],:,:,:], train_targets[idxs_for_train[i]:idxs_for_train[i+1],:],\n","          reverse_one_hot=False,\n","          normalize_X_func=None,\n","          dataset_id=f\"Train{str(i)}_{idxs_for_train[i]}-{idxs_for_train[i+1]}\")\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PzPD9hg1vTTk"},"outputs":[],"source":["\n","# # saving the features of WideResNets for the Validation data  \n","\n","check_model_string = \"WideResNet28-10_\"\n","not_check_model_string = [\"arallel\",\"Collab_\"]\n","\n","acceptable_chunk = 5000\n","idxs_for_validation = np.unique(list(range(0,validation_data_grey.shape[0],acceptable_chunk)) + [validation_data_grey.shape[0]]).tolist()\n","\n","for dir_entry in os.scandir(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/\"):\n","  if os.path.isdir(dir_entry):\n","    if check_model_string in str(dir_entry) and all([ (not ncs in dir_entry.name) for ncs in not_check_model_string]):\n","      for i in range(len(idxs_for_validation[:-1])):\n","        print(f\"{dir_entry.name}__{idxs_for_validation[i]}-{idxs_for_validation[i+1]}\")\n","        write_features_from_models(\n","          f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{dir_entry.name}\",\n","          \"CLASSIFIER_FL\",\n","          validation_data_grey[idxs_for_validation[i]:idxs_for_validation[i+1],:,:,:], validation_targets[idxs_for_validation[i]:idxs_for_validation[i+1],:],\n","          reverse_one_hot=False,\n","          normalize_X_func=None,\n","          dataset_id=f\"Validation{str(i)}_{idxs_for_validation[i]}-{idxs_for_validation[i+1]}\")     \n","        write_features_from_models(\n","          f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{dir_entry.name}\",\n","          \"CLASSIFIER_D1\",\n","          validation_data_grey[idxs_for_validation[i]:idxs_for_validation[i+1],:,:,:], validation_targets[idxs_for_validation[i]:idxs_for_validation[i+1],:],\n","          reverse_one_hot=False,\n","          normalize_X_func=None,\n","          dataset_id=f\"Validation{str(i)}_{idxs_for_validation[i]}-{idxs_for_validation[i+1]}\")     \n","      "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"quehv3Q9fDC8"},"outputs":[],"source":["gpus = tf.config.experimental.list_physical_devices('GPU') \n","# for gpu in gpus: \n","#   tf.config.experimental.set_memory_growth(gpu, True)\n","gpus"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"T3g_gaA1S2iP"},"outputs":[],"source":["\n","# # saving the features of WideResNets for the Test data  \n","\n","check_model_string = \"WideResNet28-10_\"\n","not_check_model_string = [\"arallel\",\"Collab_\"] \n","\n","acceptable_chunk = 5000\n","idxs_for_test = np.unique(list(range(0,test_data_grey.shape[0],acceptable_chunk)) + [test_data_grey.shape[0]]).tolist()\n","\n","for dir_entry in os.scandir(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/\"):\n","  if os.path.isdir(dir_entry):\n","    if check_model_string in str(dir_entry) and all([ (not ncs in dir_entry.name) for ncs in not_check_model_string]):\n","      for i in range(len(idxs_for_test[:-1])):\n","      # # saving the features of WideResNets for the Test data      \n","        print(f\"{dir_entry.name}__{idxs_for_test[i]}-{idxs_for_test[i+1]}\")\n","        write_features_from_models(\n","          f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{dir_entry.name}\",\n","          \"CLASSIFIER_FL\",\n","          test_data_grey[idxs_for_test[i]:idxs_for_test[i+1],:,:,:], test_targets[idxs_for_test[i]:idxs_for_test[i+1],:],\n","          reverse_one_hot=False,\n","          normalize_X_func=None,\n","          dataset_id=f\"TestBatch{str(i)}_{idxs_for_test[i]}-{idxs_for_test[i+1]}\")      \n","        write_features_from_models(\n","          f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{dir_entry.name}\",\n","          \"CLASSIFIER_D1\",\n","          test_data_grey[idxs_for_test[i]:idxs_for_test[i+1],:,:,:], test_targets[idxs_for_test[i]:idxs_for_test[i+1],:],\n","          reverse_one_hot=False,\n","          normalize_X_func=None,\n","          dataset_id=f\"TestBatch{str(i)}_{idxs_for_test[i]}-{idxs_for_test[i+1]}\")              "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9TxCYEyUEnem"},"outputs":[],"source":["# getting the scores for the individual WideResNets on the test data set\n","scores_wideresnets_simple = []\n","check_model_string = \"WideResNet28-10_\"\n","not_check_model_string = [\"arallel\",\"Collab_\"]\n","for dir_entry in os.scandir(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/\"):\n","  if os.path.isdir(dir_entry):\n","    if check_model_string in str(dir_entry) and all([ (not ncs in dir_entry.name) for ncs in not_check_model_string]):\n","      print(dir_entry.name)\n","      model_here = tf.keras.models.load_model(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{dir_entry.name}\")  \n","      y_predict_here = np.array(model_here.predict(test_data_grey), dtype='float64')\n","      y_predict_here = np.apply_along_axis(np.argmax, 1, y_predict_here)\n","      print(pr_rc_f1_acc_from_supplied(y_predict_here, test_targets))\n","      scores_wideresnets_simple.append(pr_rc_f1_acc_from_supplied(y_predict_here, test_targets))\n","\n","np.save(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/individual_WideResNet2810_summary_{datetime.datetime.now():%Y%m%d%H%M%S}\", np.array(scores_wideresnets_simple), \n","               allow_pickle=True, \n","               fix_imports=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vu9-in9HfWZc"},"outputs":[],"source":["# print(test_data_grey.shape)\n","# print(test_targets.shape)\n","# a=np.unique(list(range(0,test_data_grey.shape[0],5000)) + [test_data_grey.shape[0]]).tolist()\n","# print(a)\n","# a[:-1]"]},{"cell_type":"markdown","metadata":{"id":"ib862JpbN7oI"},"source":["# Save the validation results for all the DNN, CNN, ResNet50, WideResnets to see whether it can be used for 'selecting' models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_09nA-cDONBT"},"outputs":[],"source":["import pandas as pd\n","\n","def get_validation_acc_from_history_file(f1):\n","  pd1 = pd.read_csv(f1)\n","  return pd1[pd1.epoch==max(pd1.epoch)].val_sparse_categorical_accuracy.iloc[0]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rqG47BAEONen"},"outputs":[],"source":["all_histories = os.listdir(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_history\")\n","\n","all_dnn_histories = [f for f in all_histories if \"DNN_\" in f and \"arallel\" not in f and \"ollab\" not in f]\n","all_cnn_histories = [f for f in all_histories if \"CNN_\" in f and \"arallel\" not in f and \"ollab\" not in f]\n","all_resnet50_histories = [f for f in all_histories if \"RestNet50\" in f and \"arallel\" not in f and \"ollab\" not in f]\n","all_WideResNet_histories = [f for f in all_histories if \"WideResNet\" in f and \"arallel\" not in f and \"ollab\" not in f]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7yHc2JWiiFpL"},"outputs":[],"source":["dnn_val_accs = pd.DataFrame( { \"Type\": \"DNN\", \"File\": all_dnn_histories, \"ValAcc\" : [ get_validation_acc_from_history_file(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_history/{f}\") for f in all_dnn_histories  ] } )\n","cnn_val_accs = pd.DataFrame( { \"Type\": \"CNN\", \"File\": all_cnn_histories, \"ValAcc\" : [ get_validation_acc_from_history_file(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_history/{f}\") for f in all_cnn_histories  ] } )\n","resnet50_val_accs = pd.DataFrame( { \"Type\": \"ResNet50\", \"File\": all_resnet50_histories, \"ValAcc\" : [ get_validation_acc_from_history_file(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_history/{f}\") for f in all_resnet50_histories  ] } )\n","wideResNet_val_accs = pd.DataFrame( { \"Type\": \"WideResNet\", \"File\": all_WideResNet_histories, \"ValAcc\" : [ get_validation_acc_from_history_file(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_history/{f}\") for f in all_WideResNet_histories  ] } )\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"feXi7f-Zq4vz"},"outputs":[],"source":["import datetime\n","val_accs_base_models = pd.concat([dnn_val_accs, cnn_val_accs, resnet50_val_accs, wideResNet_val_accs])\n","val_accs_base_models.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_ccs/validation_accs_base_{datetime.datetime.now():%Y%m%d%H%M%S}.csv\", index=False)"]},{"cell_type":"markdown","metadata":{"id":"d4NWpQ7wAS1u"},"source":["# Set up ensembles of 20 DNN, 20 CNN or 10 CNN/10 DNN to predict (randomly assembled from the 100 before)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iIL2mp4cmkT0"},"outputs":[],"source":["num_of_repeats = 20\n","num_of_models = 20\n","\n","x_input = test_data_grey\n","y_input = test_targets\n","\n","# create traditional ensemble of 20 DNNs \n","scores_dnn20 = []\n","for repc in range(num_of_repeats):\n","  dnn_models_to_use = [ tf.keras.models.load_model(mfile) for mfile in sorted(random.sample(dnn_model_dirs, num_of_models))]\n","  dnn_model_predictions = [ model.predict(x_input) for model in dnn_models_to_use]\n","  y_ens_preds = avgfilter_ensemble_predictions(dnn_model_predictions, x_input)\n","  scores_dnn20.append(pr_rc_f1_acc_from_supplied(y_ens_preds,y_input))\n","\n","# create traditional ensemble of 20 CNNs \n","scores_cnn20 = []\n","for repc in range(num_of_repeats):\n","  cnn_models_to_use = [ tf.keras.models.load_model(mfile) for mfile in sorted(random.sample(cnn_model_dirs, num_of_models))]\n","  cnn_model_predictions = [ model.predict(x_input) for model in cnn_models_to_use]\n","  y_ens_preds = avgfilter_ensemble_predictions(cnn_model_predictions, x_input)\n","  scores_cnn20.append(pr_rc_f1_acc_from_supplied(y_ens_preds,y_input))\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FROXYexX_CcB"},"outputs":[],"source":["num_of_repeats = 20\n","num_of_models = 20\n","\n","x_input = test_data_grey\n","y_input = test_targets\n","\n","# create traditional ensemble of 10 CNNs and 10 DNNs\n","scores_cnn10dnn10 = []\n","for repc in range(num_of_repeats):\n","  cnn_models_to_use = [ tf.keras.models.load_model(mfile) for mfile in sorted(random.sample(cnn_model_dirs, int(num_of_models/2)))]\n","  cnn_model_predictions = [ model.predict(x_input) for model in cnn_models_to_use]\n","  dnn_models_to_use = [ tf.keras.models.load_model(mfile) for mfile in sorted(random.sample(dnn_model_dirs, int(num_of_models/2)))]\n","  models_to_use = cnn_models_to_use\n","  models_to_use.extend(dnn_models_to_use)\n","  model_predictions = [ model.predict(x_input) for model in models_to_use]\n","  y_ens_preds = avgfilter_ensemble_predictions(model_predictions, x_input)\n","  scores_cnn10dnn10.append(pr_rc_f1_acc_from_supplied(y_ens_preds,y_input))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zVqJ8RVFV5pn"},"source":["Save the score results of the ensembles"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DNE4dpF_V5pn"},"outputs":[],"source":["np.save(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/traditional_ensemble_dnn20_summary_{datetime.datetime.now():%Y%m%d%H%M%S}\", np.array(scores_dnn20), \n","               allow_pickle=True, \n","               fix_imports=True)\n","np.save(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/traditional_ensemble_cnn20_summary_{datetime.datetime.now():%Y%m%d%H%M%S}\", np.array(scores_cnn20), \n","               allow_pickle=True, \n","               fix_imports=True)\n","np.save(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/traditional_ensemble_cnn10dnn10_summary_{datetime.datetime.now():%Y%m%d%H%M%S}\", np.array(scores_cnn10dnn10), \n","               allow_pickle=True, \n","               fix_imports=True)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"N3pww0F8PDy4"},"source":["# Set up ensembles of 20 ResNets50 (randomly assembled from the 40 before) and 10 WideResNets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VctHsz0EPDy4"},"outputs":[],"source":["num_of_repeats = 10\n","num_of_models = 20\n","\n","x_input = test_data_grey\n","y_input = test_targets\n","\n","# create traditional ensemble of 20 resnets \n","scores_resnets = []\n","for repc in range(num_of_repeats):\n","  print(repc)\n","  resnets_models_to_use = [ tf.keras.models.load_model(mfile) for mfile in sorted(random.sample(resnets_model_dirs, num_of_models))]\n","  resnets_model_predictions = [ model.predict(x_input) for model in resnets_models_to_use]\n","  y_ens_preds = avgfilter_ensemble_predictions(resnets_model_predictions, x_input)\n","  scores_resnets.append(pr_rc_f1_acc_from_supplied(y_ens_preds,y_input))\n","  del resnets_models_to_use\n","  del resnets_model_predictions\n","  del y_ens_preds\n","\n","\n","# create a traditional ensemble of 10 wideresnets \n","scores_wideresnets = []\n","for repc in range(1):\n","  print(repc)\n","  wideresnets_models_to_use = [ tf.keras.models.load_model(mfile) for mfile in sorted(random.sample(wideresnets_model_dirs, 10))]\n","  wideresnets_model_predictions = [ model.predict(x_input) for model in wideresnets_models_to_use]\n","  y_ens_preds = avgfilter_ensemble_predictions(wideresnets_model_predictions, x_input)\n","  scores_wideresnets.append(pr_rc_f1_acc_from_supplied(y_ens_preds,y_input))\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0VkkQpmnAdAE"},"source":["Save the score results of the ensembles resnets and wideResNets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oC7uHy83spxp"},"outputs":[],"source":["np.save(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/traditional_ensemble_resnets_summary_{datetime.datetime.now():%Y%m%d%H%M%S}\", np.array(scores_resnets), \n","               allow_pickle=True, \n","               fix_imports=True)\n","np.save(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/traditional_ensemble_wideresnet28-10_summary_{datetime.datetime.now():%Y%m%d%H%M%S}\", np.array(scores_wideresnets), \n","               allow_pickle=True, \n","               fix_imports=True)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"05H-E-Hb21S9"},"source":["# Set up the models + features file lists for CNN/DNN/ResNet50/WideResNet28-10 splits for reuse SVHN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ll_pxppktl28"},"outputs":[],"source":["acceptable_string_grabs = [ \"CNN_A_\", \"DNN_A_\", \"RestNet50_A_\", \"WideResNet28-10_ID\"]\n","not_check_model_string = [\"arallel\",\"Collab_\"]\n","\n","model_dirs = [  f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{dir_entry.name}\" \n","                for dir_entry in os.scandir(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/\") \n","                if os.path.isdir(dir_entry) and any(xs in dir_entry.name for xs in acceptable_string_grabs) ]\n","dnn_model_dirs = [ s for s in model_dirs if \"DNN\" in s and all([ (not ncs in s) for ncs in not_check_model_string])] \n","cnn_model_dirs = [ s for s in model_dirs if \"CNN\" in s and all([ (not ncs in s) for ncs in not_check_model_string])] \n","resnets_model_dirs = [ s for s in model_dirs if \"RestNet50\" in s and all([ (not ncs in s) for ncs in not_check_model_string])] \n","wideresnets_model_dirs = [ s for s in model_dirs if \"WideResNet\" in s and all([ (not ncs in s) for ncs in not_check_model_string])] \n","\n","model_features_files = [  f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features/{file_entry.name}\" \n","                for file_entry in os.scandir(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features/\") \n","                if os.path.isfile(file_entry) and any(xs in file_entry.name for xs in acceptable_string_grabs) ]\n","\n","dnn_features_files = [ s for s in model_features_files if \"DNN\" in s and all([ (not ncs in s) for ncs in not_check_model_string])] \n","cnn_features_files = [ s for s in model_features_files if \"CNN\" in s and all([ (not ncs in s) for ncs in not_check_model_string])] \n","resnets_features_files = [ s for s in model_features_files if \"RestNet50\" in s and all([ (not ncs in s) for ncs in not_check_model_string])] \n","wideresnets_features_files = [ s for s in model_features_files if \"WideResNet\" in s and all([ (not ncs in s) for ncs in not_check_model_string])] \n","\n","dnn_identifier = acceptable_string_grabs[1]\n","# dnn_layer_name = \"DRP1\"\n","\n","cnn_identifier = acceptable_string_grabs[0]\n","# cnn_layer_name = \"D3R\"\n","\n","wideresnet_identifier = acceptable_string_grabs[3]\n","# wideresnet_layer_name = \"CLASSIFIER_FL\"\n","\n","\n","def avgfilter_ensemble_predictions(y_pred_ms, xtest):\n","  y_preds_ens_prb = np.apply_along_axis(np.mean, 0, y_pred_ms)\n","  y_preds_ens_idx = np.apply_along_axis(np.argmax, 1, y_preds_ens_prb) \n","  # y_preds_ens_idx = y_preds_ens_idx + 1\n","  return y_preds_ens_idx  \n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7NfTm6C0efVR"},"source":["# Set up data for ensemble plot collection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0N1xIkYzeOqW"},"outputs":[],"source":["# set up the data\n","ensemble_test_results = None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vlEnnUKUYQsU"},"outputs":[],"source":["x_input = test_data_grey\n","y_input = test_targets\n","\n","num_of_repeats = 10\n","num_of_models = [3,4,7,8,13,16,19]\n","# [1,2,5,10,12,15,18,20]\n","\n","# resnet_model_predicted_values =  dict(zip(resnets_model_dirs,[ resnet_loaded_models[model].predict(x_input) for model in resnets_model_dirs]))\n","# dnn_model_predicted_values =  dict(zip(dnn_model_dirs,[ dnn_loaded_models[model].predict(x_input) for model in dnn_model_dirs]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pLSgKRgSmPGz"},"outputs":[],"source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')"]},{"cell_type":"markdown","metadata":{"id":"zarZ-ubirq5w"},"source":["# Data for plot of #DNN in ensemble "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KY_OZVbUvt1p"},"outputs":[],"source":["dnn_loaded_models = dict(zip(dnn_model_dirs,[ tf.keras.models.load_model(mfile) for mfile in dnn_model_dirs]))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iTD7m-7DJ9iu"},"outputs":[],"source":["# from numba import jit, prange\n","# @jit(nopython=True, parallel=True)\n","# def parallel_sum(A):\n","#     sum = 0.0\n","#     for i in prange(A.shape[0]):\n","#         sum += A[i]\n","#     return sum"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UFsdTnB3PEHI"},"outputs":[],"source":["x_input = test_data_grey\n","y_input = test_targets\n","\n","num_of_repeats = 4\n","num_of_models = [22,25,30,35,40]  # [2,3,4,6,7,8,9,11,12,13,14,16,19]\n","\n","idxCount = 0 if ensemble_test_results is None else len(ensemble_test_results.index)\n","for mc in num_of_models:\n","  for repc in range(num_of_repeats):\n","    selected_model_names = sorted(random.sample(dnn_model_dirs, min(len(dnn_model_dirs),mc)))\n","    dnn_models_to_use = [ dnn_loaded_models[mn] for mn in selected_model_names ]\n","    dnn_model_predictions = [ model.predict(x_input) for model in dnn_models_to_use ]\n","    y_ens_preds = avgfilter_ensemble_predictions(dnn_model_predictions, x_input)\n","    pr, rc, f1, acc = pr_rc_f1_acc_from_supplied(y_ens_preds,y_input)\n","    print (mc, repc, pr, rc, f1, acc)\n","    if ensemble_test_results is None:\n","      ensemble_test_results = pd.DataFrame({\"Type\": \"DNN\", \n","                                            \"Data\" : \"Test\",\n","                                            \"NumOfModels\": len(selected_model_names), \n","                                            \"RepC\": repc, \n","                                            \"Pr\": pr,\n","                                            \"Rc\": rc,\n","                                            \"F1\": f1,\n","                                            \"Acc\": acc,\n","                                            \"ModelNames\": \"XOX\".join([ \"_\".join(zz.split(\"DNN_A_\")[1].split(\"_\")[:2]) for zz in selected_model_names])\n","                                            }, index = [idxCount])\n","    else:\n","      ensemble_test_results = pd.concat([ensemble_test_results,\n","                                         pd.DataFrame({\"Type\": \"DNN\", \n","                                            \"Data\" : \"Test\",\n","                                            \"NumOfModels\": len(selected_model_names), \n","                                            \"RepC\": repc, \n","                                            \"Pr\": pr,\n","                                            \"Rc\": rc,\n","                                            \"F1\": f1,\n","                                            \"Acc\": acc,\n","                                            \"ModelNames\": \"XOX\".join([ \"_\".join(zz.split(\"DNN_A_\")[1].split(\"_\")[:2]) for zz in selected_model_names])\n","                                            }, index = [idxCount])\n","                                         ])\n","    idxCount = idxCount + 1\n","    del dnn_models_to_use\n","    del dnn_model_predictions\n","    del y_ens_preds\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lfyPSdOghY4j"},"outputs":[],"source":["ensemble_test_results.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/ensemble_test_results_{datetime.datetime.now():%Y%m%d%H%M%S}.csv\")"]},{"cell_type":"markdown","metadata":{"id":"Qr4uwylNzQl1"},"source":["# Data for plot of #CNN in ensemble "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VRWdcBVizQl2"},"outputs":[],"source":["\n","# preload all the models\n","# resnet_loaded_models = dict(zip(resnets_model_dirs,[ tf.keras.models.load_model(mfile) for mfile in resnets_model_dirs]))\n","# dnn_loaded_models = dict(zip(dnn_model_dirs,[ tf.keras.models.load_model(mfile) for mfile in dnn_model_dirs]))\n","cnn_loaded_models = dict(zip(cnn_model_dirs,[ tf.keras.models.load_model(mfile) for mfile in cnn_model_dirs]))\n","# wideresnet_loaded_models = dict(zip(wideresnets_model_dirs,[ tf.keras.models.load_model(mfile) for mfile in wideresnets_model_dirs]))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MR3GuoTdzqGc"},"outputs":[],"source":["# for CNN we try to memoize the predictions... ?\n","# cnn_model_predictions = [ cnn_loaded_models[mn].predict(x_input) for mn in list(cnn_loaded_models.keys()) ]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WCDDvCaczQl2"},"outputs":[],"source":["# from numba import jit, prange\n","# @jit(nopython=True, parallel=True)\n","# def parallel_sum(A):\n","#     sum = 0.0\n","#     for i in prange(A.shape[0]):\n","#         sum += A[i]\n","#     return sum"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lPhanQJWzQl2"},"outputs":[],"source":["# num_of_models = [16,20]\n","\n","x_input = test_data_grey\n","y_input = test_targets\n","\n","num_of_repeats = 4\n","num_of_models = [22,25,30,35,40] # [2,3,4,6,7,8,9,11,12,13,14,16,19]\n","\n","idxCount = 0 if ensemble_test_results is None else len(ensemble_test_results.index)\n","for mc in num_of_models:\n","  for repc in range(num_of_repeats):\n","    selected_model_names = sorted(random.sample(cnn_model_dirs, min(len(cnn_model_dirs),mc)))\n","    cnn_models_to_use = [ cnn_loaded_models[mn] for mn in selected_model_names ]\n","    cnn_model_predictions = [ model.predict(x_input) for model in cnn_models_to_use ]\n","    y_ens_preds = avgfilter_ensemble_predictions(cnn_model_predictions, x_input)\n","    pr, rc, f1, acc = pr_rc_f1_acc_from_supplied(y_ens_preds,y_input)\n","    print (mc, repc, pr, rc, f1, acc)\n","    if ensemble_test_results is None:\n","      ensemble_test_results = pd.DataFrame({\"Type\": \"CNN\", \n","                                            \"Data\" : \"Test\",\n","                                            \"NumOfModels\": len(selected_model_names), \n","                                            \"RepC\": repc, \n","                                            \"Pr\": pr,\n","                                            \"Rc\": rc,\n","                                            \"F1\": f1,\n","                                            \"Acc\": acc,\n","                                            \"ModelNames\": \"XOX\".join([ \"_\".join(zz.split(\"CNN_A_\")[1].split(\"_\")[:2]) for zz in selected_model_names])\n","                                            }, index = [idxCount])\n","    else:\n","      ensemble_test_results = pd.concat([ensemble_test_results,\n","                                         pd.DataFrame({\"Type\": \"CNN\", \n","                                            \"Data\" : \"Test\",\n","                                            \"NumOfModels\": len(selected_model_names), \n","                                            \"RepC\": repc, \n","                                            \"Pr\": pr,\n","                                            \"Rc\": rc,\n","                                            \"F1\": f1,\n","                                            \"Acc\": acc,\n","                                            \"ModelNames\": \"XOX\".join([ \"_\".join(zz.split(\"CNN_A_\")[1].split(\"_\")[:2]) for zz in selected_model_names])\n","                                            }, index = [idxCount])\n","                                         ])\n","    idxCount = idxCount + 1\n","    del cnn_models_to_use\n","    del cnn_model_predictions\n","    del y_ens_preds\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mqinAog5zQl2"},"outputs":[],"source":["ensemble_test_results.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/cnn_ensemble_test_results_{datetime.datetime.now():%Y%m%d%H%M%S}.csv\")\n"]},{"cell_type":"markdown","metadata":{"id":"nlcSr-vhOENw"},"source":["# Data for plot of #ResNets in ensemble "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0sIHDYSOOENw"},"outputs":[],"source":["\n","# preload all the models\n","resnet_loaded_models = dict(zip(resnets_model_dirs,[ tf.keras.models.load_model(mfile) for mfile in resnets_model_dirs]))\n","# dnn_loaded_models = dict(zip(dnn_model_dirs,[ tf.keras.models.load_model(mfile) for mfile in dnn_model_dirs]))\n","# cnn_loaded_models = dict(zip(cnn_model_dirs,[ tf.keras.models.load_model(mfile) for mfile in cnn_model_dirs]))\n","# wideresnet_loaded_models = dict(zip(wideresnets_model_dirs,[ tf.keras.models.load_model(mfile) for mfile in wideresnets_model_dirs]))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X7C96YxfOENw"},"outputs":[],"source":["x_input = test_data_grey\n","y_input = test_targets\n","\n","num_of_repeats = 4\n","num_of_models = [22,25,30,35,39]  # [1,2,5,10,12,15,18,20] \n","# [3,4,7,8,13,16,19]\n","# [1,2,5,10,12,15,18,20]\n","del cnn_loaded_models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y-Sv3xjyXv7W"},"outputs":[],"source":["# del resnet_model_predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J0TWDQcWOENw"},"outputs":[],"source":["# for ResNets we try to memoize the predictions... ?\n","resnet_model_predictions = dict(zip(resnets_model_dirs, [ resnet_loaded_models[mn].predict(x_input) for mn in resnets_model_dirs ]))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AW3AdzcXOENx"},"outputs":[],"source":["\n","idxCount = 0 if ensemble_test_results is None else len(ensemble_test_results.index)\n","for mc in num_of_models:\n","  for repc in range(num_of_repeats):\n","    selected_model_names = sorted(random.sample(resnets_model_dirs, min(len(resnets_model_dirs),mc)))\n","    # resnet_models_to_use = [ resnet_loaded_models[mn] for mn in selected_model_names ]\n","    resnet_model_predictions_here = [ resnet_model_predictions[mn] for mn in selected_model_names ]\n","    y_ens_preds = avgfilter_ensemble_predictions(resnet_model_predictions_here, x_input)\n","    pr, rc, f1, acc = pr_rc_f1_acc_from_supplied(y_ens_preds,y_input)\n","    print (mc, repc, pr, rc, f1, acc)\n","    if ensemble_test_results is None:\n","      ensemble_test_results = pd.DataFrame({\"Type\": \"ResNet50\", \n","                                            \"Data\" : \"Test\",\n","                                            \"NumOfModels\": len(selected_model_names), \n","                                            \"RepC\": repc, \n","                                            \"Pr\": pr,\n","                                            \"Rc\": rc,\n","                                            \"F1\": f1,\n","                                            \"Acc\": acc,\n","                                            \"ModelNames\": \"XOX\".join([ \"_\".join(zz.split(\"RestNet50_A_\")[1].split(\"_\")[:2]) for zz in selected_model_names])\n","                                            }, index = [idxCount])\n","    else:\n","      ensemble_test_results = pd.concat([ensemble_test_results,\n","                                         pd.DataFrame({\"Type\": \"ResNet50\", \n","                                            \"Data\" : \"Test\",\n","                                            \"NumOfModels\": len(selected_model_names), \n","                                            \"RepC\": repc, \n","                                            \"Pr\": pr,\n","                                            \"Rc\": rc,\n","                                            \"F1\": f1,\n","                                            \"Acc\": acc,\n","                                            \"ModelNames\": \"XOX\".join([ \"_\".join(zz.split(\"RestNet50_A_\")[1].split(\"_\")[:2]) for zz in selected_model_names])\n","                                            }, index = [idxCount])\n","                                         ])\n","    idxCount = idxCount + 1\n","    # del resnet_models_to_use\n","    del resnet_model_predictions_here\n","    del y_ens_preds\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"68ex5xdfOENx"},"outputs":[],"source":["ensemble_test_results.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/resnet_ensemble_test_results_{datetime.datetime.now():%Y%m%d%H%M%S}.csv\")\n"]},{"cell_type":"markdown","metadata":{"id":"pVGqX79lZ3pw"},"source":["# Data for plot of #WideResNets in ensemble "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"753xmge4Z3pw"},"outputs":[],"source":["\n","# preload all the models\n","# resnet_loaded_models = dict(zip(resnets_model_dirs,[ tf.keras.models.load_model(mfile) for mfile in resnets_model_dirs]))\n","# dnn_loaded_models = dict(zip(dnn_model_dirs,[ tf.keras.models.load_model(mfile) for mfile in dnn_model_dirs]))\n","# cnn_loaded_models = dict(zip(cnn_model_dirs,[ tf.keras.models.load_model(mfile) for mfile in cnn_model_dirs]))\n","wideresnets_loaded_models = dict(zip(wideresnets_model_dirs,[ tf.keras.models.load_model(mfile) for mfile in wideresnets_model_dirs]))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IdoovBqTZ3pw"},"outputs":[],"source":["x_input = test_data_grey\n","y_input = test_targets\n","\n","num_of_repeats = 10\n","num_of_models = [1,2,3,4,5,6,7,8,9,10] \n","# [3,4,7,8,13,16,19]\n","# [1,2,5,10,12,15,18,20]\n","# del resnet_loaded_models\n","# del resnet_model_predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V5-jUpgRZ3pw"},"outputs":[],"source":["# for WideResNet we try to memoize the predictions... ?\n","wideresnets_model_predictions = dict(zip(wideresnets_model_dirs, [ wideresnet_loaded_models[mn].predict(x_input) for mn in wideresnets_model_dirs ]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TKQun312Z3pw"},"outputs":[],"source":["\n","idxCount = 0 if ensemble_test_results is None else len(ensemble_test_results.index)\n","for mc in num_of_models:\n","  for repc in range(num_of_repeats):\n","    selected_model_names = sorted(random.sample(wideresnets_model_dirs, min(len(wideresnets_model_dirs),mc)))\n","    # resnet_models_to_use = [ resnet_loaded_models[mn] for mn in selected_model_names ]\n","    wideresnet_model_predictions_here = [ wideresnets_model_predictions[mn] for mn in selected_model_names ]\n","    y_ens_preds = avgfilter_ensemble_predictions(wideresnet_model_predictions_here, x_input)\n","    pr, rc, f1, acc = pr_rc_f1_acc_from_supplied(y_ens_preds,y_input)\n","    print (mc, repc, pr, rc, f1, acc)\n","    if ensemble_test_results is None:\n","      ensemble_test_results = pd.DataFrame({\"Type\": \"WideResNet\", \n","                                            \"Data\" : \"Test\",\n","                                            \"NumOfModels\": len(selected_model_names), \n","                                            \"RepC\": repc, \n","                                            \"Pr\": pr,\n","                                            \"Rc\": rc,\n","                                            \"F1\": f1,\n","                                            \"Acc\": acc,\n","                                            \"ModelNames\": \"XOX\".join([ \"_\".join(zz.split(\"WideResNet28-10_\")[1].split(\"_\")[:3]) for zz in selected_model_names])\n","                                            }, index = [idxCount])\n","    else:\n","      ensemble_test_results = pd.concat([ensemble_test_results,\n","                                         pd.DataFrame({\"Type\": \"WideResNet\", \n","                                            \"Data\" : \"Test\",\n","                                            \"NumOfModels\": len(selected_model_names), \n","                                            \"RepC\": repc, \n","                                            \"Pr\": pr,\n","                                            \"Rc\": rc,\n","                                            \"F1\": f1,\n","                                            \"Acc\": acc,\n","                                            \"ModelNames\": \"XOX\".join([ \"_\".join(zz.split(\"WideResNet28-10_\")[1].split(\"_\")[:3]) for zz in selected_model_names])\n","                                            }, index = [idxCount])\n","                                         ])\n","    idxCount = idxCount + 1\n","    # del resnet_models_to_use\n","    del wideresnet_model_predictions_here\n","    del y_ens_preds\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uquPC-rGZ3px"},"outputs":[],"source":["ensemble_test_results.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/wideresnet_ensemble_test_results_{datetime.datetime.now():%Y%m%d%H%M%S}.csv\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5YEfUSsWG6bm"},"source":["# Data for Contour Plot on Mixed CNN/WideResNet ensemble"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IuYJjSlHHT9Y"},"outputs":[],"source":["wideresnets_loaded_models = dict(zip(wideresnets_model_dirs,[ tf.keras.models.load_model(mfile) for mfile in wideresnets_model_dirs]))\n","cnn_loaded_models = dict(zip(cnn_model_dirs,[ tf.keras.models.load_model(mfile) for mfile in cnn_model_dirs]))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fpy7lzitHiJ9"},"outputs":[],"source":["x_input = test_data_grey\n","y_input = test_targets\n","\n","num_of_repeats = 10\n","num_of_models = [2,3,4,5,6,7,8,9,10,11,12] \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_-U8hTmpHiJ9"},"outputs":[],"source":["# for WideResNet we try to memoize the predictions... ?\n","wideresnets_model_predictions = [ wideresnets_loaded_models[mn].predict(x_input) for mn in wideresnets_model_dirs ]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c_gCb_W0-1UK"},"outputs":[],"source":["cnn_model_predictions = [ cnn_loaded_models[mn].predict(x_input) for mn in list(cnn_loaded_models.keys()) ]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xSZg_DX5bz43"},"outputs":[],"source":["# wideresnets_model_dirs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RFU-Ro3xG6bn"},"outputs":[],"source":["# set up the combinations we are going to try\n","# len(wideresnets_features_files)\n","cnn_wideresnet_ensemble_contour_data = None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Lfkuj2vHz0B"},"outputs":[],"source":["idxCount = 0 if cnn_wideresnet_ensemble_contour_data is None else len(cnn_wideresnet_ensemble_contour_data.index)\n","for mc in num_of_models:\n","  for repc in range(num_of_repeats):\n","    for num_of_wrn in range(mc+1):\n","      num_of_cnn = mc - num_of_wrn\n","\n","      ensemble_model_predictions_here = []\n","      if num_of_cnn > 0:\n","        ensemble_model_predictions_here.extend(random.sample(cnn_model_predictions, min(num_of_cnn,len(cnn_model_predictions)) ))\n","      if num_of_wrn > 0:\n","        ensemble_model_predictions_here.extend(random.sample(wideresnets_model_predictions, min(num_of_wrn,len(wideresnets_model_predictions)) ))\n","\n","      y_ens_preds = avgfilter_ensemble_predictions(ensemble_model_predictions_here, x_input)\n","      pr, rc, f1, acc = pr_rc_f1_acc_from_supplied(y_ens_preds,y_input)\n","      print (mc, num_of_cnn, repc, pr, rc, f1, acc)\n","      if cnn_wideresnet_ensemble_contour_data is None:\n","        cnn_wideresnet_ensemble_contour_data = pd.DataFrame({\"TypeA\": \"CNN\", \n","                                                             \"TypeB\": \"WideResNet\", \n","                                                              \"Data\" : \"Test\",\n","                                                             \"Layer\" : \"Ensemble\",\n","                                                    \"NumOfA\": num_of_cnn, \n","                                                    \"NumOfB\": num_of_wrn, \n","                                                    \"RepC\": repc, \n","                                                    \"Pr\": pr,\n","                                                    \"Rc\": rc,\n","                                                    \"F1\": f1,\n","                                                    \"Acc\": acc\n","                                              }, index = [idxCount])\n","      else:\n","        cnn_wideresnet_ensemble_contour_data = pd.concat([cnn_wideresnet_ensemble_contour_data,\n","                                                          pd.DataFrame({\"TypeA\": \"CNN\", \n","                                                                        \"TypeB\": \"WideResNet\", \n","                                                                        \"Data\" : \"Test\",\n","                                                                        \"Layer\" : \"Ensemble\",\n","                                                                        \"NumOfA\": num_of_cnn, \n","                                                                        \"NumOfB\": num_of_wrn, \n","                                                                        \"RepC\": repc, \n","                                                                        \"Pr\": pr,\n","                                                                        \"Rc\": rc,\n","                                                                        \"F1\": f1,\n","                                                                        \"Acc\": acc\n","                                                                        }, index = [idxCount])])\n","      idxCount = idxCount + 1\n","      # del resnet_models_to_use\n","      del ensemble_model_predictions_here\n","      del y_ens_preds\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nP3uqLTL9M2v"},"outputs":[],"source":["cnn_wideresnet_ensemble_contour_data.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/cnn_wideresnet_ensemble_contour_data_{datetime.datetime.now():%Y%m%d%H%M%S}.csv\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"W8fThz_isxYI"},"source":["# Data for Contour Plot on Mixed CNN/DNN ensemble"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IxNNRDdLsxYY"},"outputs":[],"source":["dnn_loaded_models = dict(zip(dnn_model_dirs,[ tf.keras.models.load_model(mfile) for mfile in dnn_model_dirs]))\n","cnn_loaded_models = dict(zip(cnn_model_dirs,[ tf.keras.models.load_model(mfile) for mfile in cnn_model_dirs]))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1iFW3cntsxYY"},"outputs":[],"source":["x_input = test_data_grey\n","y_input = test_targets\n","\n","num_of_repeats = 4\n","num_of_models = [2,4,6,8,10,15,20,25,30] \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-jFlm_Q5sxYY"},"outputs":[],"source":["# for WideResNet we try to memoize the predictions... ?\n","dnn_model_predictions = [ dnn_loaded_models[mn].predict(x_input) for mn in dnn_model_dirs ]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CVNAmSrSsxYZ"},"outputs":[],"source":["cnn_model_predictions = [ cnn_loaded_models[mn].predict(x_input) for mn in list(cnn_loaded_models.keys()) ]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BNPPTYjAsxYZ"},"outputs":[],"source":["# wideresnets_model_dirs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mk0nfl46sxYZ"},"outputs":[],"source":["# set up the combinations we are going to try\n","# len(wideresnets_features_files)\n","cnn_dnn_ensemble_contour_data = None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"04d4jlM1sxYa"},"outputs":[],"source":["idxCount = 0 if cnn_dnn_ensemble_contour_data is None else len(cnn_dnn_ensemble_contour_data.index)\n","for mc in num_of_models:\n","  for repc in range(num_of_repeats):\n","    for num_of_dnn in range(mc+1):\n","      num_of_cnn = mc - num_of_dnn\n","\n","      ensemble_model_predictions_here = []\n","      if num_of_cnn > 0:\n","        ensemble_model_predictions_here.extend(random.sample(cnn_model_predictions, min(num_of_cnn,len(cnn_model_predictions)) ))\n","      if num_of_dnn > 0:\n","        ensemble_model_predictions_here.extend(random.sample(dnn_model_predictions, min(num_of_dnn,len(dnn_model_predictions)) ))\n","\n","      y_ens_preds = avgfilter_ensemble_predictions(ensemble_model_predictions_here, x_input)\n","      pr, rc, f1, acc = pr_rc_f1_acc_from_supplied(y_ens_preds,y_input)\n","      print (mc, num_of_cnn, repc, pr, rc, f1, acc)\n","      if cnn_dnn_ensemble_contour_data is None:\n","        cnn_dnn_ensemble_contour_data = pd.DataFrame({\"TypeA\": \"CNN\", \n","                                                             \"TypeB\": \"DNN\", \n","                                                              \"Data\" : \"Test\",\n","                                                             \"Layer\" : \"Ensemble\",\n","                                                    \"NumOfA\": num_of_cnn, \n","                                                    \"NumOfB\": num_of_dnn, \n","                                                    \"RepC\": repc, \n","                                                    \"Pr\": pr,\n","                                                    \"Rc\": rc,\n","                                                    \"F1\": f1,\n","                                                    \"Acc\": acc\n","                                              }, index = [idxCount])\n","      else:\n","        cnn_dnn_ensemble_contour_data = pd.concat([cnn_dnn_ensemble_contour_data,\n","                                                          pd.DataFrame({\"TypeA\": \"CNN\", \n","                                                                        \"TypeB\": \"DNN\", \n","                                                                        \"Data\" : \"Test\",\n","                                                                        \"Layer\" : \"Ensemble\",\n","                                                                        \"NumOfA\": num_of_cnn, \n","                                                                        \"NumOfB\": num_of_dnn, \n","                                                                        \"RepC\": repc, \n","                                                                        \"Pr\": pr,\n","                                                                        \"Rc\": rc,\n","                                                                        \"F1\": f1,\n","                                                                        \"Acc\": acc\n","                                                                        }, index = [idxCount])])\n","      idxCount = idxCount + 1\n","      # del resnet_models_to_use\n","      del ensemble_model_predictions_here\n","      del y_ens_preds\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jEpM9VOesxYa"},"outputs":[],"source":["cnn_dnn_ensemble_contour_data.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/cnn_dnn_ensemble_contour_data_{datetime.datetime.now():%Y%m%d%H%M%S}.csv\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3pW2evV5b2wJ"},"source":["# Data for Contour Plot on Mixed DNN/WideResNet ensemble"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ewEnZ4Xmb2wK"},"outputs":[],"source":["wideresnets_loaded_models = dict(zip(wideresnets_model_dirs,[ tf.keras.models.load_model(mfile) for mfile in wideresnets_model_dirs]))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DMWBmmO-cBxf"},"outputs":[],"source":["dnn_loaded_models = dict(zip(dnn_model_dirs,[ tf.keras.models.load_model(mfile) for mfile in dnn_model_dirs]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9DtyfYapb2wL"},"outputs":[],"source":["x_input = test_data_grey\n","y_input = test_targets\n","\n","num_of_repeats = 10\n","num_of_models = [2,3,4,5,6,7,8,9,10,11,12] "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yznjta6xb2wM"},"outputs":[],"source":["# for WideResNet we try to memoize the predictions... ?\n","wideresnets_model_predictions = [ wideresnets_loaded_models[mn].predict(x_input) for mn in wideresnets_model_dirs ]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HXlMbVhwb2wM"},"outputs":[],"source":["dnn_model_predictions = [ dnn_loaded_models[mn].predict(x_input) for mn in list(dnn_loaded_models.keys()) ]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZIi9Y-Rab2wN"},"outputs":[],"source":["# wideresnets_model_dirs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-RcqU1O-b2wN"},"outputs":[],"source":["# set up the combinations we are going to try\n","# len(wideresnets_features_files)\n","dnn_wideresnet_ensemble_contour_data = None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZIwA5Y0Ub2wO"},"outputs":[],"source":["idxCount = 0 if dnn_wideresnet_ensemble_contour_data is None else len(dnn_wideresnet_ensemble_contour_data.index)\n","for mc in num_of_models:\n","  for repc in range(num_of_repeats):\n","    for num_of_wrn in range(mc+1):\n","\n","      num_of_dnn = mc - num_of_wrn\n","\n","      ensemble_model_predictions_here = []\n","      if num_of_dnn > 0:\n","        ensemble_model_predictions_here.extend(random.sample(dnn_model_predictions, min(num_of_dnn,len(dnn_model_predictions)) ))\n","      if num_of_wrn > 0:\n","        ensemble_model_predictions_here.extend(random.sample(wideresnets_model_predictions, min(num_of_wrn,len(wideresnets_model_predictions)) ))\n","\n","      y_ens_preds = avgfilter_ensemble_predictions(ensemble_model_predictions_here, x_input)\n","      pr, rc, f1, acc = pr_rc_f1_acc_from_supplied(y_ens_preds,y_input)\n","      print (mc, num_of_cnn, repc, pr, rc, f1, acc)\n","      if dnn_wideresnet_ensemble_contour_data is None:\n","        dnn_wideresnet_ensemble_contour_data = pd.DataFrame({\"TypeA\": \"DNN\", \n","                                                             \"TypeB\": \"WideResNet\", \n","                                                              \"Data\" : \"Test\",\n","                                                             \"Layer\" : \"Ensemble\",\n","                                                    \"NumOfA\": num_of_dnn, \n","                                                    \"NumOfB\": num_of_wrn, \n","                                                    \"RepC\": repc, \n","                                                    \"Pr\": pr,\n","                                                    \"Rc\": rc,\n","                                                    \"F1\": f1,\n","                                                    \"Acc\": acc\n","                                              }, index = [idxCount])\n","      else:\n","        dnn_wideresnet_ensemble_contour_data = pd.concat([dnn_wideresnet_ensemble_contour_data,\n","                                                          pd.DataFrame({\"TypeA\": \"DNN\", \n","                                                                        \"TypeB\": \"WideResNet\", \n","                                                                        \"Data\" : \"Test\",\n","                                                                        \"Layer\" : \"Ensemble\",\n","                                                                        \"NumOfA\": num_of_dnn, \n","                                                                        \"NumOfB\": num_of_wrn, \n","                                                                        \"RepC\": repc, \n","                                                                        \"Pr\": pr,\n","                                                                        \"Rc\": rc,\n","                                                                        \"F1\": f1,\n","                                                                        \"Acc\": acc\n","                                                                        }, index = [idxCount])])\n","      idxCount = idxCount + 1\n","      # del resnet_models_to_use\n","      del ensemble_model_predictions_here\n","      del y_ens_preds\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nkxTQ3E8b2wP"},"outputs":[],"source":["dnn_wideresnet_ensemble_contour_data.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/dnn_wideresnet_ensemble_contour_data_{datetime.datetime.now():%Y%m%d%H%M%S}.csv\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9SyehzPysxYa"},"source":["# Experiment with CCA on the DNN/CNN models for selection into ensemble/collab\n","### Could also be used to initialize an integrated fully parallel model of DNN/CNN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ttBlxirJsxYb"},"outputs":[],"source":["def get_input_features(X_data, m1, m2, m1_layer_name = \"leaky_re_lu_6\", m2_layer_name = \"dropout_7\"):\n","  extractor_m1 = Model(inputs=m1.inputs, outputs=m1.get_layer(m1_layer_name).output)\n","  features_m1 = extractor_m1.predict(X_data)\n","  extractor_m2 = Model(inputs=m2.inputs, outputs=m2.get_layer(m2_layer_name).output)\n","  features_m2 = extractor_m2.predict(X_data)\n","  return tf.concat([features_m1,features_m2])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4pF0jD2vsxYb"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","from sklearn.cross_decomposition import CCA\n","from sklearn.decomposition import PCA\n","pd.set_option('display.max_colwidth', None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-3aOwsdSsxYb"},"outputs":[],"source":["available_dnn_feature_layers = list(set([ x.split(\"_Validation\")[0].split(\"_features_\")[1].split(\"_\")[0] for x in dnn_features_files if \"_X\" in x ]))\n","available_cnn_feature_layers = list(set([ x.split(\"_Validation\")[0].split(\"_features_\")[1].split(\"_\")[0] for x in cnn_features_files if \"_X\" in x ]))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ny0Iy_fQgq80"},"outputs":[],"source":["available_wrn_feature_layers = list(set([ x.split(\"_Validation\")[0].split(\"_features_\")[1].split(\"_\")[1] for x in wideresnets_features_files if \"_X\" in x ]))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WWo7QIj6nwup"},"outputs":[],"source":["# wideresnets_features_files\n","available_wrn_feature_layers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yIxHgOuZsxYb"},"outputs":[],"source":["def get_cca1_pca1_from_file_names(f1,f2):\n","\n","  f1t = np.load(f1)\n","  f2t = np.load(f2)  \n","  scaler1 = StandardScaler()\n","  scaler2 = StandardScaler()\n","\n","  scaler1.fit(f1t)\n","  scaler2.fit(f2t)\n","\n","  f1tt = scaler1.transform(f1t)\n","  f2tt = scaler2.transform(f2t)\n","\n","  pca_f1 = PCA(n_components=1)\n","  pca_f1.fit(f1tt)\n","  pca_f2 = PCA(n_components=1)\n","  pca_f2.fit(f2tt)\n","\n","  ca = CCA()\n","  ca.fit(f1tt, f2tt)\n","  f1tt_c, f2tt_c = ca.transform(f1tt, f2tt)\n","\n","  f1_l = np.apply_along_axis(lambda x: np.dot(pca_f1.components_,np.expand_dims(x, axis=1)),1,f1tt)\n","  f2_l = np.apply_along_axis(lambda x: np.dot(pca_f2.components_,np.expand_dims(x, axis=1)),1,f2tt)\n","  \n","  return (np.corrcoef(f1tt_c[:,0],f2tt_c[:,0])[0,1] , np.corrcoef(f1_l[:,0,0],f2_l[:,0,0])[0,1])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PUTtSBsJJ5rX"},"outputs":[],"source":["def get_wrn_validation_features(f2):\n","  layer_name = f2.split(\"_CLASSIFIER_\")[1].split(\"_Validation\")[0]\n","  model_type = \"WideResNet\"\n","  num_of_models = 1\n","\n","  feature_files_used = [f2]\n","\n","  base_patterns_for_validations = [ f2.split(\"_Validation\")[0] ]\n","  # base_patterns_for_validations = get_base_patterns_for_validation(feature_files_used, layer_name, model_type)\n","\n","  np_x_validation_collab = None\n","  for base_val_str in base_patterns_for_validations:\n","      validation_batch_files = sorted([ ff for ff in wideresnets_features_files if base_val_str in ff and \"Validation\" in ff and \"_X\" in ff])\n","      # print(len(validation_batch_files))\n","      np_x_validation_collab_batch = np.array([np.load(ff) for ff in validation_batch_files])\n","      np_x_validation_collab_batch = np.concatenate(np_x_validation_collab_batch, axis=0)\n","      if np_x_validation_collab is None:\n","        np_x_validation_collab = np_x_validation_collab_batch.copy()\n","      else:\n","        np_x_validation_collab = np.concatenate([np_x_validation_collab, np_x_validation_collab_batch], axis=axis_to_concat)\n","\n","  return np_x_validation_collab\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sIpRyxRvbU09"},"outputs":[],"source":["# print(pca_f1.explained_variance_ratio_[0])\n","# pca_f1a = PCA(n_components=10)\n","# pca_f1a.fit(f1tt)\n","# print(pca_f1a.explained_variance_ratio_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4KvY8PACuz1l"},"outputs":[],"source":["def get_pca1_from_file_names(f1,f2,memoized_file_pca1s, add_perc=True):\n","  \n","  f1_l = None\n","  if f1 not in memoized_file_pca1s.keys():\n","    f1t = get_wrn_validation_features(f1) if \"WideResNet\" in f1 else np.load(f1) \n","    scaler1 = StandardScaler()\n","    scaler1.fit(f1t)\n","    f1tt = scaler1.transform(f1t)\n","    pca_f1 = PCA(n_components=1)\n","    pca_f1.fit(f1tt)\n","    f1_l = np.apply_along_axis(lambda x: np.dot(pca_f1.components_,np.expand_dims(x, axis=1)),1,f1tt)\n","    exp_var1 = pca_f1.explained_variance_ratio_.round(3)[0] if add_perc else None\n","    memoized_file_pca1s[f1] = (f1_l,exp_var1)\n","    del f1_t\n","    del pca_f1\n","    del f1tt\n","    del f1t\n","    del scaler1    \n","  f1_l,exp_var1 = memoized_file_pca1s[f1]\n","\n","  f2_l = None\n","  if f2 not in memoized_file_pca1s.keys():\n","    f2t = get_wrn_validation_features(f2) if \"WideResNet\" in f2 else np.load(f2)\n","    scaler2 = StandardScaler()\n","    scaler2.fit(f2t)\n","    f2tt = scaler2.transform(f2t)\n","    pca_f2 = PCA(n_components=1)\n","    pca_f2.fit(f2tt)\n","    f2_l = np.apply_along_axis(lambda x: np.dot(pca_f2.components_,np.expand_dims(x, axis=1)),1,f2tt)\n","    exp_var2 = pca_f2.explained_variance_ratio_.round(3)[0] if add_perc else None\n","    memoized_file_pca1s[f2] = (f2_l,exp_var2)\n","    del f2_t\n","    del pca_f2\n","    del f2tt\n","    del f2t\n","    del scaler2\n","  f2_l,exp_var2 = memoized_file_pca1s[f2]\n","\n","  return (np.corrcoef(f1_l[:,0,0],f2_l[:,0,0])[0,1], exp_var1, exp_var2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B8jkZQjB61mZ"},"outputs":[],"source":["def get_pca1_cca_from_file_names(f1,f2,memoized_file_pca1s, memoized_file_ccas, add_perc=True):\n","  \n","  f1_l = None\n","  f1tt = None\n","  if f1 not in memoized_file_pca1s.keys():\n","    f1t = get_wrn_validation_features(f1) if \"WideResNet\" in f1 else np.load(f1) \n","    scaler1 = StandardScaler()\n","    scaler1.fit(f1t)\n","    f1tt = scaler1.transform(f1t)\n","    pca_f1 = PCA(n_components=1)\n","    pca_f1.fit(f1tt)\n","    f1_l = np.apply_along_axis(lambda x: np.dot(pca_f1.components_,np.expand_dims(x, axis=1)),1,f1tt)\n","    exp_var1 = pca_f1.explained_variance_ratio_.round(3)[0] if add_perc else None\n","    memoized_file_pca1s[f1] = (f1_l,exp_var1, f1tt)\n","    del f1_t\n","    del pca_f1\n","    del f1t\n","    del scaler1    \n","  f1_l,exp_var1, f1tt = memoized_file_pca1s[f1]\n","\n","  f2_l = None\n","  f2tt = None\n","  if f2 not in memoized_file_pca1s.keys():\n","    f2t = get_wrn_validation_features(f2) if \"WideResNet\" in f2 else np.load(f2)\n","    scaler2 = StandardScaler()\n","    scaler2.fit(f2t)\n","    f2tt = scaler2.transform(f2t)\n","    pca_f2 = PCA(n_components=1)\n","    pca_f2.fit(f2tt)\n","    f2_l = np.apply_along_axis(lambda x: np.dot(pca_f2.components_,np.expand_dims(x, axis=1)),1,f2tt)\n","    exp_var2 = pca_f2.explained_variance_ratio_.round(3)[0] if add_perc else None\n","    memoized_file_pca1s[f2] = (f2_l,exp_var2, f2tt)\n","    del f2_t\n","    del pca_f2\n","    del f2t\n","    del scaler2\n","  f2_l,exp_var2, f2tt = memoized_file_pca1s[f2]\n","\n","  if (f1,f2) not in memoized_file_ccas.keys():\n","    ca = CCA()\n","    ca.fit(f1tt, f2tt)\n","    f1tt_c, f2tt_c = ca.transform(f1tt, f2tt)\n","    memoized_file_ccas[(f1,f2)] = np.corrcoef(f1tt_c[:,0],f2tt_c[:,0])[0,1]\n","  cca_f1f2 = memoized_file_ccas[(f1,f2)]\n","\n","  return (cca_f1f2 , np.corrcoef(f1_l[:,0,0],f2_l[:,0,0])[0,1], exp_var1, exp_var2 )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HSniHBhRJVor"},"outputs":[],"source":["def get_cca_from_file_names(f1,f2,memoized_file_ccas):\n","\n","  if (f1,f2) not in memoized_file_ccas.keys():\n","    f1t = get_wrn_validation_features(f1) if \"WideResNet\" in f1 else np.load(f1) \n","    scaler1 = StandardScaler()\n","    scaler1.fit(f1t)\n","    f1tt = scaler1.transform(f1t)\n","    \n","    f2t = get_wrn_validation_features(f1) if \"WideResNet\" in f2 else np.load(f2) \n","    scaler2 = StandardScaler()\n","    scaler2.fit(f2t)\n","    f2tt = scaler2.transform(f2t)\n","\n","    ca = CCA()\n","    ca.fit(f1tt, f2tt)\n","    f1tt_c, f2tt_c = ca.transform(f1tt, f2tt)\n","    memoized_file_ccas[(f1,f2)] = np.corrcoef(f1tt_c[:,0],f2tt_c[:,0])[0,1]\n","\n","    del f1t\n","    del f2t\n","    del scaler1\n","    del scaler2\n","    del f1tt\n","    del f2tt\n","    del f1tt_c\n","    del f2tt_c\n","    \n","  return memoized_file_ccas[(f1,f2)]\n"]},{"cell_type":"markdown","metadata":{"id":"MEY9GoMrsxYc"},"source":["### Create an empty summary DataFrame"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OTixLruYsxYc"},"outputs":[],"source":["cframe = pd.DataFrame(columns=[\"f1\",\"f2\", \"CCA\", \"PCA1\", \"PercPCA1F1\", \"PercPCA1F2\"])\n","\n","number_per_type = 6\n","\n","all_relevant_dnn_feature_files = []\n","for dnn_layer in available_dnn_feature_layers:\n","  all_relevant_dnn_feature_files.extend(sorted(random.sample(\n","      [ f for f in dnn_features_files if \"_Validation\" in f and \"_features_\" in f and \"_X\" in f and dnn_layer in f],2)))\n","\n","all_relevant_cnn_feature_files = []\n","for cnn_layer in available_cnn_feature_layers:\n","  all_relevant_cnn_feature_files.extend(sorted(random.sample(\n","      [ f for f in cnn_features_files if \"_Validation\" in f and \"_features_\" in f and \"_X\" in f and cnn_layer in f],2)))\n","\n","all_relevant_wrn_feature_files = []\n","for wrn_layer in available_wrn_feature_layers:\n","  all_relevant_wrn_feature_files.extend(sorted(random.sample(\n","      [ f for f in wideresnets_features_files if \"_Validation\" in f and \"_features_\" in f and \"_X\" in f and wrn_layer in f],number_per_type)))\n","\n","all_relevant_feature_files = all_relevant_dnn_feature_files + all_relevant_cnn_feature_files + all_relevant_wrn_feature_files\n","\n","for xfile in all_relevant_feature_files:\n","  cframe = pd.concat([cframe,pd.DataFrame({\"f1\":[xfile]*len(all_relevant_feature_files), \"f2\": all_relevant_feature_files, \n","                                            \"CCA\": np.nan, \"PCA1\": np.nan, \"PercPCA1F1\" : np.nan, \"PercPCA1F2\" : np.nan  })])      \n","cframe = cframe.drop_duplicates()\n","cframe.loc[ (cframe.f1==cframe.f2), [\"PCA1\"]] = [1.0]\n","cframe.loc[ (cframe.f1==cframe.f2), [\"CCA\"]] = [1.0]\n","\n","# causes a double log in corr matrix\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ib2_MyjvwRus"},"outputs":[],"source":["cframe = cframe.reset_index()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PM1qKukjocnM"},"outputs":[],"source":["print(cframe.columns.values)\n","print(cframe.shape)\n","print(cframe.index)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c2fsTzarpRvm"},"outputs":[],"source":["memoized_file_pca1s = {}\n","memoized_file_ccas = {}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WRiJMqM4lRiZ"},"outputs":[],"source":["import datetime\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","from progressbar import ProgressBar\n","pbar = ProgressBar()\n","\n","for rc in pbar(cframe.index):\n","  xfile = cframe.loc[rc, 'f1']\n","  yfile = cframe.loc[rc, 'f2']\n","  if xfile != yfile:\n","    try:\n","      cframe.loc[rc, [\"PCA1\",\"PercPCA1F1\",\"PercPCA1F2\"]]  = get_pca1_from_file_names(xfile, yfile, memoized_file_pca1s)     \n","      cframe.loc[rc, [\"CCA\"]]  = get_cca_from_file_names(xfile,yfile,memoized_file_ccas)\n","      if (rc % 40) == 0:\n","        print(rc)\n","    except:\n","      print(xfile)\n","      print(yfile)\n","      pass\n","      \n","    # print(rc,cframe.loc[rc, \"PCA1\"])      \n","\n","cframe.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_ccs/all_Validation_ccs_{datetime.datetime.now():%Y%m%d%H%M%S}.csv\",index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QtduTw9C7OC-"},"outputs":[],"source":["import os\n","os.listdir(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_ccs/\")"]},{"cell_type":"code","source":["# cframe\n","cframe.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_ccs/all_Validation_ccs_{datetime.datetime.now():%Y%m%d%H%M%S}.csv\",index=False)\n"],"metadata":{"id":"ThVNj_-S4O26"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j3PDqCkYx5r_"},"outputs":[],"source":["print(cframe.loc[2, 'f1'])\n","print(cframe.loc[2, 'f2'])\n","print(memoized_file_pca1s[cframe.loc[2, 'f1']][:,0,0])\n","print(memoized_file_pca1s[cframe.loc[2, 'f2']][:,0,0])\n","\n","# np.corrcoef(memoized_file_pca1s[cframe.loc[2, 'f1']][:,0,0],memoized_file_pca1s[cframe.loc[2, 'f2']][:,0,0]) # [0,1]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vuN3skzK-fAy"},"outputs":[],"source":["import random\n","import re\n","\n","def get_base_patterns_for_validation(features_files, layer_name = \"CLASSIFIER_D1\", model_type = \"WideResNet\"):\n","  base_patterns_for_validations = []\n","  for ff in [ s for s in features_files if \"Validation\" in s]:\n","    validation_search = re.search(f'^.*({model_type}.*_ID.*features_{layer_name}_Validation)[0-9]+.*$', ff, re.IGNORECASE)\n","    if validation_search:\n","        base_patterns_for_validations.append(validation_search.group(1))\n","\n","  base_patterns_for_validations = sorted(list(set(base_patterns_for_validations)))\n","  return base_patterns_for_validations\n","\n","\n","def get_validation_features_for_layer_using_subbatches(feature_files, layer_name, num_of_models, model_type=\"WideResNet\", axis_to_concat = 1):\n"," \n","  feature_files_used = [ff for ff in feature_files if \"Validation\" in ff and \"_X\" in ff and f\"_{layer_name}_\" in ff ]\n","  \n","  # base_patterns_for_validations = []\n","  # for ff in [ s for s in feature_files_used ]:\n","  #   validation_search = re.search(f'^.*(_ID.*features_{layer_name}_Validation)[0-9]+.*$', ff, re.IGNORECASE)\n","  #   if validation_search:\n","  #       base_patterns_for_validations.append(validation_search.group(1))\n","  # base_patterns_for_validations = sorted(list(set(base_patterns_for_validations)))\n","\n","  base_patterns_for_validations = get_base_patterns_for_validation(feature_files_used, layer_name, model_type)\n","  base_patterns_for_validations = sorted(random.sample(base_patterns_for_validations, min(num_of_models,len(base_patterns_for_validations)) ))\n","\n","  np_x_validation_collab = None\n","  for base_val_str in base_patterns_for_validations:\n","      validation_batch_files = sorted([ ff for ff in feature_files_used if base_val_str in ff])\n","      np_x_validation_collab_batch = np.array([np.load(ff) for ff in validation_batch_files])\n","      np_x_validation_collab_batch = np.concatenate(np_x_validation_collab_batch, axis=0)\n","      if np_x_validation_collab is None:\n","        np_x_validation_collab = np_x_validation_collab_batch.copy()\n","      else:\n","        np_x_validation_collab = np.concatenate([np_x_validation_collab, np_x_validation_collab_batch], axis=axis_to_concat)\n","  \n","  return np_x_validation_collab  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fxjZPT2gsxYc"},"outputs":[],"source":["# import datetime \n","\n","# import warnings\n","# warnings.filterwarnings(\"ignore\")\n","\n","# for dnn_layer in available_dnn_feature_layers:\n","#   all_relevant_feature_files =  [ f for f in dnn_features_files if \"_Validation\" in f and \"_features_\" in f and \"_X\" in f and dnn_layer in f]\n","#   for xfileCount, xfile in enumerate(all_relevant_feature_files):\n","#     for yfile in all_relevant_feature_files:\n","#       if xfile != yfile:\n","#         if np.isnan(cframe.loc[(cframe.f1==xfile) & (cframe.f2 == yfile),\"CCA1\"].iloc[0]):          \n","#           cca1_val, pca1_val = get_cca1_pca1_from_file_names(xfile, yfile)          \n","#           cframe.loc[ (cframe.f1==xfile) & (cframe.f2 == yfile), [\"CCA1\",\"PCA1\"] ] = [ cca1_val, pca1_val ] \n","#           cframe.loc[ (cframe.f1==yfile) & (cframe.f2 == xfile),[\"CCA1\",\"PCA1\"] ] = [ cca1_val, pca1_val ] \n","#     print(f\"DNN {xfileCount}/{len(all_relevant_feature_files)} {dnn_layer}\")\n","\n","\n","# cframe.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_ccs/dnn_Validation_temp_ccs_{datetime.datetime.now():%Y%m%d%H%M%S}.csv\",index=False)\n","\n","\n","# for cnn_layer in available_cnn_feature_layers:\n","#   all_relevant_feature_files =  [ f for f in cnn_features_files if \"_Validation\" in f and \"_features_\" in f and \"_X\" in f and cnn_layer in f]\n","#   for xfileCount, xfile in enumerate(all_relevant_feature_files):\n","#     for yfile in all_relevant_feature_files:\n","#       if xfile != yfile:\n","#         if np.isnan(cframe.loc[(cframe.f1==xfile) & (cframe.f2 == yfile),\"CCA1\"].iloc[0]):          \n","#           cca1_val, pca1_val = get_cca1_pca1_from_file_names(xfile, yfile)          \n","#           cframe.loc[ (cframe.f1==xfile) & (cframe.f2 == yfile), [\"CCA1\",\"PCA1\"] ] = [ cca1_val, pca1_val ] \n","#           cframe.loc[ (cframe.f1==yfile) & (cframe.f2 == xfile),[\"CCA1\",\"PCA1\"] ] = [ cca1_val, pca1_val ] \n","#     print(f\"CNN {xfileCount}/{len(all_relevant_feature_files)} {cnn_layer}\")\n","\n","\n","# cframe.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_ccs/cnn_Validation_ccs_{datetime.datetime.now():%Y%m%d%H%M%S}.csv\",index=False)\n","\n","\n","# warnings.resetwarnings()"]},{"cell_type":"markdown","metadata":{"id":"XtjoNp7qsxYd"},"source":["### Select the most distant DNN/CNN validation that still make some sense"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3h4Kt3rJsxYd"},"outputs":[],"source":["# cframe.loc[cframe.CCA1==1.0][\"f1\"]\n","\n","import os,sys\n","import numpy as np\n","import pandas as pd\n","\n","dnn_distresults = pd.read_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_ccs/dnn_Validation_ccs_20211115040852.csv\")\n","cnn_distresults = pd.read_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_ccs/cnn_Validation_ccs_20211115184258.csv\")\n","\n","dnn_distresults = dnn_distresults[dnn_distresults.Type==\"DNN\"]\n","# individual_dnns = np.load(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/individual_dnn_summary_20211014152626.npy\")\n","# individual_cnns = np.load(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/individual_cnn_summary_20211014152108.npy\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w7qMZNmasxYd"},"outputs":[],"source":["# cnn_distresults.sort_values(\"CCA1\")\n","# cnn_distresults[cnn_distresults.Layer == \"SFTMX1\"].sort_values(\"PCA1\")\n","# cnn_distresults[cnn_distresults.Layer == \"SFTMX1\"].sort_values(\"CCA1\")[0:40]\n","# dnn_distresults[dnn_distresults.Layer == \"SFTMX1\"].sort_values(\"CCA1\")\n","\n","cnn_avg_cca = cnn_distresults.groupby([\"Type\",\"Layer\",\"f1\"])[\"CCA1\"].mean().reset_index()\n","dnn_avg_cca = dnn_distresults.groupby([\"Type\",\"Layer\",\"f1\"])[\"CCA1\"].mean().reset_index()\n","cnn_avg_cca = cnn_avg_cca.sort_values('CCA1')\n","dnn_avg_cca = pd.concat([dnn_avg_cca[dnn_avg_cca.CCA1 > 0.45].sort_values('CCA1'), dnn_avg_cca[dnn_avg_cca.CCA1 <= 0.45]])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yfwMt8ZrsxYd"},"outputs":[],"source":["cnn_avg_pca = cnn_distresults.groupby([\"Type\",\"Layer\",\"f1\"])[\"PCA1\"].apply(lambda x: np.mean(np.abs(x))).reset_index()\n","dnn_avg_pca = dnn_distresults.groupby([\"Type\",\"Layer\",\"f1\"])[\"PCA1\"].apply(lambda x: np.mean(np.abs(x))).reset_index()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7cAF5i8WsxYe"},"outputs":[],"source":["# ggplot(dnn_avg_pca) + geom_density(aes('PCA1'))\n","# ggplot(dnn_avg_cca) + geom_density(aes('CCA1'))\n","# ggplot(cnn_avg_cca) + geom_density(aes('CCA1'))\n","\n","\n","\n","# start going through it pairwise"]},{"cell_type":"markdown","metadata":{"id":"8GvmAzHIp7te"},"source":["### Create an empty summary DataFrame"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jBsx6oAbp-Bf"},"outputs":[],"source":["cframe = pd.DataFrame(columns=[\"f1\",\"f2\",\"CCA1\",\"PCA1\",\"Type\",\"Layer\"])\n","\n","for dnn_layer in available_dnn_feature_layers:\n","  all_relevant_feature_files =  [ f for f in dnn_features_files if \"_Validation\" in f and \"_features_\" in f and \"_X\" in f and dnn_layer in f]\n","  for xfile in all_relevant_feature_files:\n","    cframe = pd.concat([cframe,pd.DataFrame({\"f1\":[xfile]*len(all_relevant_feature_files), \"f2\": all_relevant_feature_files, \"CCA1\": np.nan, \"PCA1\": np.nan, \"Type\": \"DNN\", \"Layer\": dnn_layer  })])      \n","\n","for cnn_layer in available_cnn_feature_layers:\n","  all_relevant_feature_files =  [ f for f in cnn_features_files if \"_Validation\" in f and \"_features_\" in f and \"_X\" in f and cnn_layer in f]\n","  for xfile in all_relevant_feature_files:\n","    cframe = pd.concat([cframe,pd.DataFrame({\"f1\":[xfile]*len(all_relevant_feature_files), \"f2\": all_relevant_feature_files, \"CCA1\": np.nan, \"PCA1\": np.nan, \"Type\": \"CNN\", \"Layer\": cnn_layer  })])      \n","\n","cframe = cframe.drop_duplicates()\n","cframe.loc[ (cframe.f1==cframe.f2), [\"CCA1\",\"PCA1\"]] = [1.0,1.0]\n","\n","# causes a double log in corr matrix\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e96k_d6dqynr"},"outputs":[],"source":["import datetime \n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","for dnn_layer in available_dnn_feature_layers:\n","  all_relevant_feature_files =  [ f for f in dnn_features_files if \"_Validation\" in f and \"_features_\" in f and \"_X\" in f and dnn_layer in f]\n","  for xfileCount, xfile in enumerate(all_relevant_feature_files):\n","    for yfile in all_relevant_feature_files:\n","      if xfile != yfile:\n","        if np.isnan(cframe.loc[(cframe.f1==xfile) & (cframe.f2 == yfile),\"CCA1\"].iloc[0]):          \n","          cca1_val, pca1_val = get_cca1_pca1_from_file_names(xfile, yfile)          \n","          cframe.loc[ (cframe.f1==xfile) & (cframe.f2 == yfile), [\"CCA1\",\"PCA1\"] ] = [ cca1_val, pca1_val ] \n","          cframe.loc[ (cframe.f1==yfile) & (cframe.f2 == xfile),[\"CCA1\",\"PCA1\"] ] = [ cca1_val, pca1_val ] \n","    print(f\"DNN {xfileCount}/{len(all_relevant_feature_files)} {dnn_layer}\")\n","\n","\n","cframe.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_ccs/dnn_Validation_temp_ccs_{datetime.datetime.now():%Y%m%d%H%M%S}.csv\",index=False)\n","\n","\n","for cnn_layer in available_cnn_feature_layers:\n","  all_relevant_feature_files =  [ f for f in cnn_features_files if \"_Validation\" in f and \"_features_\" in f and \"_X\" in f and cnn_layer in f]\n","  for xfileCount, xfile in enumerate(all_relevant_feature_files):\n","    for yfile in all_relevant_feature_files:\n","      if xfile != yfile:\n","        if np.isnan(cframe.loc[(cframe.f1==xfile) & (cframe.f2 == yfile),\"CCA1\"].iloc[0]):          \n","          cca1_val, pca1_val = get_cca1_pca1_from_file_names(xfile, yfile)          \n","          cframe.loc[ (cframe.f1==xfile) & (cframe.f2 == yfile), [\"CCA1\",\"PCA1\"] ] = [ cca1_val, pca1_val ] \n","          cframe.loc[ (cframe.f1==yfile) & (cframe.f2 == xfile),[\"CCA1\",\"PCA1\"] ] = [ cca1_val, pca1_val ] \n","    print(f\"CNN {xfileCount}/{len(all_relevant_feature_files)} {cnn_layer}\")\n","\n","\n","cframe.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_ccs/cnn_Validation_ccs_{datetime.datetime.now():%Y%m%d%H%M%S}.csv\",index=False)\n","\n","\n","warnings.resetwarnings()"]},{"cell_type":"markdown","metadata":{"id":"kIiJPc3OqE-V"},"source":["### Select the most distant DNN/CNN validation that still make some sense"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Lfl0MK_UdlL"},"outputs":[],"source":["# cframe.loc[cframe.CCA1==1.0][\"f1\"]\n","\n","import os,sys\n","import numpy as np\n","import pandas as pd\n","\n","dnn_distresults = pd.read_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_ccs/dnn_Validation_ccs_20211115040852.csv\")\n","cnn_distresults = pd.read_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_ccs/cnn_Validation_ccs_20211115184258.csv\")\n","\n","dnn_distresults = dnn_distresults[dnn_distresults.Type==\"DNN\"]\n","# individual_dnns = np.load(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/individual_dnn_summary_20211014152626.npy\")\n","# individual_cnns = np.load(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/individual_cnn_summary_20211014152108.npy\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H1HIOBeUz-SL"},"outputs":[],"source":["# cnn_distresults.sort_values(\"CCA1\")\n","# cnn_distresults[cnn_distresults.Layer == \"SFTMX1\"].sort_values(\"PCA1\")\n","# cnn_distresults[cnn_distresults.Layer == \"SFTMX1\"].sort_values(\"CCA1\")[0:40]\n","# dnn_distresults[dnn_distresults.Layer == \"SFTMX1\"].sort_values(\"CCA1\")\n","\n","cnn_avg_cca = cnn_distresults.groupby([\"Type\",\"Layer\",\"f1\"])[\"CCA1\"].mean().reset_index()\n","dnn_avg_cca = dnn_distresults.groupby([\"Type\",\"Layer\",\"f1\"])[\"CCA1\"].mean().reset_index()\n","cnn_avg_cca = cnn_avg_cca.sort_values('CCA1')\n","dnn_avg_cca = pd.concat([dnn_avg_cca[dnn_avg_cca.CCA1 > 0.45].sort_values('CCA1'), dnn_avg_cca[dnn_avg_cca.CCA1 <= 0.45]])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BWR5WCGCgJ5_"},"outputs":[],"source":["cnn_avg_pca = cnn_distresults.groupby([\"Type\",\"Layer\",\"f1\"])[\"PCA1\"].apply(lambda x: np.mean(np.abs(x))).reset_index()\n","dnn_avg_pca = dnn_distresults.groupby([\"Type\",\"Layer\",\"f1\"])[\"PCA1\"].apply(lambda x: np.mean(np.abs(x))).reset_index()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mCP0hSiL1NR2"},"outputs":[],"source":["# ggplot(dnn_avg_pca) + geom_density(aes('PCA1'))\n","# ggplot(dnn_avg_cca) + geom_density(aes('CCA1'))\n","# ggplot(cnn_avg_cca) + geom_density(aes('CCA1'))\n","\n","\n","\n","# start going through it pairwise"]},{"cell_type":"markdown","metadata":{"id":"-nbwCVHSRRfM"},"source":["# Parallel model with no GP and no collaborative but with different parallel DNN/CNN/WideResnet streams. The _best_ parallel fully integrated model."]},{"cell_type":"markdown","metadata":{"id":"-ntNI2qC_9Hm"},"source":["function for basic parallel 20 DNN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kXWOr1f4RQbd"},"outputs":[],"source":["def basic_ParallelDNN_A(model_name, inshape, num_classes = 10, num_of_parallels = 20):\n","\n","  base_input = Input(shape=inshape, name='base_input')\n","  f1_output = Flatten(name='F1')(base_input)\n","\n","  d3rs = []\n","  for pc in range(num_of_parallels):\n","    d1_output = Dense(128, activation='relu', name = f'p{pc+1}_D1')(f1_output)\n","    d2_output = Dense(128, activation='relu', name = f'p{pc+1}_D2')(d1_output)\n","    d3r_output = Dense(48, activation='relu', kernel_regularizer=tf.keras.regularizers.l1(0.005), name = f'p{pc+1}_D3R')(d2_output)\n","    d3rs.append(d3r_output)\n","\n","  # merging_layer = tf.keras.layers.concatenate(d3rs)\n","  concat1_output = Concatenate(name=\"CONCAT1\")(d3rs)\n","  model_output = Dense(num_classes,  activation='softmax', name = 'SFTMX1')(concat1_output)\n","  model = Model(base_input, model_output, name = model_name)\n","\n","  return model"]},{"cell_type":"markdown","metadata":{"id":"CdiHdMZbAzHK"},"source":["function for basic parallel 20 CNN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"inikwtpIW4M3"},"outputs":[],"source":["def basic_ParallelCNN_A(model_name, inshape, num_classes = 10, num_of_parallels = 20):\n","\n","  # Input Layer\n","  base_input = Input(shape=inshape, name='base_input')\n","\n","  drp1s = []\n","  for pc in range(num_of_parallels):\n","    c1_output = Conv2D(filters=32,kernel_size=[5, 5],padding=\"same\",activation=\"relu\", name=f\"p{pc+1}_C1\")(base_input)\n","    mxp1_output = MaxPooling2D(pool_size=[2, 2], strides=2, name=f\"p{pc+1}_MXP1\")(c1_output)\n","    c2_output = Conv2D(filters=64,kernel_size=[5, 5],padding=\"same\",activation=\"relu\",name=f\"p{pc+1}_C2\")(mxp1_output)\n","    mxp2_output = MaxPooling2D(pool_size=[2, 2], strides=2, name=f\"p{pc+1}_MXP2\")(c2_output)\n","    f1_output = Flatten(name=f\"p{pc+1}_F1\")(mxp2_output)\n","    d1_output = Dense(units=256, activation=\"relu\", name=f\"p{pc+1}_D1\")(f1_output)\n","    drp1_output = Dropout(rate=0.5, name=f\"p{pc+1}_DRP1\")(d1_output)\n","    drp1s.append(drp1_output)\n","\n","  concat1_output = Concatenate(name=\"CONCAT1\")(drp1s)\n","  model_output = Dense(num_classes,  activation='softmax', name = 'SFTMX1')(concat1_output)\n","  model = Model(base_input, model_output, name = model_name)\n","  return model\n"]},{"cell_type":"markdown","metadata":{"id":"PJEQRsKaA2fq"},"source":["function for basic parallel 10 CNN + 10 DNN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EFDcerJycgBz"},"outputs":[],"source":["def basic_ParallelCNNDNN_A(model_name, inshape, num_classes = 10, num_of_parallels = 20):\n","\n","  # Input Layer\n","  base_input = Input(shape=inshape, name='base_input')\n","\n","  to_merge = []\n","  for pc in range(int(num_of_parallels/2)):\n","    c1_output = Conv2D(filters=32,kernel_size=[5, 5],padding=\"same\",activation=\"relu\", name=f\"p{pc+1}_C1_CNN\")(base_input)\n","    mxp1_output = MaxPooling2D(pool_size=[2, 2], strides=2, name=f\"p{pc+1}_MXP1_CNN\")(c1_output)\n","    c2_output = Conv2D(filters=64,kernel_size=[5, 5],padding=\"same\",activation=\"relu\",name=f\"p{pc+1}_C2_CNN\")(mxp1_output)\n","    mxp2_output = MaxPooling2D(pool_size=[2, 2], strides=2, name=f\"p{pc+1}_MXP2_CNN\")(c2_output)\n","    f1_output = Flatten(name=f\"p{pc+1}_F1_CNN\")(mxp2_output)\n","    d1_output = Dense(units=256, activation=\"relu\", name=f\"p{pc+1}_D1_CNN\")(f1_output)\n","    drp1_output = Dropout(rate=0.5, name=f\"p{pc+1}_DRP1_CNN\")(d1_output)\n","    to_merge.append(drp1_output)\n","\n","  for pc in range(int(num_of_parallels/2)):\n","    d1_output = Dense(128, activation='relu', name = f'p{pc+1}_D1_DNN')(f1_output)\n","    d2_output = Dense(128, activation='relu', name = f'p{pc+1}_D2_DNN')(d1_output)\n","    d3r_output = Dense(48, activation='relu', kernel_regularizer=tf.keras.regularizers.l1(0.005), name = f'p{pc+1}_D3R_DNN')(d2_output)\n","    to_merge.append(d3r_output)\n","\n","  concat1_output = Concatenate(name=\"CONCAT1\")(to_merge)\n","  model_output = Dense(num_classes,  activation='softmax', name = 'SFTMX1')(concat1_output)\n","  model = Model(base_input, model_output, name = model_name)\n","  return model\n"]},{"cell_type":"markdown","metadata":{"id":"Gg49OZj4Htvb"},"source":["function for basic parallel 10 WideResnet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HitZBCmym3Tp"},"outputs":[],"source":["# input_shape=train_data_grey[0,:,:,:].shape\n","import functools\n","\n","def basic_ParallelWideResNet2810_A(model_name, inshape, num_classes = 10, num_of_parallels = 10,\n","                                   weight_decay = 0.0005, weight_init=\"he_normal\",\n","                                   depth=28, k = 10, dropout_probability = 0.0, channel_axis = -1,\n","                                   use_bias = False):\n","\n","  # Input Layer\n","  base_input = Input(shape=inshape, name='base_input')\n","\n","  n = (depth - 4) / 6\n","\n","  to_merge = []\n","  for pc in range(num_of_parallels):\n","    # inputs_wrn = Input(shape=input_shape,name=f\"P{str(pc)}_INPUT\")\n","    n_stages=[16, 16*k, 32*k, 64*k]\n","    conv1_wrn = Conv2D(16, \n","                    (3, 3), \n","                    strides=1,\n","                    padding=\"same\",\n","                    kernel_initializer=weight_init,\n","                    kernel_regularizer=L2(weight_decay),\n","                    use_bias=use_bias,\n","                    name=f\"P{str(pc)}_C1BLOCK\")(base_input) # \"One conv at the beginning (spatial size: 32x32)\"\n","    # Add wide residual blocks\n","    block_fn = _wide_basic\n","    conv2_wrn = _layer(functools.partial(block_fn,\n","                                         identifier=f\"P{str(pc)}_C2BLOCK\",\n","                                          channel_axis = channel_axis,\n","                                          weight_decay = weight_decay,\n","                                          weight_init= weight_init,\n","                                          use_bias = use_bias,\n","                                          dropout_probability = dropout_probability\n","                                         ), \n","                                         n_input_plane=n_stages[0], n_output_plane=n_stages[1], count=n, stride=(1,1)\n","                       )(conv1_wrn)# \"Stage 1 (spatial size: 32x32)\"\n","    conv3_wrn = _layer(functools.partial(block_fn,identifier=f\"P{str(pc)}_C3BLOCK\",\n","                                          channel_axis = channel_axis,\n","                                          weight_decay = weight_decay,\n","                                          weight_init= weight_init,\n","                                          use_bias = use_bias,\n","                                          dropout_probability = dropout_probability\n","                                         ), n_input_plane=n_stages[1], n_output_plane=n_stages[2], count=n, stride=(2,2)\n","                       )(conv2_wrn)# \"Stage 2 (spatial size: 16x16)\"\n","    conv4_wrn = _layer(functools.partial(block_fn,identifier=f\"P{str(pc)}_C4BLOCK\",\n","                                          channel_axis = channel_axis,\n","                                          weight_decay = weight_decay,\n","                                          weight_init= weight_init,\n","                                          use_bias = use_bias,\n","                                          dropout_probability = dropout_probability\n","                                         ), n_input_plane=n_stages[2], n_output_plane=n_stages[3], count=n, stride=(2,2)\n","                       )(conv3_wrn)# \"Stage 3 (spatial size: 8x8)\"\n","\n","    batch_norm_wrn = BatchNormalization(axis=channel_axis,name=f\"P{str(pc)}_BN\")(conv4_wrn)\n","    relu_wrn = Activation(\"relu\")(batch_norm_wrn)\n","                                            \n","    # Classifier block\n","    pool_wrn = AveragePooling2D(pool_size=(8, 8), strides=(1, 1), padding=\"same\", name=f\"P{str(pc)}_CLASSIFIER_AVPL\")(relu_wrn)\n","    flatten_wrn = Flatten(name=f\"P{str(pc)}_CLASSIFIER_FL\")(pool_wrn)\n","\n","    to_merge.append(flatten_wrn)\n","\n","    # predictions_wrn = Dense(units=no_classes, kernel_initializer=weight_init, use_bias=use_bias,\n","    #                     kernel_regularizer=L2(weight_decay), activation=\"softmax\", name=\"P{str(pc)}_CLASSIFIER_D1\")(flatten_wrn)\n","    # model_wrn = Model(inputs=inputs_wrn, outputs=predictions_wrn)\n","\n","  concat1_output = Concatenate(name=\"CONCAT1\")(to_merge)\n","  model_output = Dense(num_classes,  activation='softmax', name = 'SFTMX1')(concat1_output)\n","  model = Model(base_input, model_output, name = model_name)\n","\n","  return model\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kn-syuLCXyKh"},"outputs":[],"source":["import functools\n","from functools import partial"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qmOuqvetypnO"},"outputs":[],"source":[" # parallel fuly integrated 20 DNN\n"," \n"," start_time = timeit.default_timer()\n"," pdnn20, pdnn20h = compile_and_fit_model_basic( partial(basic_ParallelDNN_A,num_of_parallels=20),  \n","                    f\"ParallelDNN_A_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","                    train_data_grey[0,:,:,:].shape, \n","                    train_data_grey, train_targets,\n","                    save_max_epoch=False,\n","                    save_final=True,\n","                    patience_count = 35,\n","                    early_stopping_obs = 'val_sparse_categorical_accuracy',\n","                    log_history = True,\n","                    verbose_level = 0,                             \n","                    batch_size=512, \n","                    epochs=250, \n","                    class_weight=None, \n","                    validation_data=(validation_data_grey, validation_targets))\n","print(timeit.default_timer()-start_time)\n","\n","# /content/drive/MyDrive/data_papers/gpSVHN/model_finals/ParallelDNN_A_20211002170140_saved_model_after_fit/assets  # 88% validation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m7W5a2D_9GsK"},"outputs":[],"source":["# parallel fuly integrated 20 CNN\n","\n","start_time = timeit.default_timer()\n","pcnn20, pcnn20h = compile_and_fit_model_basic( partial(basic_ParallelCNN_A, num_of_parallels=20),  \n","                    f\"ParallelCNN_A_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","                    train_data_grey[0,:,:,:].shape, \n","                    train_data_grey, train_targets,\n","                    save_max_epoch=False,\n","                    save_final=True,\n","                    patience_count = 35,\n","                    early_stopping_obs = 'val_sparse_categorical_accuracy',\n","                    log_history = True,\n","                    verbose_level = 0,                             \n","                    batch_size=512, \n","                    epochs=250, \n","                    class_weight=None, \n","                    validation_data=(validation_data_grey, validation_targets))\n","print(timeit.default_timer()-start_time)\n"," \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K1bRQHOu9PjY"},"outputs":[],"source":["# parallel fuly integrated 10 DNN + 10 CNN\n"," \n","start_time = timeit.default_timer()\n","pdcnn20, pdcnn20h = compile_and_fit_model_basic( partial(basic_ParallelCNNDNN_A,num_of_parallels=20),  \n","                    f\"ParallelCNNDNN_A_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","                    train_data_grey[0,:,:,:].shape, \n","                    train_data_grey, train_targets,\n","                    save_max_epoch=False,\n","                    save_final=True,\n","                    patience_count = 35,\n","                    early_stopping_obs = 'val_sparse_categorical_accuracy',\n","                    log_history = True,\n","                    verbose_level = 0,                             \n","                    batch_size=512, \n","                    epochs=250, \n","                    class_weight=None, \n","                    validation_data=(validation_data_grey, validation_targets))\n","print(timeit.default_timer()-start_time)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KSTlw2v-7RDr"},"outputs":[],"source":["# parallel fuly integrated 10 WideResNet \n","# does not work!!! OOM\n","# pwrs10, pwrs10h = compile_and_fit_model_basic( basic_ParallelWideResNet2810_A,  \n","#                     f\"ParallelWideResNet_A_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","#                     train_data_grey[0,:,:,:].shape, \n","#                     train_data_grey, train_targets,\n","#                     save_max_epoch=False,\n","#                     save_final=True,\n","#                     patience_count = 35,\n","#                     early_stopping_obs = 'val_sparse_categorical_accuracy',\n","#                     log_history = True,\n","#                     verbose_level = 1,                             \n","#                     batch_size=512, \n","#                     epochs=250, \n","#                     class_weight=None, \n","#                     validation_data=(validation_data_grey, validation_targets))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SRolnh9r_bcK"},"outputs":[],"source":["plot_history(pdnn20h)\n","plot_history(pcnn20h)\n","plot_history(pdcnn20h)\n","plot_history(pwrs10h)"]},{"cell_type":"markdown","metadata":{"id":"eDrWFkawP38Q"},"source":[" the summary results for the fully parallel models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KWHWnbJj_2kH"},"outputs":[],"source":["x_input = test_data_grey\n","y_input = test_targets\n","\n","models_to_use = [pdnn20, pcnn20, pdcnn20, pwrs10]\n","model_predictions = [ model.predict(x_input) for model in models_to_use]\n","\n","y_preds = [ np.apply_along_axis(np.argmax, 1, y_pred_model) for y_pred_model in model_predictions ] \n","scores_pdnn20_pcnn20_pdcnn20 = [ pr_rc_f1_acc_from_supplied(y_pred,y_input) for y_pred in y_preds ]\n","\n","np.save(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/full_parallel_dnn20_cnn20_cnn10dnn10_wrs10_summary_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","        np.array(scores_pdnn20_pcnn20_pdcnn20), \n","        allow_pickle=True, \n","        fix_imports=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pE8Hb3aiWxjG"},"outputs":[],"source":["scores_pdnn20_pcnn20_pdcnn20"]},{"cell_type":"markdown","metadata":{"id":"1_Os-eyfeYfE"},"source":["# Functions for collaborative learning (no GP) based on last dense and softmax layer features for DNN/CNN/WideResNet inputs"]},{"cell_type":"markdown","metadata":{"id":"bDIJ2mhv525M"},"source":["A sequential model for DNN on collaborative features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"giePq9Q2189i"},"outputs":[],"source":["def model_combination_of_features(model_name, input_shape,num_classes=10):\n","    model = Sequential([\n","        tf.keras.Input(shape=input_shape),\n","        BatchNormalization(),\n","        Dense(256, kernel_initializer='RandomNormal', bias_initializer='zeros'),\n","        LeakyReLU(),\n","        Dropout(0.6),\n","        Dense(128, kernel_initializer='RandomNormal', bias_initializer='zeros', kernel_regularizer = tf.keras.regularizers.l1(1e-3)),\n","        LeakyReLU(),\n","        Dropout(0.6),\n","        Dense(32, kernel_initializer='RandomNormal', bias_initializer='zeros', kernel_regularizer = tf.keras.regularizers.l1(1e-2)),\n","        LeakyReLU(),\n","        Dropout(0.5),\n","        Dense(num_classes, activation='softmax')\n","    ], name=model_name)\n","    return model\n","\n","\n","def model_combination_of_features_with_flatten(model_name, input_shape,num_classes=10):\n","    model = Sequential([\n","        tf.keras.Input(shape=input_shape),\n","        Flatten(),\n","        BatchNormalization(),\n","        Dense(256, kernel_initializer='RandomNormal', bias_initializer='zeros'),\n","        LeakyReLU(),\n","        Dropout(0.6),\n","        Dense(128, kernel_initializer='RandomNormal', bias_initializer='zeros', kernel_regularizer = tf.keras.regularizers.l1(1e-3)),\n","        LeakyReLU(),\n","        Dropout(0.6),\n","        Dense(32, kernel_initializer='RandomNormal', bias_initializer='zeros', kernel_regularizer = tf.keras.regularizers.l1(1e-2)),\n","        LeakyReLU(),\n","        Dropout(0.5),\n","        Dense(num_classes, activation='softmax')\n","    ], name=model_name)\n","    return model    \n","\n","\n","# X_trains_out.append(np.array(features_model.predict(X_train_combined), dtype='float64'))\n","# X_train_new = np.concatenate(tuple(X_trains_out), axis=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rV8G8UWixd_l"},"outputs":[],"source":["def get_features_from_multiple_models(feature_files, axis_to_concat = 1):\n"," \n","  validation_features_to_load = [ff for ff in feature_files if \"Validation\" in ff and \"_X\" in ff ]\n","  np_x_validation_collab = np.array([np.load(ff) for ff in validation_features_to_load])\n","  np_x_validation_collab = np.concatenate(np_x_validation_collab, axis=axis_to_concat)\n","\n","  train_features_to_load = [ff.replace(\"Validation\", \"Train\") for ff in validation_features_to_load]\n","  np_x_train_collab = np.array([np.load(ff) for ff in train_features_to_load])\n","  np_x_train_collab = np.concatenate(np_x_train_collab, axis=axis_to_concat)\n","\n","  test_features_to_load = [ff.replace(\"Validation\", \"Test\") for ff in validation_features_to_load]\n","  np_x_test_collab = np.array([np.load(ff) for ff in test_features_to_load])\n","  np_x_test_collab = np.concatenate(np_x_test_collab, axis=axis_to_concat)\n","\n","  return np_x_validation_collab, np_x_train_collab, np_x_test_collab"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0PaQyNjIsR97"},"outputs":[],"source":["def get_features_for_layer(feature_files, layer_name, num_of_models, axis_to_concat = 1):\n"," \n","  feature_files_used = [ff for ff in feature_files if \"Validation\" in ff and \"_X\" in ff and f\"{layer_name}_\" in ff ]\n","  validation_features_to_load = sorted(random.sample(feature_files_used, min(num_of_models,len(feature_files_used)) ))\n","  np_x_validation_collab = np.array([np.load(ff) for ff in validation_features_to_load])\n","  np_x_validation_collab = np.concatenate(np_x_validation_collab, axis=axis_to_concat)\n","\n","  train_features_to_load = [ff.replace(\"Validation\", \"Train\") for ff in validation_features_to_load]\n","  np_x_train_collab = np.array([np.load(ff) for ff in train_features_to_load])\n","  np_x_train_collab = np.concatenate(np_x_train_collab, axis=axis_to_concat)\n","\n","  test_features_to_load = [ff.replace(\"Validation\", \"Test\") for ff in validation_features_to_load]\n","  np_x_test_collab = np.array([np.load(ff) for ff in test_features_to_load])\n","  np_x_test_collab = np.concatenate(np_x_test_collab, axis=axis_to_concat)\n","\n","  return np_x_validation_collab, np_x_train_collab, np_x_test_collab\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7IhTGp4wUo7T"},"source":["# Collaborative learning on DNN models based on last dense and softmax layer features"]},{"cell_type":"markdown","metadata":{"id":"YqPlplwbEhW2"},"source":["Do a collaborative layer on 20 DNN on features of penultimate layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fvMFC5GHoFM3"},"outputs":[],"source":["# set up the data for the DNN collaborative\n","\n","# dnn_features_files\n","# cnn_features_files\n","\n","import random\n","import datetime\n","\n","num_of_models = 20\n","layer_name_penultimate_dnn = \"D3R\"\n","\n","np_x_validation_collab_dnn_penultimate, np_y_train_collab_dnn_penultimate, np_x_test_collab_dnn_penultimate = get_features_for_layer(dnn_features_files, layer_name_penultimate_dnn, num_of_models)\n","\n","start_time = timeit.default_timer()\n","fcd1, fcdh1 = compile_and_fit_model_basic( model_combination_of_features,  \n","                    f\"Collab_{layer_name_penultimate_dnn}_DNN_A_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","                    np_y_train_collab_dnn_penultimate[0,:].shape, \n","                    np_y_train_collab_dnn_penultimate, \n","                    train_targets,\n","                    save_max_epoch=False,\n","                    save_final=True,\n","                    patience_count = 35,\n","                    early_stopping_obs = 'val_sparse_categorical_accuracy',\n","                    log_history = True,\n","                    verbose_level=0,                             \n","                    batch_size=512, \n","                    epochs=250, \n","                    class_weight=None, \n","                    validation_data=(np_x_validation_collab_dnn_penultimate, validation_targets))\n","                    # validation_data=(test_data_grey, test_targets))\n","print(timeit.default_timer()-start_time)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DU7j_6cWhxVC"},"source":["Do a collaborative layer on 20 DNN on features of softmax layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ue64ByTEhxVD"},"outputs":[],"source":["# set up the data for the DNN collaborative\n","\n","import random\n","\n","num_of_models = 20\n","layer_name_sfmx_dnn = \"SFTMX1\"\n","\n","np_x_validation_collab_dnn_sftmx, np_y_train_collab_dnn_sftmx, np_x_test_collab_dnn_sftmx = get_features_for_layer(dnn_features_files, layer_name_sfmx_dnn, num_of_models)\n","\n","start_time = timeit.default_timer()\n","fcd2, fcdh2 = compile_and_fit_model_basic( model_combination_of_features,  \n","                    f\"Collab_{layer_name_sfmx_dnn}_DNN_A_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","                    np_y_train_collab_dnn_sftmx[0,:].shape, \n","                    np_y_train_collab_dnn_sftmx, \n","                    train_targets,\n","                    save_max_epoch=False,\n","                    save_final=True,\n","                    patience_count = 35,\n","                    early_stopping_obs = 'val_sparse_categorical_accuracy',\n","                    log_history = True,\n","                    verbose_level=0,                             \n","                    batch_size=512, \n","                    epochs=250, \n","                    class_weight=None, \n","                    validation_data=(np_x_validation_collab_dnn_sftmx, validation_targets))\n","print(timeit.default_timer()-start_time)\n"]},{"cell_type":"markdown","metadata":{"id":"mIJt0f9ZVIm5"},"source":["# Collaborative learning on CNN models based on last dense and softmax layer features"]},{"cell_type":"markdown","metadata":{"id":"-orJHkR41Kql"},"source":["Do a collaborative layer on 20 CNN of features of penultimate layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uOE0m4YR1Kqq"},"outputs":[],"source":["# set up the data for the CNN collaborative\n","\n","# dnn_features_files\n","# cnn_features_files\n","\n","import random\n","\n","num_of_models = 20\n","layer_name_penultimate_cnn = \"DRP1\"\n","\n","np_x_validation_collab_cnn_penultimate, np_y_train_collab_cnn_penultimate, np_x_test_collab_cnn_penultimate = get_features_for_layer(cnn_features_files, layer_name_penultimate_cnn, num_of_models)\n","\n","start_time = timeit.default_timer()\n","fcc1, fcch1 = compile_and_fit_model_basic( model_combination_of_features,  \n","                    f\"Collab_{layer_name_penultimate_cnn}_CNN_A_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","                    np_y_train_collab_cnn_penultimate[0,:].shape, \n","                    np_y_train_collab_cnn_penultimate, \n","                    train_targets,\n","                    save_max_epoch=False,\n","                    save_final=True,\n","                    patience_count = 35,\n","                    early_stopping_obs = 'val_sparse_categorical_accuracy',\n","                    log_history = True,\n","                    verbose_level=0,                             \n","                    batch_size=512, \n","                    epochs=250, \n","                    class_weight=None, \n","                    validation_data=(np_x_validation_collab_cnn_penultimate, validation_targets))\n","print(timeit.default_timer()-start_time)"]},{"cell_type":"markdown","metadata":{"id":"MEetGroZmRtA"},"source":["Do a collaborative layer on 20 CNN of features of softmax layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tIPxhqdImRtA"},"outputs":[],"source":["# set up the data for the CNN collaborative\n","\n","# dnn_features_files\n","# cnn_features_files\n","\n","import random\n","\n","num_of_models = 20\n","layer_name_sftmx_cnn = \"SFTMX1\"\n","\n","np_x_validation_collab_cnn_sftmx, np_y_train_collab_cnn_sftmx, np_x_test_collab_cnn_sftmx =  get_features_for_layer(cnn_features_files, layer_name_sftmx_cnn, num_of_models)\n","\n","start_time = timeit.default_timer()\n","fcc2, fcch2 = compile_and_fit_model_basic( model_combination_of_features,  \n","                    f\"Collab_{layer_name_sftmx_cnn}_CNN_A_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","                    np_y_train_collab_cnn_sftmx[0,:].shape, \n","                    np_y_train_collab_cnn_sftmx, \n","                    train_targets,\n","                    save_max_epoch=False,\n","                    save_final=True,\n","                    patience_count = 35,\n","                    early_stopping_obs = 'val_sparse_categorical_accuracy',\n","                    log_history = True,\n","                    verbose_level=0,                             \n","                    batch_size=512, \n","                    epochs=250, \n","                    class_weight=None, \n","                    validation_data=(np_x_validation_collab_cnn_sftmx, validation_targets))\n","print(timeit.default_timer()-start_time)\n"]},{"cell_type":"markdown","metadata":{"id":"tgih_EEjVMdQ"},"source":["# Collaborative learning on CNN & DNN models based on last dense and softmax layer features"]},{"cell_type":"markdown","metadata":{"id":"mwv8LJSc2Jyh"},"source":["Do a collaborative layer on 10 CNN and 10 DNN on features of the penultimate layer\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"updulQYF2Jyh"},"outputs":[],"source":["# set up the data for the CNN+DNN collaborative on last dense layer\n","\n","import random\n","\n","num_of_models = 20\n","\n","layer_name_penultimate_dnn = \"D3R\"\n","layer_name_penultimate_cnn = \"DRP1\"\n","\n","validation_features_to_load_cnn = sorted(random.sample([ff for ff in cnn_features_files if \"Validation_X.npy\" in ff and f\"_{layer_name_penultimate_cnn}_\" in ff], int(num_of_models/2)))\n","validation_features_to_load_dnn  = sorted(random.sample([ff for ff in dnn_features_files if \"Validation_X.npy\" in ff and f\"_{layer_name_penultimate_dnn}_\" in ff], int(num_of_models/2)))\n","\n","validation_features_cnn = np.array([np.load(ff) for ff in validation_features_to_load_cnn ])\n","validation_features_dnn = np.array([np.load(ff) for ff in validation_features_to_load_dnn ])\n","np_x_validation_collab_cdnn = np.concatenate([validation_features_cnn,validation_features_dnn], axis=2)\n","\n","train_features_to_load_cnn = [ff.replace(\"Validation\", \"Train\") for ff in validation_features_to_load_cnn]\n","train_features_to_load_dnn = [ff.replace(\"Validation\", \"Train\") for ff in validation_features_to_load_dnn]\n","train_features_cnn = np.array([np.load(ff) for ff in train_features_to_load_cnn ])\n","train_features_dnn = np.array([np.load(ff) for ff in train_features_to_load_dnn ])\n","np_x_train_collab_cdnn = np.concatenate([train_features_cnn,train_features_dnn], axis=2)\n","\n","test_features_to_load_cnn = [ff.replace(\"Validation\", \"Test\") for ff in validation_features_to_load_cnn]\n","test_features_to_load_dnn = [ff.replace(\"Validation\", \"Test\") for ff in validation_features_to_load_dnn]\n","test_features_cnn = np.array([np.load(ff) for ff in test_features_to_load_cnn ])\n","test_features_dnn = np.array([np.load(ff) for ff in test_features_to_load_dnn ])\n","np_x_test_collab_cdnn_penultimate = np.concatenate([test_features_cnn,test_features_dnn], axis=2)\n","\n","\n","np_x_train_collab_cdnn = np.swapaxes(np_x_train_collab_cdnn, 0,1)\n","np_x_validation_collab_cdnn = np.swapaxes(np_x_validation_collab_cdnn, 0,1)\n","np_x_test_collab_cdnn_penultimate = np.swapaxes(np_x_test_collab_cdnn_penultimate, 0,1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7mbiopJ_2Jyi"},"outputs":[],"source":["layer_name = \"DRP1_D3R\"\n","\n","start_time = timeit.default_timer()\n","fccd1, fccdh1 = compile_and_fit_model_basic( model_combination_of_features_with_flatten,  \n","                    f\"Collab_{layer_name}_DCNN_A_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","                    np_x_train_collab_cdnn[0,:].shape, \n","                    np_x_train_collab_cdnn, \n","                    train_targets,\n","                    save_max_epoch=False,\n","                    save_final=True,\n","                    patience_count = 35,\n","                    early_stopping_obs = 'val_sparse_categorical_accuracy',\n","                    log_history = True,\n","                    verbose_level=0,                             \n","                    batch_size=512, \n","                    epochs=250, \n","                    class_weight=None, \n","                    validation_data=(np_x_validation_collab_cdnn, validation_targets))\n","print(timeit.default_timer()-start_time)\n"]},{"cell_type":"markdown","metadata":{"id":"rqlhpJSF932v"},"source":["Do a collaborative layer on 10 CNN and 10 DNN on features of the softmax layer\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7PeDS353932v"},"outputs":[],"source":["# set up the data for the CNN+DNN collaborative on softmax layer\n","\n","import random\n","\n","num_of_models = 20\n","\n","layer_name_sftmx_dnn = \"SFTMX1\"\n","layer_name_sftmx_cnn = \"SFTMX1\"\n","\n","validation_features_to_load_cnn = sorted(random.sample([ff for ff in cnn_features_files if \"Validation_X.npy\" in ff and f\"_{layer_name_sftmx_dnn}_\" in ff], int(num_of_models/2)))\n","validation_features_to_load_dnn  = sorted(random.sample([ff for ff in dnn_features_files if \"Validation_X.npy\" in ff and f\"_{layer_name_sftmx_cnn}_\" in ff], int(num_of_models/2)))\n","\n","validation_features_cnn = np.array([np.load(ff) for ff in validation_features_to_load_cnn ])\n","validation_features_dnn = np.array([np.load(ff) for ff in validation_features_to_load_dnn ])\n","np_x_validation_collab_cdnn = np.concatenate([validation_features_cnn,validation_features_dnn], axis=2)\n","\n","train_features_to_load_cnn = [ff.replace(\"Validation\", \"Train\") for ff in validation_features_to_load_cnn]\n","train_features_to_load_dnn = [ff.replace(\"Validation\", \"Train\") for ff in validation_features_to_load_dnn]\n","train_features_cnn = np.array([np.load(ff) for ff in train_features_to_load_cnn ])\n","train_features_dnn = np.array([np.load(ff) for ff in train_features_to_load_dnn ])\n","np_x_train_collab_cdnn = np.concatenate([train_features_cnn,train_features_dnn], axis=2)\n","\n","test_features_to_load_cnn = [ff.replace(\"Validation\", \"Test\") for ff in validation_features_to_load_cnn]\n","test_features_to_load_dnn = [ff.replace(\"Validation\", \"Test\") for ff in validation_features_to_load_dnn]\n","test_features_cnn = np.array([np.load(ff) for ff in test_features_to_load_cnn ])\n","test_features_dnn = np.array([np.load(ff) for ff in test_features_to_load_dnn ])\n","np_x_test_collab_cdnn_sftmx = np.concatenate([test_features_cnn,test_features_dnn], axis=2)\n","\n","\n","np_x_train_collab_cdnn = np.swapaxes(np_x_train_collab_cdnn, 0,1)\n","np_x_validation_collab_cdnn = np.swapaxes(np_x_validation_collab_cdnn, 0,1)\n","np_x_test_collab_cdnn_sftmx = np.swapaxes(np_x_test_collab_cdnn_sftmx, 0,1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZzSY-SuN932w"},"outputs":[],"source":["layer_name = \"SFTMX1_SFTMX1\"\n","\n","start_time = timeit.default_timer()\n","fccd2, fccdh2 = compile_and_fit_model_basic( model_combination_of_features_with_flatten,  \n","                    f\"Collab_{layer_name}_DCNN_A_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","                    np_x_train_collab_cdnn[0,:].shape, \n","                    np_x_train_collab_cdnn, \n","                    train_targets,\n","                    save_max_epoch=False,\n","                    save_final=True,\n","                    patience_count = 35,\n","                    early_stopping_obs = 'val_sparse_categorical_accuracy',\n","                    log_history = True,\n","                    verbose_level=0,                             \n","                    batch_size=512, \n","                    epochs=250, \n","                    class_weight=None, \n","                    validation_data=(np_x_validation_collab_cdnn, validation_targets))\n","print(timeit.default_timer()-start_time)"]},{"cell_type":"markdown","metadata":{"id":"eDq_scmJVVLP"},"source":["# Collaborative learning on WideResNet models based on last dense (not possible) and softmax layer features"]},{"cell_type":"markdown","metadata":{"id":"K7EjdLpd2sXO"},"source":["Do a collaborative layer on 10 WideResNet28-10 penultimate layer features\n","\n","Not possible to run because of memory constraints "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A10At6zo1iNq"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"5NX-FaO4OmYX"},"source":["Do a collaborative layer on 10 WideResNet28-10 softmax layer features\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-JhGM1AFPobp"},"outputs":[],"source":["import random\n","import re\n","\n","def get_base_patterns_for_validation(features_files, layer_name = \"CLASSIFIER_D1\", model_type = \"WideResNet\"):\n","  base_patterns_for_validations = []\n","  for ff in [ s for s in features_files if \"Validation\" in s]:\n","    validation_search = re.search(f'^.*({model_type}.*_ID.*features_{layer_name}_Validation)[0-9]+.*$', ff, re.IGNORECASE)\n","    if validation_search:\n","        base_patterns_for_validations.append(validation_search.group(1))\n","\n","  base_patterns_for_validations = sorted(list(set(base_patterns_for_validations)))\n","  return base_patterns_for_validations\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LBpPvn3SS8MW"},"outputs":[],"source":["def get_features_for_layer_using_subbatches(feature_files, layer_name, num_of_models, model_type=\"WideResNet\", axis_to_concat = 1):\n"," \n","  feature_files_used = [ff for ff in feature_files if \"Validation\" in ff and \"_X\" in ff and f\"_{layer_name}_\" in ff ]\n","  \n","  # base_patterns_for_validations = []\n","  # for ff in [ s for s in feature_files_used ]:\n","  #   validation_search = re.search(f'^.*(_ID.*features_{layer_name}_Validation)[0-9]+.*$', ff, re.IGNORECASE)\n","  #   if validation_search:\n","  #       base_patterns_for_validations.append(validation_search.group(1))\n","  # base_patterns_for_validations = sorted(list(set(base_patterns_for_validations)))\n","\n","  base_patterns_for_validations = get_base_patterns_for_validation(feature_files_used, layer_name, model_type)\n","  base_patterns_for_validations = sorted(random.sample(base_patterns_for_validations, min(num_of_models,len(base_patterns_for_validations)) ))\n","\n","  np_x_validation_collab = None\n","  for base_val_str in base_patterns_for_validations:\n","      validation_batch_files = sorted([ ff for ff in feature_files_used if base_val_str in ff])\n","      np_x_validation_collab_batch = np.array([np.load(ff) for ff in validation_batch_files])\n","      np_x_validation_collab_batch = np.concatenate(np_x_validation_collab_batch, axis=0)\n","      if np_x_validation_collab is None:\n","        np_x_validation_collab = np_x_validation_collab_batch.copy()\n","      else:\n","        np_x_validation_collab = np.concatenate([np_x_validation_collab, np_x_validation_collab_batch], axis=axis_to_concat)\n","  \n","  np_x_test_collab = None\n","  for base_val_str in [ ff.replace(\"Validation\",\"Test\") for ff in base_patterns_for_validations]:\n","      test_batch_files = dir_has_file_with_regex(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features\",f\".*{base_val_str}.*_X.*$\")\n","      np_x_test_collab_batch = np.array([np.load(ff) for ff in test_batch_files])\n","      np_x_test_collab_batch = np.concatenate(np_x_test_collab_batch, axis=0)\n","      if np_x_test_collab is None:\n","        np_x_test_collab = np_x_test_collab_batch.copy()\n","      else:\n","        np_x_test_collab = np.concatenate([np_x_test_collab, np_x_test_collab_batch], axis=axis_to_concat)\n","\n","  np_x_train_collab = None\n","  for base_val_str in [ ff.replace(\"Validation\",\"Train\") for ff in base_patterns_for_validations]:\n","      train_batch_files = dir_has_file_with_regex(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features\",f\".*{base_val_str}.*_X.*$\")\n","      np_x_train_collab_batch = np.array([np.load(ff) for ff in train_batch_files])\n","      np_x_train_collab_batch = np.concatenate(np_x_train_collab_batch, axis=0)\n","      if np_x_train_collab is None:\n","        np_x_train_collab = np_x_train_collab_batch.copy()\n","      else:\n","        np_x_train_collab = np.concatenate([np_x_train_collab, np_x_train_collab_batch], axis=axis_to_concat)\n","\n","  return np_x_validation_collab, np_x_train_collab, np_x_test_collab"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nTpZ0n_PK6XR"},"outputs":[],"source":["import random\n","import re\n","\n","num_of_models = 10\n","layer_name = \"CLASSIFIER_D1\"\n","model_type = \"WideResNet\"\n","\n","np_x_validation_collab_wideresnets, np_x_train_collab_wideresnets, np_x_test_collab_wideresnets =  get_features_for_layer_using_subbatches(wideresnets_features_files,layer_name,num_of_models,model_type )\n","\n","start_time = timeit.default_timer()\n","wrsftmx, wrsftmxh = compile_and_fit_model_basic( model_combination_of_features,  \n","                    f\"Collab_{layer_name}_WideResnet28-10_A_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","                    np_x_train_collab_wideresnets[0,:].shape, \n","                    np_x_train_collab_wideresnets, \n","                    train_targets,\n","                    save_max_epoch=False,\n","                    save_final=True,\n","                    patience_count = 35,\n","                    early_stopping_obs = 'val_sparse_categorical_accuracy',\n","                    log_history = True,\n","                    verbose_level=0,                             \n","                    batch_size=512, \n","                    epochs=250, \n","                    class_weight=None, \n","                    validation_data=(np_x_validation_collab_wideresnets, validation_targets))\n","print(timeit.default_timer()-start_time)\n"]},{"cell_type":"markdown","metadata":{"id":"YlyIJ3CCVmCq"},"source":["# Collaborative learning on WideResNet, CNN & DNN models based on softmax layer features"]},{"cell_type":"markdown","metadata":{"id":"WX8Z0L0QyUQb"},"source":["Do a collaborative layer on 1 DNN and 4 CNN, 5 WideResNets on features of the softmax layer\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PlbJZSX9yUQb"},"outputs":[],"source":["# set up the data for the WideResnet+CNN+DNN collaborative on softmax layer\n","\n","import random\n","\n","num_of_models_dnn = 1\n","num_of_models_cnn = 4\n","num_of_models_wideresnets = 5\n","\n","layer_name_sftmx_dnn = \"SFTMX1\"\n","layer_name_sftmx_cnn = \"SFTMX1\"\n","layer_name_sftmx_wideresnets = \"CLASSIFIER_D1\"\n","\n","np_x_validation_collab_dnn, np_x_train_collab_dnn, np_x_test_collab_dnn = get_features_for_layer(dnn_features_files,layer_name_sftmx_dnn,num_of_models_dnn)\n","np_x_validation_collab_cnn, np_x_train_collab_cnn, np_x_test_collab_cnn = get_features_for_layer(cnn_features_files,layer_name_sftmx_cnn,num_of_models_cnn)\n","np_x_validation_collab_wideresnets, np_x_train_collab_wideresnets, np_x_test_collab_wideresnets = get_features_for_layer_using_subbatches(wideresnets_features_files,layer_name_sftmx_wideresnets,num_of_models_wideresnets,model_type=\"WideResNet\")\n","\n","\n","np_x_train_collab_wrcdnn = np.concatenate([np_x_train_collab_dnn,np_x_train_collab_cnn,np_x_train_collab_wideresnets], axis=1)\n","np_x_validation_collab_wrcdnn = np.concatenate([np_x_validation_collab_dnn,np_x_validation_collab_cnn,np_x_validation_collab_wideresnets], axis=1)\n","np_x_test_collab_wrcdnn = np.concatenate([np_x_test_collab_dnn,np_x_test_collab_cnn,np_x_test_collab_wideresnets], axis=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1aX3tTiDyUQc"},"outputs":[],"source":["layer_name = \"SFTMX1_SFTMX1_CLASSIFIER_D1\"\n","\n","start_time = timeit.default_timer()\n","wrsdcnnsftmx, wrsdcnnsftmxh = compile_and_fit_model_basic( model_combination_of_features_with_flatten,  \n","                    f\"Collab_{layer_name}_DCNNWRNTS_A_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","                    np_x_train_collab_wrcdnn[0,:].shape, \n","                    np_x_train_collab_wrcdnn, \n","                    train_targets,\n","                    save_max_epoch=False,\n","                    save_final=True,\n","                    patience_count = 35,\n","                    early_stopping_obs = 'val_sparse_categorical_accuracy',\n","                    log_history = True,\n","                    verbose_level=0,                             \n","                    batch_size=512, \n","                    epochs=250, \n","                    class_weight=None, \n","                    validation_data=(np_x_validation_collab_wrcdnn, validation_targets))\n","print(timeit.default_timer()-start_time)\n"]},{"cell_type":"markdown","metadata":{"id":"Hj3nBkOG0gzh"},"source":["Do a collaborative layer on 2 DNN and 8 CNN, 10 WideResNets on features of the softmax layer\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nVTtEzUn0gzh"},"outputs":[],"source":["# set up the data for the WideResnet+CNN+DNN collaborative on softmax layer\n","\n","import random\n","\n","num_of_models_dnn = 2\n","num_of_models_cnn = 8\n","num_of_models_wideresnets = 10\n","\n","layer_name_sftmx_dnn = \"SFTMX1\"\n","layer_name_sftmx_cnn = \"SFTMX1\"\n","layer_name_sftmx_wideresnets = \"CLASSIFIER_D1\"\n","\n","np_x_validation_collab_dnn, np_x_train_collab_dnn, np_x_test_collab_dnn = get_features_for_layer(dnn_features_files,layer_name_sftmx_dnn,num_of_models_dnn)\n","np_x_validation_collab_cnn, np_x_train_collab_cnn, np_x_test_collab_cnn = get_features_for_layer(cnn_features_files,layer_name_sftmx_cnn,num_of_models_cnn)\n","np_x_validation_collab_wideresnets, np_x_train_collab_wideresnets, np_x_test_collab_wideresnets = get_features_for_layer_using_subbatches(wideresnets_features_files,layer_name_sftmx_wideresnets,num_of_models_wideresnets,model_type=\"WideResNet\")\n","\n","\n","np_x_train_collab_wrcdnn = np.concatenate([np_x_train_collab_dnn,np_x_train_collab_cnn,np_x_train_collab_wideresnets], axis=1)\n","np_x_validation_collab_wrcdnn = np.concatenate([np_x_validation_collab_dnn,np_x_validation_collab_cnn,np_x_validation_collab_wideresnets], axis=1)\n","np_x_test_collab_wrcdnn2 = np.concatenate([np_x_test_collab_dnn,np_x_test_collab_cnn,np_x_test_collab_wideresnets], axis=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sB1THFzz0gzh"},"outputs":[],"source":["layer_name = \"SFTMX1_SFTMX1_CLASSIFIER_D1\"\n","\n","start_time = timeit.default_timer()\n","wrsdcnnsftmx2, wrsdcnnsftmxh2 = compile_and_fit_model_basic( model_combination_of_features_with_flatten,  \n","                    f\"Collab_{layer_name}_DCNNWRNTS_A_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","                    np_x_train_collab_wrcdnn[0,:].shape, \n","                    np_x_train_collab_wrcdnn, \n","                    train_targets,\n","                    save_max_epoch=False,\n","                    save_final=True,\n","                    patience_count = 35,\n","                    early_stopping_obs = 'val_sparse_categorical_accuracy',\n","                    log_history = True,\n","                    verbose_level=0,                             \n","                    batch_size=512, \n","                    epochs=250, \n","                    class_weight=None, \n","                    validation_data=(np_x_validation_collab_wrcdnn, validation_targets))\n","print(timeit.default_timer()-start_time)"]},{"cell_type":"markdown","metadata":{"id":"FIWn1BVd_y3F"},"source":["Do a collaborative layer on 5 CNN, 5 WideResNets on features of the softmax layer\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CuPo4NEK_y3G"},"outputs":[],"source":["# set up the data for the WideResnet+CNN+DNN collaborative on softmax layer\n","\n","import random\n","\n","num_of_models_cnn = 5\n","num_of_models_wideresnets = 5\n","\n","layer_name_sftmx_cnn = \"SFTMX1\"\n","layer_name_sftmx_wideresnets = \"CLASSIFIER_D1\"\n","\n","np_x_validation_collab_cnn, np_x_train_collab_cnn, np_x_test_collab_cnn = get_features_for_layer(cnn_features_files,layer_name_sftmx_cnn,num_of_models_cnn)\n","np_x_validation_collab_wideresnets, np_x_train_collab_wideresnets, np_x_test_collab_wideresnets = get_features_for_layer_using_subbatches(wideresnets_features_files,layer_name_sftmx_wideresnets,num_of_models_wideresnets,model_type=\"WideResNet\")\n","\n","np_x_train_collab_wrcnn = np.concatenate([np_x_train_collab_cnn,np_x_train_collab_wideresnets], axis=1)\n","np_x_validation_collab_wrcnn = np.concatenate([np_x_validation_collab_cnn,np_x_validation_collab_wideresnets], axis=1)\n","np_x_test_collab_wrcnn = np.concatenate([np_x_test_collab_cnn,np_x_test_collab_wideresnets], axis=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HE2FGKUv_y3G"},"outputs":[],"source":["layer_name = \"SFTMX1_CLASSIFIER_D1\"\n","start_time = timeit.default_timer()\n","wrscnnsftmx, wrscnnsftmxh = compile_and_fit_model_basic( model_combination_of_features_with_flatten,  \n","                    f\"Collab_{layer_name}_CNNWRNTS_A_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","                    np_x_train_collab_wrcnn[0,:].shape, \n","                    np_x_train_collab_wrcnn, \n","                    train_targets,\n","                    save_max_epoch=False,\n","                    save_final=True,\n","                    patience_count = 35,\n","                    early_stopping_obs = 'val_sparse_categorical_accuracy',\n","                    log_history = True,\n","                    verbose_level=0,                             \n","                    batch_size=512, \n","                    epochs=250, \n","                    class_weight=None, \n","                    validation_data=(np_x_validation_collab_wrcnn, validation_targets))\n","print(timeit.default_timer()-start_time)\n"]},{"cell_type":"markdown","metadata":{"id":"upPlyiuGnrbK"},"source":["# A summary results for all collaborative models on the last layer features (DNN 20, CNN 20, CNN10/DNN10, WideResnet, CNN/DNN/WideReset, CNN/WideResNet) using a DNN to collaborate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TBpiSG03piyX"},"outputs":[],"source":["models_to_use_collab_dnnf = [fcd1, fcd2, fcc1, fcc2, fccd1, fccd2, wrsftmx, wrsdcnnsftmx, wrsdcnnsftmx2, wrscnnsftmx]\n","model_predictions_collab_dnnf = [ fcd1.predict(np_x_test_collab_dnn_penultimate), \n","                                  fcd2.predict(np_x_test_collab_dnn_sftmx), \n","                                  fcc1.predict(np_x_test_collab_cnn_penultimate), \n","                                  fcc2.predict(np_x_test_collab_cnn_sftmx), \n","                                  fccd1.predict(np_x_test_collab_cdnn_penultimate),\n","                                  fccd2.predict(np_x_test_collab_cdnn_sftmx),\n","                                  wrsftmx.predict(np_x_test_collab_wideresnets),\n","                                  wrsdcnnsftmx.predict(np_x_test_collab_wrcdnn),\n","                                  wrsdcnnsftmx2.predict(np_x_test_collab_wrcdnn2),\n","                                 wrscnnsftmx.predict(np_x_test_collab_wrcnn)]\n","\n","y_preds_collab_dnnf = [ np.apply_along_axis(np.argmax, 1, y_pred_model) for y_pred_model in model_predictions_collab_dnnf ] \n","scores_fCollabdnnDNN20_fCollabdnnCNN20_fCollabdnnCNN10DNN10 = [ pr_rc_f1_acc_from_supplied(y_pred,test_targets) for y_pred in y_preds_collab_dnnf ]\n","\n","np.save(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/feature_collabUsingDnn_dnn20_cnn20_cnn10dnn10_wideresnet2810_wrs5cnn4dnn1_wrs10cnn8dnn2_wrs5cnn5_summary_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","        np.array(scores_fCollabdnnDNN20_fCollabdnnCNN20_fCollabdnnCNN10DNN10), \n","        allow_pickle=True, \n","        fix_imports=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S91-VXhYXqYE"},"outputs":[],"source":["scores_fCollabdnnDNN20_fCollabdnnCNN20_fCollabdnnCNN10DNN10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dR603zQv3p5-"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"DaQvakL4rU88"},"source":["# Load CCA/PCA Data for Collaborative CNN/DNN models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_SAYfptJrN-V"},"outputs":[],"source":["# cframe.loc[cframe.CCA1==1.0][\"f1\"]\n","\n","import os,sys\n","import numpy as np\n","import pandas as pd\n","\n","dnn_distresults = pd.read_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_ccs/dnn_Validation_ccs_20211115040852.csv\")\n","cnn_distresults = pd.read_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_ccs/cnn_Validation_ccs_20211115184258.csv\")\n","\n","dnn_distresults = dnn_distresults[dnn_distresults.Type==\"DNN\"]\n","# individual_dnns = np.load(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/individual_dnn_summary_20211014152626.npy\")\n","# individual_cnns = np.load(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/individual_cnn_summary_20211014152108.npy\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3m5dSqo9rN-V"},"outputs":[],"source":["# cnn_distresults.sort_values(\"CCA1\")\n","# cnn_distresults[cnn_distresults.Layer == \"SFTMX1\"].sort_values(\"PCA1\")\n","# cnn_distresults[cnn_distresults.Layer == \"SFTMX1\"].sort_values(\"CCA1\")[0:40]\n","# dnn_distresults[dnn_distresults.Layer == \"SFTMX1\"].sort_values(\"CCA1\")\n","\n","cnn_avg_cca = cnn_distresults.groupby([\"Type\",\"Layer\",\"f1\"])[\"CCA1\"].mean().reset_index()\n","dnn_avg_cca = dnn_distresults.groupby([\"Type\",\"Layer\",\"f1\"])[\"CCA1\"].mean().reset_index()\n","cnn_avg_cca = cnn_avg_cca.sort_values('CCA1')\n","dnn_avg_cca = pd.concat([dnn_avg_cca[dnn_avg_cca.CCA1 > 0.45].sort_values('CCA1'), dnn_avg_cca[dnn_avg_cca.CCA1 <= 0.45]])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L5iV3Im5rN-V"},"outputs":[],"source":["cnn_avg_pca = cnn_distresults.groupby([\"Type\",\"Layer\",\"f1\"])[\"PCA1\"].apply(lambda x: np.mean(np.abs(x))).reset_index()\n","dnn_avg_pca = dnn_distresults.groupby([\"Type\",\"Layer\",\"f1\"])[\"PCA1\"].apply(lambda x: np.mean(np.abs(x))).reset_index()\n"]},{"cell_type":"markdown","metadata":{"id":"TXy-xmor32UY"},"source":["# Data for Plot on Collaborative DNN softmax layers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Uk02GgJsT1x"},"outputs":[],"source":["collaborativeFullyC_softmax_data = None\n","\n","x_input = test_data_grey\n","y_input = test_targets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sAUaJo6XFvVI"},"outputs":[],"source":["# dnn_features_files_here = [ f for f in dnn_features_files if layer_name_sfmx_dnn in f]\n","# dnn_features_files_here\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lawIAx1L32UZ"},"outputs":[],"source":["# set up the data for the DNN collaborative\n","\n","import random\n","import datetime\n","\n","num_of_repeats = 4\n","num_of_models = [25,30,35,40,45,50]  #[2,3,4,5,6,8,10,12,14,16,18,20]\n","layer_name_sfmx_dnn = \"SFTMX1\"\n","\n","idxCount = 0 if collaborativeFullyC_softmax_data is None else len(collaborativeFullyC_softmax_data.index)\n","for repc in range(num_of_repeats):\n","  for mc in num_of_models:\n","    model_name = f\"Collab_{layer_name_sfmx_dnn}_DNN_A_{datetime.datetime.now():%Y%m%d%H%M%S}\"\n","    np_x_validation_collab_dnn_sftmx, np_y_train_collab_dnn_sftmx, np_x_test_collab_dnn_sftmx = get_features_for_layer(dnn_features_files, layer_name_sfmx_dnn, mc)\n","    model_here, history_here = compile_and_fit_model_basic( model_combination_of_features,  \n","                        model_name, \n","                        np_y_train_collab_dnn_sftmx[0,:].shape, \n","                        np_y_train_collab_dnn_sftmx, \n","                        train_targets,\n","                        save_max_epoch = False,\n","                        save_final = True,\n","                        patience_count = 35,\n","                        early_stopping_obs = 'val_sparse_categorical_accuracy',\n","                        log_history = True,\n","                        verbose_level=0,                             \n","                        batch_size=512, \n","                        epochs=250, \n","                        class_weight=None, \n","                        validation_data=(np_x_validation_collab_dnn_sftmx, validation_targets))\n","    del np_x_validation_collab_dnn_sftmx\n","    del np_y_train_collab_dnn_sftmx\n","    y_pred_model = model_here.predict(np_x_test_collab_dnn_sftmx)\n","    del np_x_test_collab_dnn_sftmx\n","    y_pred = np.apply_along_axis(np.argmax, 1, y_pred_model) \n","    pr, rc, f1, acc = pr_rc_f1_acc_from_supplied(y_pred,test_targets)\n","    print (mc, repc, pr, rc, f1, acc)\n","    del model_here\n","    del history_here\n","    if collaborativeFullyC_softmax_data is None:\n","      collaborativeFullyC_softmax_data = pd.DataFrame({\"Type\": \"DNN\", \n","                                            \"Data\" : \"Test\",\n","                                            \"NumOfModels\": mc, \n","                                            \"RepC\": repc, \n","                                            \"Pr\": pr,\n","                                            \"Rc\": rc,\n","                                            \"F1\": f1,\n","                                            \"Acc\": acc,\n","                                            }, index = [idxCount])\n","    else:\n","      collaborativeFullyC_softmax_data = pd.concat([collaborativeFullyC_softmax_data,\n","                                         pd.DataFrame({\"Type\": \"DNN\", \n","                                            \"Data\" : \"Test\",\n","                                            \"NumOfModels\": mc, \n","                                            \"RepC\": repc, \n","                                            \"Pr\": pr,\n","                                            \"Rc\": rc,\n","                                            \"F1\": f1,\n","                                            \"Acc\": acc\n","                                            }, index = [idxCount])\n","                                         ])\n","    idxCount = idxCount + 1\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0fkz6m8Ly2e3"},"outputs":[],"source":["collaborativeFullyC_softmax_data.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/dnn_collab_test_results_{datetime.datetime.now():%Y%m%d%H%M%S}.csv\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fh84XOFV4ZZp"},"outputs":[],"source":["dnn_avg_cca"]},{"cell_type":"markdown","metadata":{"id":"uLw5jkNprylq"},"source":["# Data for Plot on Collaborative DNN softmax layers using CCA/PCA "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WYAa6Bd5ro78"},"outputs":[],"source":["collaborativeFullyC_softmax_using_CCA_data = None\n","\n","x_input = test_data_grey\n","y_input = test_targets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WBr8MZZqtcVy"},"outputs":[],"source":["# [f for f in dnn_features_files if \"DNN_A_3_20211002235817_features_SFTMX1\" in f]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dVitC_n-ro78"},"outputs":[],"source":["# set up the data for the DNN collaborative on the softmax using the CCA data\n","\n","import random\n","import datetime\n","\n","num_of_repeats = 1\n","num_of_models = [2,3,4,5,6,8,10,12,14,16,18,20,25,30,35,40,45,50] \n","layer_name_sfmx_dnn = \"SFTMX1\"\n","\n","idxCount = 0 if collaborativeFullyC_softmax_using_CCA_data is None else len(collaborativeFullyC_softmax_using_CCA_data.index)\n","for repc in range(num_of_repeats):\n","  for mc in num_of_models:\n","    model_name = f\"Collab_{layer_name_sfmx_dnn}_DNNCCA_A_{datetime.datetime.now():%Y%m%d%H%M%S}\"\n","    \n","    val_files_used = dnn_avg_cca.loc[(dnn_avg_cca.Layer==layer_name_sfmx_dnn) & (dnn_avg_cca.Type==\"DNN\"),\"f1\"].head(mc)\n","    matches_required = [ vf.split(\"model_features/\")[1].split(\"_Validation\")[0] for vf in val_files_used  ] \n","    feature_files_considered = [ ff for ff in dnn_features_files if any([mu in ff for mu in matches_required]) ]\n","    np_x_validation_collab_dnn_sftmx, np_y_train_collab_dnn_sftmx, np_x_test_collab_dnn_sftmx = get_features_from_multiple_models(feature_files_considered)\n","\n","    model_here, history_here = compile_and_fit_model_basic( model_combination_of_features,  \n","                        model_name, \n","                        np_y_train_collab_dnn_sftmx[0,:].shape, \n","                        np_y_train_collab_dnn_sftmx, \n","                        train_targets,\n","                        save_max_epoch = False,\n","                        save_final = True,\n","                        patience_count = 35,\n","                        early_stopping_obs = 'val_sparse_categorical_accuracy',\n","                        log_history = True,\n","                        verbose_level=0,                             \n","                        batch_size=512, \n","                        epochs=250, \n","                        class_weight=None, \n","                        validation_data=(np_x_validation_collab_dnn_sftmx, validation_targets))\n","    del np_x_validation_collab_dnn_sftmx\n","    del np_y_train_collab_dnn_sftmx\n","    y_pred_model = model_here.predict(np_x_test_collab_dnn_sftmx)\n","    del np_x_test_collab_dnn_sftmx\n","    y_pred = np.apply_along_axis(np.argmax, 1, y_pred_model) \n","    pr, rc, f1, acc = pr_rc_f1_acc_from_supplied(y_pred,test_targets)\n","    print (mc, repc, pr, rc, f1, acc)\n","    del model_here\n","    del history_here\n","    if collaborativeFullyC_softmax_using_CCA_data is None:\n","      collaborativeFullyC_softmax_using_CCA_data = pd.DataFrame({\"Type\": \"DNN\", \n","                                            \"Data\" : \"Test\",\n","                                            \"NumOfModels\": mc, \n","                                            \"RepC\": repc, \n","                                            \"Pr\": pr,\n","                                            \"Rc\": rc,\n","                                            \"F1\": f1,\n","                                            \"Acc\": acc,\n","                                            \"Xtra\" : \"CCAmin\"\n","                                            }, index = [idxCount])\n","    else:\n","      collaborativeFullyC_softmax_using_CCA_data = pd.concat([collaborativeFullyC_softmax_using_CCA_data,\n","                                         pd.DataFrame({\"Type\": \"DNN\", \n","                                            \"Data\" : \"Test\",\n","                                            \"NumOfModels\": mc, \n","                                            \"RepC\": repc, \n","                                            \"Pr\": pr,\n","                                            \"Rc\": rc,\n","                                            \"F1\": f1,\n","                                            \"Acc\": acc,\n","                                            \"Xtra\" : \"CCAmin\"\n","                                            }, index = [idxCount])\n","                                         ])\n","    idxCount = idxCount + 1\n","    collaborativeFullyC_softmax_using_CCA_data.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/temp/dnn_temp_Collab_ccaorder_sftmx_test_results_{datetime.datetime.now():%Y%m%d%H%M%S}.csv\")\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hEyQeXSLro79"},"outputs":[],"source":["collaborativeFullyC_softmax_using_CCA_data.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/dnn_collab_ccaorder_sftmx_test_results_{datetime.datetime.now():%Y%m%d%H%M%S}.csv\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Ul_SqJXjz_s9"},"source":["# Data for Plot on Collaborative CNN softmax layers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pdqqNRyFz_s9"},"outputs":[],"source":["# carry this over from DNN if possible (so keep commented out)\n","## collaborativeFullyC_softmax_data = None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BhDBg50vz_s9"},"outputs":[],"source":["# set up the data for the CNN collaborative\n","\n","import random\n","import datetime\n","\n","num_of_repeats = 4\n","num_of_models = [22,25,30,35,40,50]  # [2,3,4,5,6,8,10,12,14,16,18,20]\n","layer_name_sfmx_cnn = \"SFTMX1\"\n","\n","# layer_name_sftmx_cnn = \"SFTMX1\"\n","# layer_name_sftmx_wideresnets = \"CLASSIFIER_D1\"\n","\n","idxCount = 0 if collaborativeFullyC_softmax_data is None else len(collaborativeFullyC_softmax_data.index)\n","for repc in range(num_of_repeats):\n","  for mc in num_of_models:\n","    model_name = f\"Collab_{layer_name_sfmx_cnn}_CNN_A_{datetime.datetime.now():%Y%m%d%H%M%S}\"\n","    np_x_validation_collab_cnn_sftmx, np_y_train_collab_cnn_sftmx, np_x_test_collab_cnn_sftmx = get_features_for_layer(cnn_features_files, layer_name_sfmx_cnn, mc)\n","    model_here, history_here = compile_and_fit_model_basic( model_combination_of_features,  \n","                        model_name, \n","                        np_y_train_collab_cnn_sftmx[0,:].shape, \n","                        np_y_train_collab_cnn_sftmx, \n","                        train_targets,\n","                        save_max_epoch = False,\n","                        save_final = True,\n","                        patience_count = 35,\n","                        early_stopping_obs = 'val_sparse_categorical_accuracy',\n","                        log_history = True,\n","                        verbose_level=0,                             \n","                        batch_size=512, \n","                        epochs=250, \n","                        class_weight=None, \n","                        validation_data=(np_x_validation_collab_cnn_sftmx, validation_targets))\n","    del np_x_validation_collab_cnn_sftmx\n","    del np_y_train_collab_cnn_sftmx\n","    y_pred_model = model_here.predict(np_x_test_collab_cnn_sftmx)\n","    del np_x_test_collab_cnn_sftmx\n","    y_pred = np.apply_along_axis(np.argmax, 1, y_pred_model) \n","    pr, rc, f1, acc = pr_rc_f1_acc_from_supplied(y_pred,test_targets)\n","    print (mc, repc, pr, rc, f1, acc)\n","    del model_here\n","    del history_here\n","    if collaborativeFullyC_softmax_data is None:\n","      collaborativeFullyC_softmax_data = pd.DataFrame({\"Type\": \"CNN\", \n","                                            \"Data\" : \"Test\",\n","                                            \"NumOfModels\": mc, \n","                                            \"RepC\": repc, \n","                                            \"Pr\": pr,\n","                                            \"Rc\": rc,\n","                                            \"F1\": f1,\n","                                            \"Acc\": acc,\n","                                            }, index = [idxCount])\n","    else:\n","      collaborativeFullyC_softmax_data = pd.concat([collaborativeFullyC_softmax_data,\n","                                         pd.DataFrame({\"Type\": \"CNN\", \n","                                            \"Data\" : \"Test\",\n","                                            \"NumOfModels\": mc, \n","                                            \"RepC\": repc, \n","                                            \"Pr\": pr,\n","                                            \"Rc\": rc,\n","                                            \"F1\": f1,\n","                                            \"Acc\": acc\n","                                            }, index = [idxCount])\n","                                         ])\n","    idxCount = idxCount + 1\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xl-IjQYV2-3Q"},"outputs":[],"source":["collaborativeFullyC_softmax_data.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/cnn_collab_test_results_{datetime.datetime.now():%Y%m%d%H%M%S}.csv\")\n"]},{"cell_type":"markdown","metadata":{"id":"Zv4uI-Rf_P4R"},"source":["# Data for Plot on Collaborative CNN softmax layers using CCA/PCA "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QthquvcK_P4R"},"outputs":[],"source":["collaborativeCNN_softmax_using_CCA_data = None\n","\n","x_input = test_data_grey\n","y_input = test_targets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ESk9xrs1_P4R"},"outputs":[],"source":["# [f for f in dnn_features_files if \"DNN_A_3_20211002235817_features_SFTMX1\" in f]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"itBFK4Kk_P4R"},"outputs":[],"source":["# set up the data for the DNN collaborative on the softmax using the CCA data\n","\n","import random\n","import datetime\n","\n","num_of_repeats = 1\n","num_of_models = [2,3,4,5,6,8,10,12,14,16,18,20,25,30,35,40] \n","layer_name_sfmx_cnn = \"SFTMX1\"\n","\n","idxCount = 0 if collaborativeCNN_softmax_using_CCA_data is None else len(collaborativeCNN_softmax_using_CCA_data.index)\n","for repc in range(num_of_repeats):\n","  for mc in num_of_models:\n","    model_name = f\"Collab_{layer_name_sfmx_cnn}_CNNCCA_A_{datetime.datetime.now():%Y%m%d%H%M%S}\"\n","    \n","    val_files_used = cnn_avg_cca.loc[(cnn_avg_cca.Layer==layer_name_sfmx_cnn) & (cnn_avg_cca.Type==\"CNN\"),\"f1\"].head(mc)\n","    matches_required = [ vf.split(\"model_features/\")[1].split(\"_Validation\")[0] for vf in val_files_used  ] \n","    feature_files_considered = [ ff for ff in cnn_features_files if any([mu in ff for mu in matches_required]) ]\n","    np_x_validation_collab_cnn_sftmx, np_y_train_collab_cnn_sftmx, np_x_test_collab_cnn_sftmx = get_features_from_multiple_models(feature_files_considered)\n","\n","    model_here, history_here = compile_and_fit_model_basic( model_combination_of_features,  \n","                        model_name, \n","                        np_y_train_collab_cnn_sftmx[0,:].shape, \n","                        np_y_train_collab_cnn_sftmx, \n","                        train_targets,\n","                        save_max_epoch = False,\n","                        save_final = True,\n","                        patience_count = 35,\n","                        early_stopping_obs = 'val_sparse_categorical_accuracy',\n","                        log_history = True,\n","                        verbose_level=0,                             \n","                        batch_size=512, \n","                        epochs=250, \n","                        class_weight=None, \n","                        validation_data=(np_x_validation_collab_cnn_sftmx, validation_targets))\n","    del np_x_validation_collab_cnn_sftmx\n","    del np_y_train_collab_cnn_sftmx\n","    y_pred_model = model_here.predict(np_x_test_collab_cnn_sftmx)\n","    del np_x_test_collab_cnn_sftmx\n","    y_pred = np.apply_along_axis(np.argmax, 1, y_pred_model) \n","    pr, rc, f1, acc = pr_rc_f1_acc_from_supplied(y_pred,test_targets)\n","    print (mc, repc, pr, rc, f1, acc)\n","    del model_here\n","    del history_here\n","    if collaborativeCNN_softmax_using_CCA_data is None:\n","      collaborativeCNN_softmax_using_CCA_data = pd.DataFrame({\"Type\": \"CNN\", \n","                                            \"Data\" : \"Test\",\n","                                            \"NumOfModels\": mc, \n","                                            \"RepC\": repc, \n","                                            \"Pr\": pr,\n","                                            \"Rc\": rc,\n","                                            \"F1\": f1,\n","                                            \"Acc\": acc,\n","                                            \"Xtra\" : \"CCAmin\"\n","                                            }, index = [idxCount])\n","    else:\n","      collaborativeCNN_softmax_using_CCA_data = pd.concat([collaborativeCNN_softmax_using_CCA_data,\n","                                         pd.DataFrame({\"Type\": \"CNN\", \n","                                            \"Data\" : \"Test\",\n","                                            \"NumOfModels\": mc, \n","                                            \"RepC\": repc, \n","                                            \"Pr\": pr,\n","                                            \"Rc\": rc,\n","                                            \"F1\": f1,\n","                                            \"Acc\": acc,\n","                                            \"Xtra\" : \"CCAmin\"\n","                                            }, index = [idxCount])\n","                                         ])\n","    idxCount = idxCount + 1\n","    collaborativeCNN_softmax_using_CCA_data.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/temp/cnn_temp_Collab_ccaorder_sftmx_test_results_{datetime.datetime.now():%Y%m%d%H%M%S}.csv\")\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b7WJ_0_M_P4S"},"outputs":[],"source":["collaborativeCNN_softmax_using_CCA_data.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/cnn_collab_ccaorder_sftmx_test_results_{datetime.datetime.now():%Y%m%d%H%M%S}.csv\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pfPdlux63Hqa"},"source":["# Data for Plot on Collaborative WideResNet softmax layers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BaKSB7u93Hqa"},"outputs":[],"source":["# carry this over from DNN/CNN if possible (so keep commented out)\n","## collaborativeFullyC_softmax_data = None\n","\n","# wideresnets_features_files\n","# cnn_features_files\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RrUkssLPG2yb"},"outputs":[],"source":["import random\n","import re\n","\n","def get_base_patterns_for_validation(features_files, layer_name = \"CLASSIFIER_D1\", model_type = \"WideResNet\"):\n","  base_patterns_for_validations = []\n","  for ff in [ s for s in features_files if \"Validation\" in s]:\n","    validation_search = re.search(f'^.*({model_type}.*_ID.*features_{layer_name}_Validation)[0-9]+.*$', ff, re.IGNORECASE)\n","    if validation_search:\n","        base_patterns_for_validations.append(validation_search.group(1))\n","\n","  base_patterns_for_validations = sorted(list(set(base_patterns_for_validations)))\n","  return base_patterns_for_validations\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_lRcoa4qG2yc"},"outputs":[],"source":["def get_features_for_layer_using_subbatches(feature_files, layer_name, num_of_models, model_type=\"WideResNet\", axis_to_concat = 1):\n"," \n","  feature_files_used = [ff for ff in feature_files if \"Validation\" in ff and \"_X\" in ff and f\"_{layer_name}_\" in ff ]\n","  \n","  # base_patterns_for_validations = []\n","  # for ff in [ s for s in feature_files_used ]:\n","  #   validation_search = re.search(f'^.*(_ID.*features_{layer_name}_Validation)[0-9]+.*$', ff, re.IGNORECASE)\n","  #   if validation_search:\n","  #       base_patterns_for_validations.append(validation_search.group(1))\n","  # base_patterns_for_validations = sorted(list(set(base_patterns_for_validations)))\n","\n","  base_patterns_for_validations = get_base_patterns_for_validation(feature_files_used, layer_name, model_type)\n","  base_patterns_for_validations = sorted(random.sample(base_patterns_for_validations, min(num_of_models,len(base_patterns_for_validations)) ))\n","\n","  np_x_validation_collab = None\n","  for base_val_str in base_patterns_for_validations:\n","      validation_batch_files = sorted([ ff for ff in feature_files_used if base_val_str in ff])\n","      np_x_validation_collab_batch = np.array([np.load(ff) for ff in validation_batch_files])\n","      np_x_validation_collab_batch = np.concatenate(np_x_validation_collab_batch, axis=0)\n","      if np_x_validation_collab is None:\n","        np_x_validation_collab = np_x_validation_collab_batch.copy()\n","      else:\n","        np_x_validation_collab = np.concatenate([np_x_validation_collab, np_x_validation_collab_batch], axis=axis_to_concat)\n","  \n","  np_x_test_collab = None\n","  for base_val_str in [ ff.replace(\"Validation\",\"Test\") for ff in base_patterns_for_validations]:\n","      test_batch_files = dir_has_file_with_regex(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features\",f\".*{base_val_str}.*_X.*$\")\n","      np_x_test_collab_batch = np.array([np.load(ff) for ff in test_batch_files])\n","      np_x_test_collab_batch = np.concatenate(np_x_test_collab_batch, axis=0)\n","      if np_x_test_collab is None:\n","        np_x_test_collab = np_x_test_collab_batch.copy()\n","      else:\n","        np_x_test_collab = np.concatenate([np_x_test_collab, np_x_test_collab_batch], axis=axis_to_concat)\n","\n","  np_x_train_collab = None\n","  for base_val_str in [ ff.replace(\"Validation\",\"Train\") for ff in base_patterns_for_validations]:\n","      train_batch_files = dir_has_file_with_regex(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features\",f\".*{base_val_str}.*_X.*$\")\n","      np_x_train_collab_batch = np.array([np.load(ff) for ff in train_batch_files])\n","      np_x_train_collab_batch = np.concatenate(np_x_train_collab_batch, axis=0)\n","      if np_x_train_collab is None:\n","        np_x_train_collab = np_x_train_collab_batch.copy()\n","      else:\n","        np_x_train_collab = np.concatenate([np_x_train_collab, np_x_train_collab_batch], axis=axis_to_concat)\n","\n","  return np_x_validation_collab, np_x_train_collab, np_x_test_collab"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mgc3WW_Y3Hqa"},"outputs":[],"source":["# set up the data for the WideResNet collaborative\n","\n","import random\n","import datetime\n","\n","num_of_repeats = 4\n","num_of_models = [1,2,3,4,5,6,7,8,9,10]\n","layer_name_sftmx_wideresnets = \"CLASSIFIER_D1\"\n","model_type = \"WideResNet\"\n","\n","idxCount = 0 if collaborativeFullyC_softmax_data is None else len(collaborativeFullyC_softmax_data.index)\n","for repc in range(num_of_repeats):\n","  for mc in num_of_models:\n","    model_name = f\"Collab_{layer_name_sftmx_wideresnets}_WideResNet_A_{datetime.datetime.now():%Y%m%d%H%M%S}\"\n","    np_x_validation_collab_wideresnets_sftmx, np_y_train_collab_wideresnets_sftmx, np_x_test_collab_wideresnets_sftmx = get_features_for_layer_using_subbatches(wideresnets_features_files,layer_name_sftmx_wideresnets,mc,model_type )\n","    model_here, history_here = compile_and_fit_model_basic( model_combination_of_features,  \n","                        model_name, \n","                        np_y_train_collab_wideresnets_sftmx[0,:].shape, \n","                        np_y_train_collab_wideresnets_sftmx, \n","                        train_targets,\n","                        save_max_epoch = False,\n","                        save_final = True,\n","                        patience_count = 35,\n","                        early_stopping_obs = 'val_sparse_categorical_accuracy',\n","                        log_history = True,\n","                        verbose_level=0,                             \n","                        batch_size=512, \n","                        epochs=250, \n","                        class_weight=None, \n","                        validation_data=(np_x_validation_collab_wideresnets_sftmx, validation_targets))\n","    del np_x_validation_collab_wideresnets_sftmx\n","    del np_y_train_collab_wideresnets_sftmx\n","    y_pred_model = model_here.predict(np_x_test_collab_wideresnets_sftmx)\n","    del np_x_test_collab_wideresnets_sftmx\n","    y_pred = np.apply_along_axis(np.argmax, 1, y_pred_model) \n","    pr, rc, f1, acc = pr_rc_f1_acc_from_supplied(y_pred,test_targets)\n","    print (mc, repc, pr, rc, f1, acc)\n","    del model_here\n","    del history_here\n","    if collaborativeFullyC_softmax_data is None:\n","      collaborativeFullyC_softmax_data = pd.DataFrame({\"Type\": \"WideResNet\", \n","                                            \"Data\" : \"Test\",\n","                                            \"NumOfModels\": mc, \n","                                            \"RepC\": repc, \n","                                            \"Pr\": pr,\n","                                            \"Rc\": rc,\n","                                            \"F1\": f1,\n","                                            \"Acc\": acc,\n","                                            }, index = [idxCount])\n","    else:\n","      collaborativeFullyC_softmax_data = pd.concat([collaborativeFullyC_softmax_data,\n","                                         pd.DataFrame({\"Type\": \"WideResNet\", \n","                                            \"Data\" : \"Test\",\n","                                            \"NumOfModels\": mc, \n","                                            \"RepC\": repc, \n","                                            \"Pr\": pr,\n","                                            \"Rc\": rc,\n","                                            \"F1\": f1,\n","                                            \"Acc\": acc\n","                                            }, index = [idxCount])\n","                                         ])\n","    idxCount = idxCount + 1\n","\n","\n","# np_x_validation_collab_wideresnets, np_x_train_collab_wideresnets, np_x_test_collab_wideresnets =  get_features_for_layer_using_subbatches(wideresnets_features_files,layer_name,num_of_models,model_type )\n","# wrsftmx, wrsftmxh = compile_and_fit_model_basic( model_combination_of_features,  \n","#                     f\"Collab_{layer_name}_WideResnet28-10_A_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","#                     np_x_train_collab_wideresnets[0,:].shape, \n","#                     np_x_train_collab_wideresnets, \n","#                     train_targets,\n","#                     save_max_epoch=False,\n","#                     save_final=True,\n","#                     patience_count = 35,\n","#                     early_stopping_obs = 'val_sparse_categorical_accuracy',\n","#                     log_history = True,\n","#                     verbose_level=1,                             \n","#                     batch_size=512, \n","#                     epochs=250, \n","#                     class_weight=None, \n","#                     validation_data=(np_x_validation_collab_wideresnets, validation_targets))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-pu-2cp23Hqb"},"outputs":[],"source":["collaborativeFullyC_softmax_data.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/wideresnet_collab_test_results_{datetime.datetime.now():%Y%m%d%H%M%S}.csv\")\n"]},{"cell_type":"markdown","metadata":{"id":"qSwgurq54dx7"},"source":["# Data for Contour Plot on Collaborative CNN/WideResNet softmax layers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bnWPbCsx4dx8"},"outputs":[],"source":["import random\n","import re\n","\n","def get_base_patterns_for_validation(features_files, layer_name = \"CLASSIFIER_D1\", model_type = \"WideResNet\"):\n","  base_patterns_for_validations = []\n","  for ff in [ s for s in features_files if \"Validation\" in s]:\n","    validation_search = re.search(f'^.*({model_type}.*_ID.*features_{layer_name}_Validation)[0-9]+.*$', ff, re.IGNORECASE)\n","    if validation_search:\n","        base_patterns_for_validations.append(validation_search.group(1))\n","\n","  base_patterns_for_validations = sorted(list(set(base_patterns_for_validations)))\n","  return base_patterns_for_validations\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ibzvZiTJ4dx8"},"outputs":[],"source":["def get_features_for_layer_using_subbatches(feature_files, layer_name, num_of_models, model_type=\"WideResNet\", axis_to_concat = 1):\n"," \n","  feature_files_used = [ff for ff in feature_files if \"Validation\" in ff and \"_X\" in ff and f\"_{layer_name}_\" in ff ]\n","  \n","  # base_patterns_for_validations = []\n","  # for ff in [ s for s in feature_files_used ]:\n","  #   validation_search = re.search(f'^.*(_ID.*features_{layer_name}_Validation)[0-9]+.*$', ff, re.IGNORECASE)\n","  #   if validation_search:\n","  #       base_patterns_for_validations.append(validation_search.group(1))\n","  # base_patterns_for_validations = sorted(list(set(base_patterns_for_validations)))\n","\n","  base_patterns_for_validations = get_base_patterns_for_validation(feature_files_used, layer_name, model_type)\n","  base_patterns_for_validations = sorted(random.sample(base_patterns_for_validations, min(num_of_models,len(base_patterns_for_validations)) ))\n","\n","  np_x_validation_collab = None\n","  for base_val_str in base_patterns_for_validations:\n","      validation_batch_files = sorted([ ff for ff in feature_files_used if base_val_str in ff])\n","      np_x_validation_collab_batch = np.array([np.load(ff) for ff in validation_batch_files])\n","      np_x_validation_collab_batch = np.concatenate(np_x_validation_collab_batch, axis=0)\n","      if np_x_validation_collab is None:\n","        np_x_validation_collab = np_x_validation_collab_batch.copy()\n","      else:\n","        np_x_validation_collab = np.concatenate([np_x_validation_collab, np_x_validation_collab_batch], axis=axis_to_concat)\n","  \n","  np_x_test_collab = None\n","  for base_val_str in [ ff.replace(\"Validation\",\"Test\") for ff in base_patterns_for_validations]:\n","      test_batch_files = dir_has_file_with_regex(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features\",f\".*{base_val_str}.*_X.*$\")\n","      np_x_test_collab_batch = np.array([np.load(ff) for ff in test_batch_files])\n","      np_x_test_collab_batch = np.concatenate(np_x_test_collab_batch, axis=0)\n","      if np_x_test_collab is None:\n","        np_x_test_collab = np_x_test_collab_batch.copy()\n","      else:\n","        np_x_test_collab = np.concatenate([np_x_test_collab, np_x_test_collab_batch], axis=axis_to_concat)\n","\n","  np_x_train_collab = None\n","  for base_val_str in [ ff.replace(\"Validation\",\"Train\") for ff in base_patterns_for_validations]:\n","      train_batch_files = dir_has_file_with_regex(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features\",f\".*{base_val_str}.*_X.*$\")\n","      np_x_train_collab_batch = np.array([np.load(ff) for ff in train_batch_files])\n","      np_x_train_collab_batch = np.concatenate(np_x_train_collab_batch, axis=0)\n","      if np_x_train_collab is None:\n","        np_x_train_collab = np_x_train_collab_batch.copy()\n","      else:\n","        np_x_train_collab = np.concatenate([np_x_train_collab, np_x_train_collab_batch], axis=axis_to_concat)\n","\n","  return np_x_validation_collab, np_x_train_collab, np_x_test_collab\n","\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HF7tZpgXioib"},"outputs":[],"source":["# set up the combinations we are going to try\n","# len(wideresnets_features_files)\n","\n","cnn_wideresnet_contour_data = None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AeM917nGy0Gd"},"outputs":[],"source":["cnn_features_files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dNlenvgshmJG"},"outputs":[],"source":["# set up the data for the WideResnet+CNN collaborative on softmax layer\n","import random\n","layer_name_sftmx_cnn = \"SFTMX1\"\n","layer_name_sftmx_wideresnets = \"CLASSIFIER_D1\"\n","total_num_components = [2,3,4,5,6,7,8,9,10]\n","num_of_repeats = 3\n","\n","idxCount = 0 if cnn_wideresnet_contour_data is None else len(cnn_wideresnet_contour_data.index)\n","\n","for repc in range(num_of_repeats):\n","  for component_count in total_num_components:\n","    for num_of_models_cnn in range(component_count+1):\n","      num_of_models_wideresnets = component_count - num_of_models_cnn\n","\n","      np_x_validation_collab_cnn, np_x_train_collab_cnn, np_x_test_collab_cnn = (None,None,None)\n","      np_x_validation_collab_wideresnets, np_x_train_collab_wideresnets, np_x_test_collab_wideresnets = (None,None,None)\n","      np_x_validation_collab_wrcdnn, np_x_train_collab_wrcdnn, np_x_test_collab_wrcdnn = (None, None, None)\n","\n","      if num_of_models_cnn > 0:\n","        np_x_validation_collab_cnn, np_x_train_collab_cnn, np_x_test_collab_cnn = get_features_for_layer(cnn_features_files,layer_name_sftmx_cnn,num_of_models_cnn)\n","\n","      if num_of_models_wideresnets > 0:\n","        np_x_validation_collab_wideresnets, np_x_train_collab_wideresnets, np_x_test_collab_wideresnets = get_features_for_layer_using_subbatches(wideresnets_features_files,layer_name_sftmx_wideresnets,num_of_models_wideresnets,model_type=\"WideResNet\")\n","      \n","      if num_of_models_cnn > 0 and num_of_models_wideresnets > 0:        \n","        np_x_train_collab_wrcdnn = np.concatenate([np_x_train_collab_cnn,np_x_train_collab_wideresnets], axis=1)\n","        np_x_validation_collab_wrcdnn = np.concatenate([np_x_validation_collab_cnn,np_x_validation_collab_wideresnets], axis=1)\n","        np_x_test_collab_wrcdnn = np.concatenate([np_x_test_collab_cnn,np_x_test_collab_wideresnets], axis=1)\n","      else: \n","        if num_of_models_cnn == 0:\n","          np_x_train_collab_wrcdnn = np_x_train_collab_wideresnets\n","          np_x_validation_collab_wrcdnn = np_x_validation_collab_wideresnets\n","          np_x_test_collab_wrcdnn = np_x_test_collab_wideresnets\n","        if num_of_models_wideresnets == 0:\n","          np_x_train_collab_wrcdnn = np_x_train_collab_cnn\n","          np_x_validation_collab_wrcdnn = np_x_validation_collab_cnn\n","          np_x_test_collab_wrcdnn = np_x_test_collab_cnn\n","      \n","      layer_name = f\"{layer_name_sftmx_cnn}_{layer_name_sftmx_wideresnets}_{str(num_of_models_cnn)}-{str(num_of_models_wideresnets)}\"\n","      model_here, history_here = compile_and_fit_model_basic( model_combination_of_features_with_flatten,  \n","                          f\"Collab_{layer_name}_CNNWRNTS_A_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","                          np_x_train_collab_wrcdnn[0,:].shape, \n","                          np_x_train_collab_wrcdnn, \n","                          train_targets,\n","                          save_max_epoch=False,\n","                          save_final=True,\n","                          patience_count = 35,\n","                          early_stopping_obs = 'val_sparse_categorical_accuracy',\n","                          log_history = True,\n","                          verbose_level=0,                             \n","                          batch_size=512, \n","                          epochs=250, \n","                          class_weight=None, \n","                          validation_data=(np_x_validation_collab_wrcdnn, validation_targets))\n","      del np_x_train_collab_wrcdnn\n","      del np_x_validation_collab_wrcdnn\n","      del np_x_validation_collab_cnn\n","      del np_x_train_collab_cnn\n","      del np_x_validation_collab_wideresnets\n","      del np_x_train_collab_wideresnets\n","      \n","      y_pred_model = model_here.predict(np_x_test_collab_wrcdnn)\n","      del np_x_test_collab_cnn\n","      del np_x_test_collab_wideresnets\n","      del np_x_test_collab_wrcdnn\n","\n","      y_pred = np.apply_along_axis(np.argmax, 1, y_pred_model) \n","      pr, rc, f1, acc = pr_rc_f1_acc_from_supplied(y_pred,test_targets)\n","      print (num_of_models_cnn, repc, pr, rc, f1, acc)\n","      del model_here\n","      del history_here\n","      if cnn_wideresnet_contour_data is None:\n","        cnn_wideresnet_contour_data = pd.DataFrame({\"TypeA\": \"CNN\", \n","                                                    \"TypeB\": \"WideResNet\", \n","                                                    \"Data\" : \"Test\",\n","                                                    \"NumOfA\": num_of_models_cnn, \n","                                                    \"NumOfB\": num_of_models_wideresnets, \n","                                                    \"RepC\": repc, \n","                                                    \"Pr\": pr,\n","                                                    \"Rc\": rc,\n","                                                    \"F1\": f1,\n","                                                    \"Acc\": acc,\n","                                                    }, index = [idxCount])\n","      else:\n","        cnn_wideresnet_contour_data = pd.concat([cnn_wideresnet_contour_data,\n","                                          pd.DataFrame({\"TypeA\": \"CNN\", \n","                                              \"TypeB\": \"WideResNet\", \n","                                              \"Data\" : \"Test\",\n","                                              \"NumOfA\": num_of_models_cnn, \n","                                              \"NumOfB\": num_of_models_wideresnets, \n","                                              \"RepC\": repc, \n","                                              \"Pr\": pr,\n","                                              \"Rc\": rc,\n","                                              \"F1\": f1,\n","                                              \"Acc\": acc\n","                                              }, index = [idxCount])\n","                                          ])\n","      idxCount = idxCount + 1\n","    cnn_wideresnet_contour_data.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/cnn_wideresnet_tempcontour_data_{datetime.datetime.now():%Y%m%d%H%M%S}.csv\")\n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_TsMTDnt4dx8"},"outputs":[],"source":["cnn_wideresnet_contour_data.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/cnn_wideresnet_contour_data_{datetime.datetime.now():%Y%m%d%H%M%S}.csv\")\n"]},{"cell_type":"markdown","metadata":{"id":"BvyzEp0EkgBD"},"source":["# Data for Contour Plot on Collaborative DNN/WideResNet softmax layers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bMV-tRMnkgBE"},"outputs":[],"source":["import random\n","import re\n","\n","def get_base_patterns_for_validation(features_files, layer_name = \"CLASSIFIER_D1\", model_type = \"WideResNet\"):\n","  base_patterns_for_validations = []\n","  for ff in [ s for s in features_files if \"Validation\" in s]:\n","    validation_search = re.search(f'^.*({model_type}.*_ID.*features_{layer_name}_Validation)[0-9]+.*$', ff, re.IGNORECASE)\n","    if validation_search:\n","        base_patterns_for_validations.append(validation_search.group(1))\n","\n","  base_patterns_for_validations = sorted(list(set(base_patterns_for_validations)))\n","  return base_patterns_for_validations\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nPUPFfJgkgBE"},"outputs":[],"source":["def get_features_for_layer_using_subbatches(feature_files, layer_name, num_of_models, model_type=\"WideResNet\", axis_to_concat = 1):\n"," \n","  feature_files_used = [ff for ff in feature_files if \"Validation\" in ff and \"_X\" in ff and f\"_{layer_name}_\" in ff ]\n","  \n","  # base_patterns_for_validations = []\n","  # for ff in [ s for s in feature_files_used ]:\n","  #   validation_search = re.search(f'^.*(_ID.*features_{layer_name}_Validation)[0-9]+.*$', ff, re.IGNORECASE)\n","  #   if validation_search:\n","  #       base_patterns_for_validations.append(validation_search.group(1))\n","  # base_patterns_for_validations = sorted(list(set(base_patterns_for_validations)))\n","\n","  base_patterns_for_validations = get_base_patterns_for_validation(feature_files_used, layer_name, model_type)\n","  base_patterns_for_validations = sorted(random.sample(base_patterns_for_validations, min(num_of_models,len(base_patterns_for_validations)) ))\n","\n","  np_x_validation_collab = None\n","  for base_val_str in base_patterns_for_validations:\n","      validation_batch_files = sorted([ ff for ff in feature_files_used if base_val_str in ff])\n","      np_x_validation_collab_batch = np.array([np.load(ff) for ff in validation_batch_files])\n","      np_x_validation_collab_batch = np.concatenate(np_x_validation_collab_batch, axis=0)\n","      if np_x_validation_collab is None:\n","        np_x_validation_collab = np_x_validation_collab_batch.copy()\n","      else:\n","        np_x_validation_collab = np.concatenate([np_x_validation_collab, np_x_validation_collab_batch], axis=axis_to_concat)\n","  \n","  np_x_test_collab = None\n","  for base_val_str in [ ff.replace(\"Validation\",\"Test\") for ff in base_patterns_for_validations]:\n","      test_batch_files = dir_has_file_with_regex(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features\",f\".*{base_val_str}.*_X.*$\")\n","      np_x_test_collab_batch = np.array([np.load(ff) for ff in test_batch_files])\n","      np_x_test_collab_batch = np.concatenate(np_x_test_collab_batch, axis=0)\n","      if np_x_test_collab is None:\n","        np_x_test_collab = np_x_test_collab_batch.copy()\n","      else:\n","        np_x_test_collab = np.concatenate([np_x_test_collab, np_x_test_collab_batch], axis=axis_to_concat)\n","\n","  np_x_train_collab = None\n","  for base_val_str in [ ff.replace(\"Validation\",\"Train\") for ff in base_patterns_for_validations]:\n","      train_batch_files = dir_has_file_with_regex(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features\",f\".*{base_val_str}.*_X.*$\")\n","      np_x_train_collab_batch = np.array([np.load(ff) for ff in train_batch_files])\n","      np_x_train_collab_batch = np.concatenate(np_x_train_collab_batch, axis=0)\n","      if np_x_train_collab is None:\n","        np_x_train_collab = np_x_train_collab_batch.copy()\n","      else:\n","        np_x_train_collab = np.concatenate([np_x_train_collab, np_x_train_collab_batch], axis=axis_to_concat)\n","\n","  return np_x_validation_collab, np_x_train_collab, np_x_test_collab\n","\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GJsVRidtkgBE"},"outputs":[],"source":["# set up the combinations we are going to try\n","# len(wideresnets_features_files)\n","dnn_wideresnet_contour_data = None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jUNzYWylkgBE"},"outputs":[],"source":["dnn_features_files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mMbDrr35kgBE"},"outputs":[],"source":["# set up the data for the WideResnet+CNN collaborative on softmax layer\n","import random\n","layer_name_sftmx_dnn = \"SFTMX1\"\n","layer_name_sftmx_wideresnets = \"CLASSIFIER_D1\"\n","total_num_components = [2,3,4,5,6,7,8,9,10]\n","num_of_repeats = 3\n","\n","idxCount = 0 if dnn_wideresnet_contour_data is None else len(dnn_wideresnet_contour_data.index)\n","\n","for repc in range(num_of_repeats):\n","  for component_count in total_num_components:\n","    for num_of_models_dnn in range(component_count+1):\n","\n","      num_of_models_wideresnets = component_count - num_of_models_dnn\n","\n","      np_x_validation_collab_dnn, np_x_train_collab_dnn, np_x_test_collab_dnn = (None,None,None)\n","      np_x_validation_collab_wideresnets, np_x_train_collab_wideresnets, np_x_test_collab_wideresnets = (None,None,None)\n","      np_x_validation_collab_wrcdnn, np_x_train_collab_wrcdnn, np_x_test_collab_wrcdnn = (None, None, None)\n","\n","      if num_of_models_dnn > 0:\n","        np_x_validation_collab_dnn, np_x_train_collab_dnn, np_x_test_collab_dnn = get_features_for_layer(dnn_features_files,layer_name_sftmx_dnn,num_of_models_dnn)\n","\n","      if num_of_models_wideresnets > 0:\n","        np_x_validation_collab_wideresnets, np_x_train_collab_wideresnets, np_x_test_collab_wideresnets = get_features_for_layer_using_subbatches(wideresnets_features_files,layer_name_sftmx_wideresnets,num_of_models_wideresnets,model_type=\"WideResNet\")\n","      \n","      if num_of_models_dnn > 0 and num_of_models_wideresnets > 0:        \n","        np_x_train_collab_wrcdnn = np.concatenate([np_x_train_collab_dnn,np_x_train_collab_wideresnets], axis=1)\n","        np_x_validation_collab_wrcdnn = np.concatenate([np_x_validation_collab_dnn,np_x_validation_collab_wideresnets], axis=1)\n","        np_x_test_collab_wrcdnn = np.concatenate([np_x_test_collab_dnn,np_x_test_collab_wideresnets], axis=1)\n","      else: \n","        if num_of_models_dnn == 0:\n","          np_x_train_collab_wrcdnn = np_x_train_collab_wideresnets\n","          np_x_validation_collab_wrcdnn = np_x_validation_collab_wideresnets\n","          np_x_test_collab_wrcdnn = np_x_test_collab_wideresnets\n","        if num_of_models_wideresnets == 0:\n","          np_x_train_collab_wrcdnn = np_x_train_collab_dnn\n","          np_x_validation_collab_wrcdnn = np_x_validation_collab_dnn\n","          np_x_test_collab_wrcdnn = np_x_test_collab_dnn\n","      \n","      layer_name = f\"{layer_name_sftmx_dnn}_{layer_name_sftmx_wideresnets}_{str(num_of_models_dnn)}-{str(num_of_models_wideresnets)}\"\n","      \n","      model_here, history_here = compile_and_fit_model_basic( model_combination_of_features_with_flatten,  \n","                          f\"Collab_{layer_name}_DNNWRNTS_A_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","                          np_x_train_collab_wrcdnn[0,:].shape, \n","                          np_x_train_collab_wrcdnn, \n","                          train_targets,\n","                          save_max_epoch=False,\n","                          save_final=True,\n","                          patience_count = 35,\n","                          early_stopping_obs = 'val_sparse_categorical_accuracy',\n","                          log_history = True,\n","                          verbose_level=0,                             \n","                          batch_size=512, \n","                          epochs=250, \n","                          class_weight=None, \n","                          validation_data=(np_x_validation_collab_wrcdnn, validation_targets))\n","      del np_x_train_collab_wrcdnn\n","      del np_x_validation_collab_wrcdnn\n","      del np_x_validation_collab_dnn\n","      del np_x_train_collab_dnn\n","      del np_x_validation_collab_wideresnets\n","      del np_x_train_collab_wideresnets\n","      \n","      y_pred_model = model_here.predict(np_x_test_collab_wrcdnn)\n","      del np_x_test_collab_dnn\n","      del np_x_test_collab_wideresnets\n","      del np_x_test_collab_wrcdnn\n","\n","      y_pred = np.apply_along_axis(np.argmax, 1, y_pred_model) \n","      pr, rc, f1, acc = pr_rc_f1_acc_from_supplied(y_pred,test_targets)\n","      print (num_of_models_dnn, repc, pr, rc, f1, acc)\n","      del model_here\n","      del history_here\n","      if dnn_wideresnet_contour_data is None:\n","        dnn_wideresnet_contour_data = pd.DataFrame({\"TypeA\": \"DNN\", \n","                                                    \"TypeB\": \"WideResNet\", \n","                                                    \"Data\" : \"Test\",\n","                                                    \"NumOfA\": num_of_models_dnn, \n","                                                    \"NumOfB\": num_of_models_wideresnets, \n","                                                    \"RepC\": repc, \n","                                                    \"Pr\": pr,\n","                                                    \"Rc\": rc,\n","                                                    \"F1\": f1,\n","                                                    \"Acc\": acc,\n","                                                    }, index = [idxCount])\n","      else:\n","        dnn_wideresnet_contour_data = pd.concat([dnn_wideresnet_contour_data,\n","                                          pd.DataFrame({\"TypeA\": \"DNN\", \n","                                              \"TypeB\": \"WideResNet\", \n","                                              \"Data\" : \"Test\",\n","                                              \"NumOfA\": num_of_models_dnn, \n","                                              \"NumOfB\": num_of_models_wideresnets, \n","                                              \"RepC\": repc, \n","                                              \"Pr\": pr,\n","                                              \"Rc\": rc,\n","                                              \"F1\": f1,\n","                                              \"Acc\": acc\n","                                              }, index = [idxCount])\n","                                          ])\n","      idxCount = idxCount + 1\n","    dnn_wideresnet_contour_data.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/dnn_wideresnet_tempcontour_data_{datetime.datetime.now():%Y%m%d%H%M%S}.csv\")\n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vrB5oFXEkgBF"},"outputs":[],"source":["dnn_wideresnet_contour_data.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/dnn_wideresnet_contour_data_{datetime.datetime.now():%Y%m%d%H%M%S}.csv\")\n"]},{"cell_type":"markdown","metadata":{"id":"nKZebazXuPRf"},"source":["# Data for Contour Plot on Collaborative CNN/DNN softmax layers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rUFA68nKuPRf"},"outputs":[],"source":["import random\n","import re\n","\n","def get_base_patterns_for_validation(features_files, layer_name = \"CLASSIFIER_D1\", model_type = \"WideResNet\"):\n","  base_patterns_for_validations = []\n","  for ff in [ s for s in features_files if \"Validation\" in s]:\n","    validation_search = re.search(f'^.*({model_type}.*_ID.*features_{layer_name}_Validation)[0-9]+.*$', ff, re.IGNORECASE)\n","    if validation_search:\n","        base_patterns_for_validations.append(validation_search.group(1))\n","\n","  base_patterns_for_validations = sorted(list(set(base_patterns_for_validations)))\n","  return base_patterns_for_validations\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-nmVKaDguPRf"},"outputs":[],"source":["# set up the combinations we are going to try\n","# len(wideresnets_features_files)\n","cnn_dnn_contour_data = None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iWjI91pDuPRf"},"outputs":[],"source":["# set up the data for the WideResnet+CNN collaborative on softmax layer\n","import random\n","layer_name_sftmx_cnn = \"SFTMX1\"\n","layer_name_sftmx_dnn = \"SFTMX1\"\n","total_num_components = [2,4,6,8,10,12,14,16,18,20,24,28,32,36,38,40]\n","num_of_repeats = 3\n","\n","idxCount = 0 if cnn_dnn_contour_data is None else len(cnn_dnn_contour_data.index)\n","\n","for repc in range(num_of_repeats):\n","  for component_count in total_num_components:\n","    for num_of_models_cnn in range(component_count+1):\n","      num_of_models_dnn = component_count - num_of_models_cnn\n","\n","      np_x_validation_collab_cnn, np_x_train_collab_cnn, np_x_test_collab_cnn = (None,None,None)\n","      np_x_validation_collab_dnn, np_x_train_collab_dnn, np_x_test_collab_dnn = (None,None,None)\n","      np_x_validation_collab_wrcdnn, np_x_train_collab_wrcdnn, np_x_test_collab_wrcdnn = (None, None, None)\n","\n","      if num_of_models_cnn > 0:\n","        np_x_validation_collab_cnn, np_x_train_collab_cnn, np_x_test_collab_cnn = get_features_for_layer(cnn_features_files,layer_name_sftmx_cnn,num_of_models_cnn)\n","\n","      if num_of_models_dnn > 0:\n","        np_x_validation_collab_dnn, np_x_train_collab_dnn, np_x_test_collab_dnn = get_features_for_layer(dnn_features_files,layer_name_sftmx_dnn,num_of_models_dnn)\n","      \n","      if num_of_models_cnn > 0 and num_of_models_dnn > 0:        \n","        np_x_train_collab_wrcdnn = np.concatenate([np_x_train_collab_cnn,np_x_train_collab_dnn], axis=1)\n","        np_x_validation_collab_wrcdnn = np.concatenate([np_x_validation_collab_cnn,np_x_validation_collab_dnn], axis=1)\n","        np_x_test_collab_wrcdnn = np.concatenate([np_x_test_collab_cnn,np_x_test_collab_dnn], axis=1)\n","      else: \n","        if num_of_models_cnn == 0:\n","          np_x_train_collab_wrcdnn = np_x_train_collab_dnn\n","          np_x_validation_collab_wrcdnn = np_x_validation_collab_dnn\n","          np_x_test_collab_wrcdnn = np_x_test_collab_dnn\n","        if num_of_models_dnn == 0:\n","          np_x_train_collab_wrcdnn = np_x_train_collab_cnn\n","          np_x_validation_collab_wrcdnn = np_x_validation_collab_cnn\n","          np_x_test_collab_wrcdnn = np_x_test_collab_cnn\n","      \n","      layer_name = f\"{layer_name_sftmx_cnn}_{layer_name_sftmx_dnn}_{str(num_of_models_cnn)}-{str(num_of_models_dnn)}\"\n","      model_here, history_here = compile_and_fit_model_basic( model_combination_of_features_with_flatten,  \n","                          f\"Collab_{layer_name}_CNNDNN_A_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","                          np_x_train_collab_wrcdnn[0,:].shape, \n","                          np_x_train_collab_wrcdnn, \n","                          train_targets,\n","                          save_max_epoch=False,\n","                          save_final=True,\n","                          patience_count = 35,\n","                          early_stopping_obs = 'val_sparse_categorical_accuracy',\n","                          log_history = True,\n","                          verbose_level=0,                             \n","                          batch_size=512, \n","                          epochs=250, \n","                          class_weight=None, \n","                          validation_data=(np_x_validation_collab_wrcdnn, validation_targets))\n","      del np_x_train_collab_wrcdnn\n","      del np_x_validation_collab_wrcdnn\n","      del np_x_validation_collab_cnn\n","      del np_x_train_collab_cnn\n","      del np_x_validation_collab_dnn\n","      del np_x_train_collab_dnn\n","      \n","      y_pred_model = model_here.predict(np_x_test_collab_wrcdnn)\n","      del np_x_test_collab_cnn\n","      del np_x_test_collab_dnn\n","      del np_x_test_collab_wrcdnn\n","\n","      y_pred = np.apply_along_axis(np.argmax, 1, y_pred_model) \n","      pr, rc, f1, acc = pr_rc_f1_acc_from_supplied(y_pred,test_targets)\n","      print (num_of_models_cnn, repc, pr, rc, f1, acc)\n","      del model_here\n","      del history_here\n","      if cnn_dnn_contour_data is None:\n","        cnn_dnn_contour_data = pd.DataFrame({\"TypeA\": \"CNN\", \n","                                                    \"TypeB\": \"DNN\", \n","                                                    \"Data\" : \"Test\",\n","                                                    \"NumOfA\": num_of_models_cnn, \n","                                                    \"NumOfB\": num_of_models_dnn, \n","                                                    \"RepC\": repc, \n","                                                    \"Pr\": pr,\n","                                                    \"Rc\": rc,\n","                                                    \"F1\": f1,\n","                                                    \"Acc\": acc,\n","                                                    }, index = [idxCount])\n","      else:\n","        cnn_dnn_contour_data = pd.concat([cnn_dnn_contour_data,\n","                                          pd.DataFrame({\"TypeA\": \"CNN\", \n","                                              \"TypeB\": \"DNN\", \n","                                              \"Data\" : \"Test\",\n","                                              \"NumOfA\": num_of_models_cnn, \n","                                              \"NumOfB\": num_of_models_dnn, \n","                                              \"RepC\": repc, \n","                                              \"Pr\": pr,\n","                                              \"Rc\": rc,\n","                                              \"F1\": f1,\n","                                              \"Acc\": acc\n","                                              }, index = [idxCount])\n","                                          ])\n","      idxCount = idxCount + 1\n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2iN0aQPfuPRg"},"outputs":[],"source":["cnn_dnn_contour_data.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/cnn_dnn_contour_data_{datetime.datetime.now():%Y%m%d%H%M%S}.csv\")\n"]},{"cell_type":"markdown","metadata":{"id":"sddcE82tOFDm"},"source":["# Data for Contour Plot on Collaborative CNN/DNN lastdense layers _In Progress_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TuFWFx6YOFDn"},"outputs":[],"source":["import random\n","import re\n","\n","def get_base_patterns_for_validation(features_files, layer_name = \"CLASSIFIER_D1\", model_type = \"WideResNet\"):\n","  base_patterns_for_validations = []\n","  for ff in [ s for s in features_files if \"Validation\" in s]:\n","    validation_search = re.search(f'^.*({model_type}.*_ID.*features_{layer_name}_Validation)[0-9]+.*$', ff, re.IGNORECASE)\n","    if validation_search:\n","        base_patterns_for_validations.append(validation_search.group(1))\n","\n","  base_patterns_for_validations = sorted(list(set(base_patterns_for_validations)))\n","  return base_patterns_for_validations\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QukEjJMcOFDn"},"outputs":[],"source":["# set up the combinations we are going to try\n","# len(wideresnets_features_files)\n","cnn_dnn_ld_contour_data = None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C6cY5BzAOFDn"},"outputs":[],"source":["# set up the data for the DNN+CNN collaborative on lastdense layer\n","import random\n","layer_name_penultimate_cnn = \"DRP1\"\n","layer_name_penultimate_dnn = \"D3R\"\n","total_num_components = [25, 30, 35, 40] # [2,4,6,8,10,12,14,16,18,20]\n","num_of_repeats = 2\n","\n","idxCount = 0 if cnn_dnn_ld_contour_data is None else len(cnn_dnn_ld_contour_data.index)\n","\n","for repc in range(num_of_repeats):\n","  for component_count in total_num_components:\n","    for num_of_models_cnn in range(component_count+1):\n","      num_of_models_dnn = component_count - num_of_models_cnn\n","\n","      np_x_validation_collab_cnn, np_x_train_collab_cnn, np_x_test_collab_cnn = (None,None,None)\n","      np_x_validation_collab_dnn, np_x_train_collab_dnn, np_x_test_collab_dnn = (None,None,None)\n","      np_x_validation_collab_wrcdnn, np_x_train_collab_wrcdnn, np_x_test_collab_wrcdnn = (None, None, None)\n","\n","      if num_of_models_cnn > 0:\n","        np_x_validation_collab_cnn, np_x_train_collab_cnn, np_x_test_collab_cnn = get_features_for_layer(cnn_features_files,layer_name_penultimate_cnn,num_of_models_cnn)\n","\n","      if num_of_models_dnn > 0:\n","        np_x_validation_collab_dnn, np_x_train_collab_dnn, np_x_test_collab_dnn = get_features_for_layer(dnn_features_files,layer_name_penultimate_dnn,num_of_models_dnn)\n","      \n","      if num_of_models_cnn > 0 and num_of_models_dnn > 0:        \n","        np_x_train_collab_wrcdnn = np.concatenate([np_x_train_collab_cnn,np_x_train_collab_dnn], axis=1)\n","        np_x_validation_collab_wrcdnn = np.concatenate([np_x_validation_collab_cnn,np_x_validation_collab_dnn], axis=1)\n","        np_x_test_collab_wrcdnn = np.concatenate([np_x_test_collab_cnn,np_x_test_collab_dnn], axis=1)\n","      else: \n","        if num_of_models_cnn == 0:\n","          np_x_train_collab_wrcdnn = np_x_train_collab_dnn\n","          np_x_validation_collab_wrcdnn = np_x_validation_collab_dnn\n","          np_x_test_collab_wrcdnn = np_x_test_collab_dnn\n","        if num_of_models_dnn == 0:\n","          np_x_train_collab_wrcdnn = np_x_train_collab_cnn\n","          np_x_validation_collab_wrcdnn = np_x_validation_collab_cnn\n","          np_x_test_collab_wrcdnn = np_x_test_collab_cnn\n","      \n","      layer_name = f\"{layer_name_penultimate_cnn}_{layer_name_penultimate_dnn}_{str(num_of_models_cnn)}-{str(num_of_models_dnn)}\"\n","      model_here, history_here = compile_and_fit_model_basic( model_combination_of_features_with_flatten,  \n","                          f\"Collab_{layer_name}_CNNDNN_A_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","                          np_x_train_collab_wrcdnn[0,:].shape, \n","                          np_x_train_collab_wrcdnn, \n","                          train_targets,\n","                          save_max_epoch=False,\n","                          save_final=True,\n","                          patience_count = 35,\n","                          early_stopping_obs = 'val_sparse_categorical_accuracy',\n","                          log_history = True,\n","                          verbose_level=0,                             \n","                          batch_size=512, \n","                          epochs=250, \n","                          class_weight=None, \n","                          validation_data=(np_x_validation_collab_wrcdnn, validation_targets))\n","      del np_x_train_collab_wrcdnn\n","      del np_x_validation_collab_wrcdnn\n","      del np_x_validation_collab_cnn\n","      del np_x_train_collab_cnn\n","      del np_x_validation_collab_dnn\n","      del np_x_train_collab_dnn\n","      \n","      y_pred_model = model_here.predict(np_x_test_collab_wrcdnn)\n","      del np_x_test_collab_cnn\n","      del np_x_test_collab_dnn\n","      del np_x_test_collab_wrcdnn\n","\n","      y_pred = np.apply_along_axis(np.argmax, 1, y_pred_model) \n","      pr, rc, f1, acc = pr_rc_f1_acc_from_supplied(y_pred,test_targets)\n","      print (num_of_models_cnn, repc, pr, rc, f1, acc)\n","      del model_here\n","      del history_here\n","      if cnn_dnn_ld_contour_data is None:\n","        cnn_dnn_ld_contour_data = pd.DataFrame({\"TypeA\": \"CNN\", \n","                                                    \"TypeB\": \"DNN\", \n","                                                    \"Data\" : \"Test\",\n","                                                    \"NumOfA\": num_of_models_cnn, \n","                                                    \"NumOfB\": num_of_models_dnn, \n","                                                    \"Layer\": \"LastDense\",\n","                                                    \"RepC\": repc, \n","                                                    \"Pr\": pr,\n","                                                    \"Rc\": rc,\n","                                                    \"F1\": f1,\n","                                                    \"Acc\": acc,\n","                                                    }, index = [idxCount])\n","      else:\n","        cnn_dnn_ld_contour_data = pd.concat([cnn_dnn_ld_contour_data,\n","                                          pd.DataFrame({\"TypeA\": \"CNN\", \n","                                              \"TypeB\": \"DNN\", \n","                                              \"Data\" : \"Test\",\n","                                              \"NumOfA\": num_of_models_cnn, \n","                                              \"NumOfB\": num_of_models_dnn, \n","                                              \"Layer\": \"LastDense\",\n","                                              \"RepC\": repc, \n","                                              \"Pr\": pr,\n","                                              \"Rc\": rc,\n","                                              \"F1\": f1,\n","                                              \"Acc\": acc\n","                                              }, index = [idxCount])\n","                                          ])\n","      idxCount = idxCount + 1\n","    cnn_dnn_ld_contour_data.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/temp/cnn_dnn_ldtemp_contour_data_{datetime.datetime.now():%Y%m%d%H%M%S}.csv\")\n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jXbwuusKOFDo"},"outputs":[],"source":["cnn_dnn_ld_contour_data.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/cnn_dnn_ld_contour_data_{datetime.datetime.now():%Y%m%d%H%M%S}.csv\")\n"]},{"cell_type":"markdown","metadata":{"id":"FWDC54QRQhyy"},"source":["# Data for Plot on Collaborative DNN last dense layers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jckmvYAKQhyy"},"outputs":[],"source":["collaborativeFullyC_lastdense_data = None\n","\n","x_input = test_data_grey\n","y_input = test_targets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qzVNJs1VQhyy"},"outputs":[],"source":["# dnn_features_files_here = [ f for f in dnn_features_files if layer_name_sfmx_dnn in f]\n","# dnn_features_files_here\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o51PtIPEQhyy"},"outputs":[],"source":["# set up the data for the DNN collaborative\n","\n","import random\n","import datetime\n","\n","num_of_repeats = 4\n","num_of_models = [25,30,35,40]\n","# [2,3,4,5,6,8,10,12,14,16,18,20]\n","layer_name_penultimate_dnn = \"D3R\"\n","\n","idxCount = 0 if collaborativeFullyC_lastdense_data is None else len(collaborativeFullyC_lastdense_data.index)\n","for repc in range(num_of_repeats):\n","  for mc in num_of_models:\n","    model_name = f\"Collab_{layer_name_penultimate_dnn}_DNN_A_{datetime.datetime.now():%Y%m%d%H%M%S}\"\n","    np_x_validation_collab_dnn_penultimate, np_y_train_collab_dnn_penultimate, np_x_test_collab_dnn_penultimate = get_features_for_layer(dnn_features_files, layer_name_penultimate_dnn, mc)\n","    model_here, history_here = compile_and_fit_model_basic( model_combination_of_features,  \n","                        model_name, \n","                        np_y_train_collab_dnn_penultimate[0,:].shape, \n","                        np_y_train_collab_dnn_penultimate, \n","                        train_targets,\n","                        save_max_epoch = False,\n","                        save_final = True,\n","                        patience_count = 35,\n","                        early_stopping_obs = 'val_sparse_categorical_accuracy',\n","                        log_history = True,\n","                        verbose_level=0,                             \n","                        batch_size=512, \n","                        epochs=250, \n","                        class_weight=None, \n","                        validation_data=(np_x_validation_collab_dnn_penultimate, validation_targets))\n","    del np_x_validation_collab_dnn_penultimate\n","    del np_y_train_collab_dnn_penultimate\n","    y_pred_model = model_here.predict(np_x_test_collab_dnn_penultimate)\n","    del np_x_test_collab_dnn_penultimate\n","    y_pred = np.apply_along_axis(np.argmax, 1, y_pred_model) \n","    pr, rc, f1, acc = pr_rc_f1_acc_from_supplied(y_pred,test_targets)\n","    print (mc, repc, pr, rc, f1, acc)\n","    del model_here\n","    del history_here\n","    if collaborativeFullyC_lastdense_data is None:\n","      collaborativeFullyC_lastdense_data = pd.DataFrame({\"Type\": \"DNN\", \n","                                            \"Data\" : \"Test\",\n","                                            \"Layer\" : layer_name_penultimate_dnn,\n","                                            \"NumOfModels\": mc, \n","                                            \"RepC\": repc, \n","                                            \"Pr\": pr,\n","                                            \"Rc\": rc,\n","                                            \"F1\": f1,\n","                                            \"Acc\": acc,\n","                                            }, index = [idxCount])\n","    else:\n","      collaborativeFullyC_lastdense_data = pd.concat([collaborativeFullyC_lastdense_data,\n","                                         pd.DataFrame({\"Type\": \"DNN\", \n","                                            \"Data\" : \"Test\",\n","                                            \"Layer\" : layer_name_penultimate_dnn,\n","                                            \"NumOfModels\": mc, \n","                                            \"RepC\": repc, \n","                                            \"Pr\": pr,\n","                                            \"Rc\": rc,\n","                                            \"F1\": f1,\n","                                            \"Acc\": acc\n","                                            }, index = [idxCount])\n","                                         ])\n","    idxCount = idxCount + 1\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w5v1LfD_Qhyy"},"outputs":[],"source":["collaborativeFullyC_lastdense_data.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/dnn_collab_lastdense_test_results_{datetime.datetime.now():%Y%m%d%H%M%S}.csv\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hx1fELMRQhyy"},"source":["# Data for Plot on Collaborative CNN last dense layers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jse5z9uRQhyy"},"outputs":[],"source":["# carry this over from DNN if possible (so keep commented out)\n","## collaborativeFullyC_softmax_data = None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Ej69RilQhyz"},"outputs":[],"source":["# set up the data for the CNN collaborative\n","\n","import random\n","import datetime\n","\n","num_of_repeats = 4\n","num_of_models = [35, 40] # [22, 25,30]\n","# [2,3,4,5,6,8,10,12,14,16,18,20]\n","layer_name_penultimate_cnn = \"DRP1\"\n","\n","idxCount = 0 if collaborativeFullyC_lastdense_data is None else len(collaborativeFullyC_lastdense_data.index)\n","for repc in range(num_of_repeats):\n","  for mc in num_of_models:\n","    model_name = f\"Collab_{layer_name_penultimate_cnn}_CNN_A_{datetime.datetime.now():%Y%m%d%H%M%S}\"\n","    np_x_validation_collab_cnn_penultimate, np_y_train_collab_cnn_penultimate, np_x_test_collab_cnn_penultimate = get_features_for_layer(cnn_features_files, layer_name_penultimate_cnn, mc)\n","    model_here, history_here = compile_and_fit_model_basic( model_combination_of_features,  \n","                        model_name, \n","                        np_y_train_collab_cnn_penultimate[0,:].shape, \n","                        np_y_train_collab_cnn_penultimate, \n","                        train_targets,\n","                        save_max_epoch = False,\n","                        save_final = True,\n","                        patience_count = 35,\n","                        early_stopping_obs = 'val_sparse_categorical_accuracy',\n","                        log_history = True,\n","                        verbose_level=0,                             \n","                        batch_size=512, \n","                        epochs=250, \n","                        class_weight=None, \n","                        validation_data=(np_x_validation_collab_cnn_penultimate, validation_targets))\n","    del np_x_validation_collab_cnn_penultimate\n","    del np_y_train_collab_cnn_penultimate\n","    y_pred_model = model_here.predict(np_x_test_collab_cnn_penultimate)\n","    del np_x_test_collab_cnn_penultimate\n","    y_pred = np.apply_along_axis(np.argmax, 1, y_pred_model) \n","    pr, rc, f1, acc = pr_rc_f1_acc_from_supplied(y_pred,test_targets)\n","    print (mc, repc, pr, rc, f1, acc)\n","    del model_here\n","    del history_here\n","    if collaborativeFullyC_lastdense_data is None:\n","      collaborativeFullyC_lastdense_data = pd.DataFrame({\"Type\": \"CNN\", \n","                                            \"Data\" : \"Test\",\n","                                            \"Layer\" : layer_name_penultimate_cnn,\n","                                            \"NumOfModels\": mc, \n","                                            \"RepC\": repc, \n","                                            \"Pr\": pr,\n","                                            \"Rc\": rc,\n","                                            \"F1\": f1,\n","                                            \"Acc\": acc,\n","                                            }, index = [idxCount])\n","    else:\n","      collaborativeFullyC_lastdense_data = pd.concat([collaborativeFullyC_lastdense_data,\n","                                         pd.DataFrame({\"Type\": \"CNN\", \n","                                            \"Data\" : \"Test\",\n","                                            \"Layer\" : layer_name_penultimate_cnn,\n","                                            \"NumOfModels\": mc, \n","                                            \"RepC\": repc, \n","                                            \"Pr\": pr,\n","                                            \"Rc\": rc,\n","                                            \"F1\": f1,\n","                                            \"Acc\": acc\n","                                            }, index = [idxCount])\n","                                         ])\n","    idxCount = idxCount + 1\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k2iGJLv1Qhyz"},"outputs":[],"source":["collaborativeFullyC_lastdense_data.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/cnn_collab_lastdense_test_results_{datetime.datetime.now():%Y%m%d%H%M%S}.csv\")\n"]},{"cell_type":"markdown","metadata":{"id":"io3MAwHuQhyz"},"source":["# Data for Plot on Collaborative WideResNet last dense layers (does not work)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XKjiiIoMQhyz"},"outputs":[],"source":["# carry this over from DNN/CNN if possible (so keep commented out)\n","## collaborativeFullyC_softmax_data = None\n","\n","# wideresnets_features_files\n","# cnn_features_files\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KGA4KvQeQhyz"},"outputs":[],"source":["import random\n","import re\n","\n","def get_base_patterns_for_validation(features_files, layer_name = \"CLASSIFIER_D1\", model_type = \"WideResNet\"):\n","  base_patterns_for_validations = []\n","  for ff in [ s for s in features_files if \"Validation\" in s]:\n","    validation_search = re.search(f'^.*({model_type}.*_ID.*features_{layer_name}_Validation)[0-9]+.*$', ff, re.IGNORECASE)\n","    if validation_search:\n","        base_patterns_for_validations.append(validation_search.group(1))\n","\n","  base_patterns_for_validations = sorted(list(set(base_patterns_for_validations)))\n","  return base_patterns_for_validations\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TaD_SmR_Qhyz"},"outputs":[],"source":["def get_features_for_layer_using_subbatches(feature_files, layer_name, num_of_models, model_type=\"WideResNet\", axis_to_concat = 1):\n"," \n","  feature_files_used = [ff for ff in feature_files if \"Validation\" in ff and \"_X\" in ff and f\"_{layer_name}_\" in ff ]\n","  \n","  # base_patterns_for_validations = []\n","  # for ff in [ s for s in feature_files_used ]:\n","  #   validation_search = re.search(f'^.*(_ID.*features_{layer_name}_Validation)[0-9]+.*$', ff, re.IGNORECASE)\n","  #   if validation_search:\n","  #       base_patterns_for_validations.append(validation_search.group(1))\n","  # base_patterns_for_validations = sorted(list(set(base_patterns_for_validations)))\n","\n","  base_patterns_for_validations = get_base_patterns_for_validation(feature_files_used, layer_name, model_type)\n","  base_patterns_for_validations = sorted(random.sample(base_patterns_for_validations, min(num_of_models,len(base_patterns_for_validations)) ))\n","\n","  np_x_validation_collab = None\n","  for base_val_str in base_patterns_for_validations:\n","      validation_batch_files = sorted([ ff for ff in feature_files_used if base_val_str in ff])\n","      np_x_validation_collab_batch = np.array([np.load(ff) for ff in validation_batch_files])\n","      np_x_validation_collab_batch = np.concatenate(np_x_validation_collab_batch, axis=0)\n","      if np_x_validation_collab is None:\n","        np_x_validation_collab = np_x_validation_collab_batch.copy()\n","      else:\n","        np_x_validation_collab = np.concatenate([np_x_validation_collab, np_x_validation_collab_batch], axis=axis_to_concat)\n","  \n","  np_x_test_collab = None\n","  for base_val_str in [ ff.replace(\"Validation\",\"Test\") for ff in base_patterns_for_validations]:\n","      test_batch_files = dir_has_file_with_regex(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features\",f\".*{base_val_str}.*_X.*$\")\n","      np_x_test_collab_batch = np.array([np.load(ff) for ff in test_batch_files])\n","      np_x_test_collab_batch = np.concatenate(np_x_test_collab_batch, axis=0)\n","      if np_x_test_collab is None:\n","        np_x_test_collab = np_x_test_collab_batch.copy()\n","      else:\n","        np_x_test_collab = np.concatenate([np_x_test_collab, np_x_test_collab_batch], axis=axis_to_concat)\n","\n","  np_x_train_collab = None\n","  for base_val_str in [ ff.replace(\"Validation\",\"Train\") for ff in base_patterns_for_validations]:\n","      train_batch_files = dir_has_file_with_regex(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features\",f\".*{base_val_str}.*_X.*$\")\n","      np_x_train_collab_batch = np.array([np.load(ff) for ff in train_batch_files])\n","      np_x_train_collab_batch = np.concatenate(np_x_train_collab_batch, axis=0)\n","      if np_x_train_collab is None:\n","        np_x_train_collab = np_x_train_collab_batch.copy()\n","      else:\n","        np_x_train_collab = np.concatenate([np_x_train_collab, np_x_train_collab_batch], axis=axis_to_concat)\n","\n","  return np_x_validation_collab, np_x_train_collab, np_x_test_collab"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qka-rNJfQhyz"},"outputs":[],"source":["# set up the data for the WideResNet collaborative\n","\n","import random\n","import datetime\n","\n","num_of_repeats = 4\n","num_of_models = [2,3,4,5,6,7,8,9,10]\n","layer_name_penultimate_wideresnets = \"CLASSIFIER_FL\"\n","model_type = \"WideResNet\"\n","\n","idxCount = 0 if collaborativeFullyC_lastdense_data is None else len(collaborativeFullyC_lastdense_data.index)\n","for repc in range(num_of_repeats):\n","  for mc in num_of_models:\n","    model_name = f\"Collab_{layer_name_penultimate_wideresnets}_WideResNet_A_{datetime.datetime.now():%Y%m%d%H%M%S}\"\n","    np_x_validation_collab_wideresnets_penultimate, np_y_train_collab_wideresnets_penultimate, np_x_test_collab_wideresnets_penultimate = get_features_for_layer_using_subbatches(wideresnets_features_files,layer_name_penultimate_wideresnets,mc,model_type )\n","    model_here, history_here = compile_and_fit_model_basic( model_combination_of_features,  \n","                        model_name, \n","                        np_y_train_collab_wideresnets_penultimate[0,:].shape, \n","                        np_y_train_collab_wideresnets_penultimate, \n","                        train_targets,\n","                        save_max_epoch = False,\n","                        save_final = True,\n","                        patience_count = 35,\n","                        early_stopping_obs = 'val_sparse_categorical_accuracy',\n","                        log_history = True,\n","                        verbose_level=0,                             \n","                        batch_size=512, \n","                        epochs=250, \n","                        class_weight=None, \n","                        validation_data=(np_x_validation_collab_wideresnets_penultimate, validation_targets))\n","    del np_x_validation_collab_wideresnets_penultimate\n","    del np_y_train_collab_wideresnets_penultimate\n","    y_pred_model = model_here.predict(np_x_test_collab_wideresnets_penultimate)\n","    del np_x_test_collab_wideresnets_penultimate\n","    y_pred = np.apply_along_axis(np.argmax, 1, y_pred_model) \n","    pr, rc, f1, acc = pr_rc_f1_acc_from_supplied(y_pred,test_targets)\n","    print (mc, repc, pr, rc, f1, acc)\n","    del model_here\n","    del history_here\n","    if collaborativeFullyC_lastdense_data is None:\n","      collaborativeFullyC_lastdense_data = pd.DataFrame({\"Type\": \"WideResNet\", \n","                                            \"Data\" : \"Test\",\n","                                            \"Layer\" : layer_name_penultimate_wideresnets,\n","                                            \"NumOfModels\": mc, \n","                                            \"RepC\": repc, \n","                                            \"Pr\": pr,\n","                                            \"Rc\": rc,\n","                                            \"F1\": f1,\n","                                            \"Acc\": acc,\n","                                            }, index = [idxCount])\n","    else:\n","      collaborativeFullyC_lastdense_data = pd.concat([collaborativeFullyC_lastdense_data,\n","                                         pd.DataFrame({\"Type\": \"WideResNet\", \n","                                            \"Data\" : \"Test\",\n","                                            \"Layer\" : layer_name_penultimate_wideresnets,\n","                                            \"NumOfModels\": mc, \n","                                            \"RepC\": repc, \n","                                            \"Pr\": pr,\n","                                            \"Rc\": rc,\n","                                            \"F1\": f1,\n","                                            \"Acc\": acc\n","                                            }, index = [idxCount])\n","                                         ])\n","    collaborativeFullyC_lastdense_data.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/wideresnet_collab_lastdense_interim_test_results_{datetime.datetime.now():%Y%m%d%H%M%S}.csv\")\n","    idxCount = idxCount + 1\n","\n","\n","# np_x_validation_collab_wideresnets, np_x_train_collab_wideresnets, np_x_test_collab_wideresnets =  get_features_for_layer_using_subbatches(wideresnets_features_files,layer_name,num_of_models,model_type )\n","# wrsftmx, wrsftmxh = compile_and_fit_model_basic( model_combination_of_features,  \n","#                     f\"Collab_{layer_name}_WideResnet28-10_A_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","#                     np_x_train_collab_wideresnets[0,:].shape, \n","#                     np_x_train_collab_wideresnets, \n","#                     train_targets,\n","#                     save_max_epoch=False,\n","#                     save_final=True,\n","#                     patience_count = 35,\n","#                     early_stopping_obs = 'val_sparse_categorical_accuracy',\n","#                     log_history = True,\n","#                     verbose_level=1,                             \n","#                     batch_size=512, \n","#                     epochs=250, \n","#                     class_weight=None, \n","#                     validation_data=(np_x_validation_collab_wideresnets, validation_targets))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KlsYre4_Qhyz"},"outputs":[],"source":["collaborativeFullyC_lastdense_data.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/wideresnet_collab_lastdense_test_results_{datetime.datetime.now():%Y%m%d%H%M%S}.csv\")\n"]},{"cell_type":"markdown","metadata":{"id":"cxBmUlk5Qhy0"},"source":["# Plot on Collaborative CNN/DNN/WideResNet last dense layers as function of number of models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YJBHqxkQQhy0"},"outputs":[],"source":["all_lastdense_collab_data = pd.concat([ pd.read_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/{f}\") for f in os.listdir(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/\") if \"collab_lastdense_test_results\" in f ])\n","all_lastdense_collab_data = all_lastdense_collab_data.drop_duplicates()\n","all_lastdense_collab_data[\"BestComponent\"] = np.nan\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"89TQ2Q1IQhy0"},"outputs":[],"source":["# read validation accur\n","val_accs = pd.read_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/val_test_accs_20211118165251.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"go025R4GQhy0"},"outputs":[],"source":["best_submodels = val_accs.loc[val_accs.groupby(\"Type\")['ValAcc'].idxmax()].copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VvmWk6U1Qhy0"},"outputs":[],"source":["for typename in best_submodels[\"Type\"]:\n","  all_lastdense_collab_data.loc[all_lastdense_collab_data.Type==typename,\"BestComponent\"] = float(best_submodels.loc[best_submodels.Type==typename, \"TestAcc\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ApUSqTHOQhy0"},"outputs":[],"source":["all_lastdense_collab_data = pd.concat([all_lastdense_collab_data.groupby([\"Type\",\"NumOfModels\"])[\"Acc\"].mean(), all_lastdense_collab_data.groupby([\"Type\",\"NumOfModels\"])[\"BestComponent\"].mean()],axis=1)\n","all_lastdense_collab_data = all_lastdense_collab_data.reset_index()\n","\n","# all_lastdense_collab_data[all_lastdense_collab_data.NumOfModels<=2]\n","# all_lastdense_collab_data = all_lastdense_collab_data[all_lastdense_collab_data.NumOfModels>2]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Pk8mYEnQhy0"},"outputs":[],"source":["ggplot(all_lastdense_collab_data) +  \\\n","  geom_line(aes(x='NumOfModels',y='Acc',group='Type',color='Type', fill='Type'),size=2) + \\\n","  geom_line(aes(x='NumOfModels',y='BestComponent',group='Type',color='Type', fill='Type'), linetype='dashed')  + \\\n","  theme_bw() + ggtitle('Test Accuracy for Collab models')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fEPMLKUxQhy0"},"outputs":[],"source":["all_lastdense_collab_data.sample(10)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JBcVku5ACyVv"},"outputs":[],"source":["all_lastdense_collab_data[\"Layer\"] = \"LASTDENSE\""]},{"cell_type":"markdown","metadata":{"id":"4Z9SnOmMTKbj"},"source":["# A GP collaborative model on the softmax layer features of input DNNX/CNNX/CNNX0.5+DNNX0.5/WideResNet28-10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-dyXfmdTb1aU"},"outputs":[],"source":["import gpflow\n","from gpflow.utilities import ops, print_summary, set_trainable\n","from gpflow.config import set_default_float, default_float, set_default_summary_fmt\n","from gpflow.ci_utils import ci_niter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SUt2vbQC62W9"},"outputs":[],"source":["import inspect\n","  \n","def retrieve_name(var):\n","    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n","    return [var_name for var_name, var_val in callers_local_vars if var_val is var]\n"," \n","def mod_retrieve_name(var):\n","    callers_local_vars = inspect.currentframe().f_back.f_back.f_locals.items()\n","    return [var_name for var_name, var_val in callers_local_vars if var_val is var]\n"," \n","def foo(bar):\n","\tprint(retrieve_name(bar))\n","\tprint(mod_retrieve_name(bar))\n","\n","# x,y,z = 1,2,3\n","# foo(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"busXdvq-TKbj"},"outputs":[],"source":["# set up the data for the collaborative models using GPs from DNN and CNN softmax\n","\n","num_of_models = 20\n","\n","layer_name_sftmx_dnn = \"SFTMX1\"\n","np_x_validation_collab_dnn, np_x_train_collab_dnn, np_x_test_collab_dnn = get_features_for_layer(dnn_features_files, layer_name_sftmx_dnn, num_of_models)\n","\n","layer_name_sftmx_cnn = \"SFTMX1\"\n","np_x_validation_collab_cnn, np_x_train_collab_cnn, np_x_test_collab_cnn = get_features_for_layer(cnn_features_files, layer_name_sftmx_cnn, num_of_models)\n","\n","# set up the data for the collaborative models using GPs from WideResNet softmax\n","\n","layer_name_sftmx_wideresnets = \"CLASSIFIER_D1\"\n","np_x_validation_collab_wideresnets, np_x_train_collab_wideresnets, np_x_test_collab_wideresnets = get_features_for_layer_using_subbatches(wideresnets_features_files, layer_name_sftmx_wideresnets, num_of_models)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YI3qO9NDwcbj"},"source":["Setup the GP-specific data structure for the DNN/CNN/WideResNet softmax features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UUDOr6jhn4qN"},"outputs":[],"source":["def setup_feature_data_for_gp(np_x_train_collab, np_x_test_collab, train_targets, test_targets):\n","  \n","  num_of_data_points = np_x_train_collab.shape[0] \n","  num_classes = np.unique(train_targets[:num_of_data_points,:]).size\n","  num_of_functions = num_classes\n","  num_of_independent_vars = np_x_train_collab.shape[-1]\n","  num_of_test_data_points = min(num_of_data_points,test_targets.shape[0])\n","\n","  data_gp_train = np_x_train_collab[:num_of_data_points,:]\n","  data_gp_test = np_x_test_collab[:num_of_test_data_points,:]\n","\n","  data_gp_train_target_hot = np.squeeze(np.eye(num_classes)[train_targets[:num_of_data_points,:]].astype(bool))\n","  data_gp_train_target = np.apply_along_axis(np.argmax, 1, data_gp_train_target_hot)\n","  data_gp_train_target = np.expand_dims(data_gp_train_target, axis=1)\n","\n","  data_gp_test_target_hot = np.squeeze(np.eye(num_classes)[test_targets[:num_of_test_data_points,:]].astype(bool))\n","  data_gp_test_target = np.apply_along_axis(np.argmax, 1, data_gp_test_target_hot)\n","  data_gp_test_target = np.expand_dims(data_gp_test_target, axis=1)\n","\n","  data_gp = ( data_gp_train, data_gp_train_target )\n","  return data_gp, data_gp_test, data_gp_test_target\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3nog8X09wcbn"},"outputs":[],"source":["# reproducibility:\n","np.random.seed(0)\n","tf.random.set_seed(123)\n","\n","data_gp_dnnf, data_gp_test_dnnf, data_gp_test_target_dnnf = setup_feature_data_for_gp(np_x_train_collab_dnn,  np_x_test_collab_dnn, train_targets, test_targets   )\n","data_gp_cnnf, data_gp_test_cnnf, data_gp_test_target_cnnf = setup_feature_data_for_gp(np_x_train_collab_cnn,  np_x_test_collab_cnn, train_targets, test_targets   )\n","data_gp_wideresnets, data_gp_test_wideresnets, data_gp_test_target_wideresnets = setup_feature_data_for_gp(np_x_train_collab_wideresnets,  np_x_test_collab_wideresnets, train_targets, test_targets   )\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I2BuEPuRS4Tt"},"outputs":[],"source":["def construct_kernel_list(num_of_independent_vars, base_lengthscales = [0.1]):\n","  possible_kernels = []\n","  for i in range(len(base_lengthscales)):\n","    possible_kernels.append(gpflow.kernels.Matern12(variance=1.0, lengthscales=[base_lengthscales[i]]*num_of_independent_vars))\n","    possible_kernels.append(gpflow.kernels.Matern32(variance=1.0, lengthscales=[base_lengthscales[i]]*num_of_independent_vars))\n","    possible_kernels.append(gpflow.kernels.RBF(variance=1.0, lengthscales=[base_lengthscales[i]]*num_of_independent_vars))\n","    # possible_kernels.append(gpflow.kernels.Matern52(variance=1.0, lengthscales=[base_lengthscales[i]]*num_of_independent_vars))\n","    # possible_kernels.append(gpflow.kernels.SquaredExponential(lengthscales=[base_lengthscales[i]]*num_of_independent_vars))  \n","\n","  return possible_kernels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g_qZ_vVF5KrG"},"outputs":[],"source":["# print(np_x_validation_collab_wideresnets.shape)\n","# print(np_x_test_collab_wideresnets.shape)\n","# print(np_x_train_collab_wideresnets.shape)\n","\n","# print(np_x_validation_collab_cnn.shape)\n","# print(np_x_test_collab_cnn.shape)\n","# print(np_x_train_collab_cnn.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6zIkvKG7qLKB"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os,sys\n","\n","\n","def run_gp_directly_on_input(data_gp, \n","                             data_gp_test,\n","                             data_gp_test_target,\n","                             id_str,\n","                             nth_inducing_ratio = 120, \n","                             var_list = [0.1, 1.0],       \n","                             early_stop_count = 100,\n","                             early_stop_check_interval = 1000,\n","                             early_stop_elbo_factor = 1.0001,\n","                             max_niters = 200000):\n","\n","  num_of_independent_vars = data_gp[0].shape[1]\n","  num_of_classes = np.unique(data_gp[1]).size\n","  num_of_functions = num_of_classes\n","  possible_kernels = construct_kernel_list(num_of_independent_vars, var_list)\n","  # Robustmax Multiclass Likelihood\n","  invlink = gpflow.likelihoods.RobustMax(num_of_functions)  # Robustmax inverse link function\n","  likelihood = gpflow.likelihoods.MultiClass(num_of_functions, invlink=invlink)  # Multiclass likelihood\n","  idxs_of_induced = sorted(random.sample(range(data_gp[0].shape[0]),int(data_gp[0].shape[0]/nth_inducing_ratio)))\n","  inducing_inputs = data_gp[0][idxs_of_induced,:].copy()  # inducing inputs\n","  gp_models = []\n","  for kernel in possible_kernels:\n","    m = gpflow.models.SVGP(\n","        kernel=kernel,\n","        likelihood=likelihood,\n","        inducing_variable=inducing_inputs,\n","        num_latent_gps=num_of_functions,\n","        whiten=True,\n","        q_diag=True,\n","    )\n","    # Only train the variational parameters\n","    # set_trainable(m.kernel.kernels[1].variance, False)\n","    set_trainable(m.inducing_variable, True)\n","    gp_models.append(m)\n","  result_dict = dict()\n","  for mcount,m in enumerate(gp_models):\n","      print(mcount)\n","      tensor_data = tuple(map(tf.convert_to_tensor, data_gp))\n","      training_loss = m.training_loss_closure(tensor_data, compile=True)\n","      starting_elbo = -training_loss().numpy()\n","      print(f'Starting ELBO {starting_elbo}')\n","      elbos = [training_loss().numpy()]\n","      optimizer = tf.optimizers.Adam()  \n","\n","      # optimizer = tf.optimizers.RMSprop()\n","      @tf.function\n","      def optim_here():\n","          optimizer.minimize(training_loss, m.trainable_variables)\n","\n","      try:\n","        for itc in range(max_niters):\n","            optim_here()\n","            elbo_now = -training_loss().numpy()\n","            elbos.append(elbo_now)\n","            if (itc % early_stop_check_interval) == 0:\n","              print(f'ELBO {elbo_now}')\n","              if len(elbos) > (early_stop_count+1):\n","                if elbos[-early_stop_count] >= elbos[-1]*early_stop_elbo_factor:\n","                    print(f'Early stopping at {itc}')\n","                    break          \n","            # needs at least a decent improvement\n","\n","        print(f'Ending ELBO {elbos[-1]}')\n","\n","        pY, pYv = m.predict_y(data_gp_test)  # Predict Y values at test locations\n","        result_dict[f\"{gp_models[0].kernel.name}_{mcount}_{id_str}\"] = dict({'model': m, 'X': data_gp_test, 'Y': data_gp_test_target, 'pY': pY, 'pYv': pYv})\n","        np.save(f\"/content/drive/MyDrive/data_papers/{paper_name}/gp_collab/feature_collabUsingGP_{id_str}_Test_{gp_models[mcount].kernel.name}_{mcount}_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","            np.array(pY), \n","            allow_pickle=True, \n","            fix_imports=True)\n","        np.save(f\"/content/drive/MyDrive/data_papers/{paper_name}/gp_collab/feature_collabUsingGP_{id_str}_Test_Targets_{gp_models[mcount].kernel.name}_{mcount}_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","            np.array(data_gp[1]), \n","            allow_pickle=True, \n","            fix_imports=True)\n","        tf.saved_model.save(m, f\"/content/drive/MyDrive/data_papers/{paper_name}/gp_collab/feature_collabUsingGP_{id_str}_model_{gp_models[mcount].kernel.name}_{mcount}_{datetime.datetime.now():%Y%m%d%H%M%S}\")\n","        print_summary(m, fmt=\"notebook\")  \n","      except:\n","        print(f'ERROR, Ending ELBO {elbos[-1]} (NOT saved)')\n","        next\n","\n","  return result_dict\n"]},{"cell_type":"markdown","metadata":{"id":"7en8tCffS4Tt"},"source":["Do a Collobarative for the softmax features of DNN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eMoheCCYyyD5"},"outputs":[],"source":["layer_name_sftmx_dnn = \"SFTMX1\"\n","gp_dnn_sftmx_dict = run_gp_directly_on_input(data_gp_dnnf, \n","                             data_gp_test_dnnf,\n","                             data_gp_test_target_dnnf,\n","                             f\"DNN_{layer_name_sftmx_dnn}_ID{str(uuid.uuid4()).split('-')[0]}\",\n","                             nth_inducing_ratio = 120, \n","                             var_list = [0.1, 1.0],       \n","                             early_stop_count = 100,\n","                             early_stop_check_interval = 1000,\n","                             early_stop_elbo_factor = 1.0001,\n","                             max_niters = 200000)\n"]},{"cell_type":"markdown","metadata":{"id":"95F5SdFc3l2v"},"source":["Do a Collobarative for the softmax features of CNN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pPFjYugx3l2w"},"outputs":[],"source":["layer_name_sftmx_cnn = \"SFTMX1\"\n","gp_cnn_sftmx_dict = run_gp_directly_on_input(data_gp_cnnf, \n","                             data_gp_test_cnnf,\n","                             data_gp_test_target_cnnf,\n","                             f\"CNN_{layer_name_sftmx_cnn}_ID{str(uuid.uuid4()).split('-')[0]}\",\n","                             nth_inducing_ratio = 120, \n","                             var_list = [0.1, 1.0],       \n","                             early_stop_count = 100,\n","                             early_stop_check_interval = 1000,\n","                             early_stop_elbo_factor = 1.0001,\n","                             max_niters = 200000)\n"]},{"cell_type":"markdown","metadata":{"id":"V4nn00xj33dI"},"source":["Do a Collobarative for the softmax features of WideResNet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0sPjC97G33dI"},"outputs":[],"source":["layer_name_sftmx_wideresnets = \"CLASSIFIER_D1\"\n","gp_wideresnets_sftmx_dict = run_gp_directly_on_input(data_gp_wideresnets, \n","                             data_gp_test_wideresnets,\n","                             data_gp_test_target_wideresnets,\n","                             f\"WideResNet28-10_{layer_name_sftmx_wideresnets}_ID{str(uuid.uuid4()).split('-')[0]}\",\n","                             nth_inducing_ratio = 120, \n","                             var_list = [0.1, 1.0],       \n","                             early_stop_count = 100,\n","                             early_stop_check_interval = 1000,\n","                             early_stop_elbo_factor = 1.0001,\n","                             max_niters = 200000)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y3rtsniIwbwP"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"yHXkywirRpmM"},"source":["# A collaborative GP model on the penultimate layer features of input DNNX/CNNX/CNNX0.5+DNNX0.5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tdG2Foo36ck8"},"outputs":[],"source":["# set up the data for the collaborative models using GPs\n","\n","num_of_models = 20\n","\n","layer_name_penultimate_dnn = \"D3R\"\n","layer_name_penultimate_cnn = \"DRP1\"\n","\n","validation_features_to_load_dnn = sorted(random.sample([ff for ff in dnn_features_files if \"Validation_X.npy\" in ff and f\"_{layer_name_penultimate_dnn}_\" in ff ], num_of_models))\n","np_x_validation_collab_dnn = np.array([np.load(ff) for ff in validation_features_to_load_dnn ])\n","np_x_validation_collab_dnn = np.concatenate(np_x_validation_collab_dnn, axis=1)\n","\n","train_features_to_load_dnn = [ff.replace(\"Validation\", \"Train\") for ff in validation_features_to_load_dnn]\n","np_x_train_collab_dnn = np.array([np.load(ff) for ff in train_features_to_load_dnn ])\n","np_x_train_collab_dnn = np.concatenate(np_x_train_collab_dnn, axis=1)\n","\n","test_features_to_load_dnn = [ff.replace(\"Validation\", \"Test\") for ff in validation_features_to_load_dnn]\n","np_x_test_collab_dnn = np.array([np.load(ff) for ff in test_features_to_load_dnn ])\n","np_x_test_collab_dnn = np.concatenate(np_x_test_collab_dnn, axis=1)\n","\n","validation_features_to_load_cnn = sorted(random.sample([ff for ff in cnn_features_files if \"Validation_X.npy\" in ff and f\"_{layer_name_penultimate_cnn}_\" in ff], num_of_models))\n","np_x_validation_collab_cnn = np.array([np.load(ff) for ff in validation_features_to_load_cnn ])\n","np_x_validation_collab_cnn = np.concatenate(np_x_validation_collab_cnn, axis=1)\n","\n","train_features_to_load_cnn = [ff.replace(\"Validation\", \"Train\") for ff in validation_features_to_load_cnn]\n","np_x_train_collab_cnn = np.array([np.load(ff) for ff in train_features_to_load_cnn ])\n","np_x_train_collab_cnn = np.concatenate(np_x_train_collab_cnn, axis=1)\n","\n","test_features_to_load_cnn = [ff.replace(\"Validation\", \"Test\") for ff in validation_features_to_load_cnn]\n","np_x_test_collab_cnn = np.array([np.load(ff) for ff in test_features_to_load_cnn ])\n","np_x_test_collab_cnn = np.concatenate(np_x_test_collab_cnn, axis=1)\n","\n","validation_features_to_load_cnn = sorted(random.sample([ff for ff in cnn_features_files if \"Validation_X.npy\" in ff and f\"_{layer_name_penultimate_cnn}_\" in ff], int(num_of_models/2)))\n","validation_features_to_load_dnn  = sorted(random.sample([ff for ff in dnn_features_files if \"Validation_X.npy\" in ff and f\"_{layer_name_penultimate_dnn}_\" in ff], int(num_of_models/2)))\n","\n","validation_features_cnn = np.array([np.load(ff) for ff in validation_features_to_load_cnn ])\n","validation_features_dnn = np.array([np.load(ff) for ff in validation_features_to_load_dnn ])\n","np_x_validation_collab_cdnn = np.concatenate([validation_features_cnn,validation_features_dnn], axis=2)\n","\n","train_features_to_load_cnn = [ff.replace(\"Validation\", \"Train\") for ff in validation_features_to_load_cnn]\n","train_features_to_load_dnn = [ff.replace(\"Validation\", \"Train\") for ff in validation_features_to_load_dnn]\n","train_features_cnn = np.array([np.load(ff) for ff in train_features_to_load_cnn ])\n","train_features_dnn = np.array([np.load(ff) for ff in train_features_to_load_dnn ])\n","np_x_train_collab_cdnn = np.concatenate([train_features_cnn,train_features_dnn], axis=2)\n","\n","test_features_to_load_cnn = [ff.replace(\"Validation\", \"Test\") for ff in validation_features_to_load_cnn]\n","test_features_to_load_dnn = [ff.replace(\"Validation\", \"Test\") for ff in validation_features_to_load_dnn]\n","test_features_cnn = np.array([np.load(ff) for ff in test_features_to_load_cnn ])\n","test_features_dnn = np.array([np.load(ff) for ff in test_features_to_load_dnn ])\n","np_x_test_collab_cdnn = np.concatenate([test_features_cnn,test_features_dnn], axis=2)\n","\n","np_x_train_collab_cdnn = np.swapaxes(np_x_train_collab_cdnn, 0,1)\n","np_x_validation_collab_cdnn = np.swapaxes(np_x_validation_collab_cdnn, 0,1)\n","np_x_test_collab_cdnn = np.swapaxes(np_x_test_collab_cdnn, 0,1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fNEi9qQeUTf1"},"outputs":[],"source":["# np_x_train_collab_dnn\n","# data_gp_train_dnnf\n","# test_targets.shape\n","# data_gp_test_target_dnnf.shape\n","# data_gp_test_dnnf.shape\n","# data_gp_train_dnnf.shape"]},{"cell_type":"markdown","metadata":{"id":"A2BIu6tAocat"},"source":["Setup the GP data for the DNN features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tupBZqUJXG7L"},"outputs":[],"source":["# reproducibility:\n","np.random.seed(0)\n","tf.random.set_seed(123)\n","\n","num_of_data_points_dnnf = np_x_train_collab_dnn.shape[0] # 2000\n","num_classes = np.unique(train_targets[:num_of_data_points_dnnf,:]).size\n","num_of_functions = num_classes\n","num_of_independent_vars_dnn = np_x_train_collab_dnn.shape[-1]\n","num_of_test_data_points_dnnf = min(num_of_data_points_dnnf,test_targets.shape[0])\n","\n","data_gp_train_dnnf = np_x_train_collab_dnn[:num_of_data_points_dnnf,:]     # np_x_train_collab_dnn # np.concatenate((data_train[0],data_train_post[0]), axis = 1)\n","data_gp_test_dnnf = np_x_test_collab_dnn[:num_of_test_data_points_dnnf,:]     # np_x_train_collab_dnn # np.concatenate((data_train[0],data_train_post[0]), axis = 1)\n","\n","data_gp_train_target_hot_dnnf = np.squeeze(np.eye(num_classes)[train_targets[:num_of_data_points_dnnf,:]].astype(bool))\n","data_gp_train_target_dnnf = np.apply_along_axis(np.argmax, 1, data_gp_train_target_hot_dnnf)\n","data_gp_train_target_dnnf = np.expand_dims(data_gp_train_target_dnnf, axis=1)\n","\n","data_gp_test_target_hot_dnnf = np.squeeze(np.eye(num_classes)[test_targets[:num_of_test_data_points_dnnf,:]].astype(bool))\n","data_gp_test_target_dnnf = np.apply_along_axis(np.argmax, 1, data_gp_test_target_hot_dnnf)\n","data_gp_test_target_dnnf = np.expand_dims(data_gp_test_target_dnnf, axis=1)\n","\n","data_gp_dnnf = ( data_gp_train_dnnf, data_gp_train_target_dnnf )\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Wp4y41yYbv1B"},"source":["Setup the GP data for the CNN features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YRrCIHWlbv1B"},"outputs":[],"source":["# reproducibility:\n","np.random.seed(0)\n","tf.random.set_seed(123)\n","\n","num_of_data_points_cnnf = np_x_train_collab_cnn.shape[0] # 2000\n","num_classes = np.unique(train_targets[:num_of_data_points_cnnf,:]).size\n","num_of_functions = num_classes\n","num_of_independent_vars_cnn = np_x_train_collab_cnn.shape[-1]\n","num_of_test_data_points_cnnf = min(num_of_data_points_cnnf,test_targets.shape[0])\n","\n","data_gp_train_cnnf = np_x_train_collab_cnn[:num_of_data_points_cnnf,:]\n","data_gp_test_cnnf = np_x_test_collab_cnn[:num_of_test_data_points_cnnf,:]\n","\n","data_gp_train_target_hot_cnnf = np.squeeze(np.eye(num_classes)[train_targets[:num_of_data_points_cnnf,:]].astype(bool))\n","data_gp_train_target_cnnf = np.apply_along_axis(np.argmax, 1, data_gp_train_target_hot_cnnf)\n","data_gp_train_target_cnnf = np.expand_dims(data_gp_train_target_cnnf, axis=1)\n","\n","data_gp_test_target_hot_cnnf = np.squeeze(np.eye(num_classes)[test_targets[:num_of_test_data_points_cnnf,:]].astype(bool))\n","data_gp_test_target_cnnf = np.apply_along_axis(np.argmax, 1, data_gp_test_target_hot_cnnf)\n","data_gp_test_target_cnnf = np.expand_dims(data_gp_test_target_cnnf, axis=1)\n","\n","data_gp_cnnf = ( data_gp_train_cnnf, data_gp_train_target_cnnf )\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cFnc2Nq7ug2n"},"outputs":[],"source":["# save this down for local analysis\n","npy_to_save = [data_gp_train_dnnf, \n","               data_gp_train_target_dnnf,\n","               data_gp_test_dnnf,\n","               data_gp_test_target_dnnf,\n","               data_gp_train_cnnf, \n","               data_gp_train_target_cnnf,\n","               data_gp_test_cnnf,\n","               data_gp_test_target_cnnf]\n","\n","for npx in npy_to_save:\n","  print([x for x in retrieve_name(npx) if 'data' in x][0])\n","  # np.save(f\"/content/drive/MyDrive/data_papers/{paper_name}/features_for_local/{[x for x in retrieve_name(npx) if 'data' in x][0]}_20models_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","  #         npx, \n","  #         allow_pickle=True, \n","  #         fix_imports=True)\n","  # np.save(f\"/content/drive/MyDrive/data_papers/{paper_name}/features_for_local/{[x for x in retrieve_name(npx) if 'data' in x][0]}_4models_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","  #         npx, \n","  #         allow_pickle=True, \n","  #         fix_imports=True)\n","  # np.save(f\"/content/drive/MyDrive/data_papers/{paper_name}/features_for_local/{[x for x in retrieve_name(npx) if 'data' in x][0]}_2models_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","  #         npx, \n","  #         allow_pickle=True, \n","  #         fix_imports=True)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z_kpE1wX--Vf"},"outputs":[],"source":["# np_x_train_collab_dnn.shape\n","# np_x_train_collab_dnn[:1000,:].shape\n","# train_targets[:1000]\n","# train_targets.shape\n","# data_gp_train_target_hot\n","# np.apply_along_axis(np.argmax, 1, data_gp_train_target_hot)\n","# data_gp_train_target\n","# num_of_independent_vars\n","# np_x_train_collab_dnn.shape\n","# possible_kernels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wG768Pf7errr"},"outputs":[],"source":["# cnn_features_files"]},{"cell_type":"markdown","metadata":{"id":"nJnGnexkeJim"},"source":["Get the explained variance graph on the DNN features PCA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LxYELqWFB0AD"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","import tensorflow as tf\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vlXxrzjzC8CM"},"outputs":[],"source":["sc_dnnf = StandardScaler()\n","sc_dnnf.fit(data_gp_train_dnnf)\n","data_gp_train_dnnf_pca = sc_dnnf.transform(data_gp_train_dnnf)\n","data_gp_test_dnnf_pca = sc_dnnf.transform(data_gp_test_dnnf)\n","\n","pca_dnnf = PCA()\n","data_gp_train_dnnf_pca = pca_dnnf.fit_transform(data_gp_train_dnnf_pca)\n","data_gp_test_dnnf_pca = pca_dnnf.transform(data_gp_test_dnnf_pca)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5g4iRu2KDeRr"},"outputs":[],"source":["exp_var_pca_dnnf = pca_dnnf.explained_variance_ratio_\n","cum_sum_eigenvalues_dnnf = np.cumsum(exp_var_pca_dnnf)\n","\n","plt.bar(range(0,len(exp_var_pca_dnnf)), exp_var_pca_dnnf, alpha=0.5, align='center', label='Individual explained variance')\n","plt.step(range(0,len(cum_sum_eigenvalues_dnnf)), cum_sum_eigenvalues_dnnf, where='mid',label='Cumulative explained variance')\n","plt.ylabel('Explained variance ratio _dnnf')\n","plt.xlabel('Principal component index _dnnf')\n","plt.legend(loc='best')\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"rH6ZnzMyeTga"},"source":["Get the explained variance graph on the CNN features PCA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z5394cPreZGS"},"outputs":[],"source":["sc_cnnf = StandardScaler()\n","sc_cnnf.fit(data_gp_train_cnnf)\n","data_gp_train_cnnf_pca = sc_cnnf.transform(data_gp_train_cnnf)\n","data_gp_test_cnnf_pca = sc_cnnf.transform(data_gp_test_cnnf)\n","\n","pca_cnnf = PCA()\n","data_gp_train_cnnf_pca = pca_cnnf.fit_transform(data_gp_train_cnnf_pca)\n","data_gp_test_cnnf_pca = pca_cnnf.transform(data_gp_test_cnnf_pca)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"awvaE0OceZGT"},"outputs":[],"source":["exp_var_pca_cnnf = pca_cnnf.explained_variance_ratio_\n","cum_sum_eigenvalues_cnnf = np.cumsum(exp_var_pca_cnnf)\n","\n","plt.bar(range(0,len(exp_var_pca_cnnf)), exp_var_pca_cnnf, alpha=0.5, align='center', label='Individual explained variance')\n","plt.step(range(0,len(cum_sum_eigenvalues_cnnf)), cum_sum_eigenvalues_cnnf, where='mid',label='Cumulative explained variance')\n","plt.ylabel('Explained variance ratio _cnnf')\n","plt.xlabel('Principal component index _cnnf')\n","plt.legend(loc='best')\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"iEtkDK7OY4k2"},"source":["Do a Collobarative on the X first PCAs for the DNN features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8niTwje9e7Hh"},"outputs":[],"source":["# trying to run it at different var% explained through PCA\n","vars_to_explain = [0.95]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uGLZOuaHpMvj"},"outputs":[],"source":["def construct_kernel_list(num_of_independent_vars, base_lengthscales = [0.1]):\n","  possible_kernels = []\n","  for i in range(len(base_lengthscales)):\n","    possible_kernels.append(gpflow.kernels.RBF(variance=1.0, lengthscales=[base_lengthscales[i]]*num_of_independent_vars))\n","    possible_kernels.append(gpflow.kernels.Matern12(variance=1.0, lengthscales=[base_lengthscales[i]]*num_of_independent_vars))\n","    possible_kernels.append(gpflow.kernels.Matern32(variance=1.0, lengthscales=[base_lengthscales[i]]*num_of_independent_vars))\n","    # possible_kernels.append(gpflow.kernels.Matern52(variance=1.0, lengthscales=[base_lengthscales[i]]*num_of_independent_vars))\n","    # possible_kernels.append(gpflow.kernels.SquaredExponential(lengthscales=[base_lengthscales[i]]*num_of_independent_vars))  \n","\n","  return possible_kernels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Yl5ILQWDhHh"},"outputs":[],"source":["\n","# nth_inducing_ratio = 80\n","nth_inducing_ratio = 120\n","\n","for min_var_explained in vars_to_explain:\n","  num_of_loadings = np.min(np.where(cum_sum_eigenvalues_dnnf>min_var_explained))\n","  data_gp_train_dnnf_pca = data_gp_train_dnnf_pca[:,:(num_of_loadings)]\n","  data_gp_test_dnnf_pca = data_gp_test_dnnf_pca[:,:(num_of_loadings)]\n","  data_gp_dnnf = ( data_gp_train_dnnf_pca, data_gp_train_target_dnnf )\n","  num_of_independent_vars = data_gp_dnnf[0].shape[1]\n","  num_of_classes = np.unique(data_gp_dnnf[1]).size\n","  num_of_functions = num_of_classes\n","  possible_kernels_dnnf = construct_kernel_list(num_of_independent_vars, [0.1, 1.0])\n","  # Robustmax Multiclass Likelihood\n","  invlink = gpflow.likelihoods.RobustMax(num_of_functions)  # Robustmax inverse link function\n","  likelihood = gpflow.likelihoods.MultiClass(num_of_functions, invlink=invlink)  # Multiclass likelihood\n","  idxs_of_induced_dnnf = sorted(random.sample(range(data_gp_dnnf[0].shape[0]),int(data_gp_dnnf[0].shape[0]/nth_inducing_ratio)))\n","  inducing_inputs_dnnf = data_gp_dnnf[0][idxs_of_induced_dnnf,:].copy()  # inducing inputs\n","  gp_models_dnnf = []\n","  for kernel in possible_kernels_dnnf:\n","    m = gpflow.models.SVGP(\n","        kernel=kernel,\n","        likelihood=likelihood,\n","        inducing_variable=inducing_inputs_dnnf,\n","        num_latent_gps=num_of_functions,\n","        whiten=True,\n","        q_diag=True,\n","    )\n","    # Only train the variational parameters\n","    # set_trainable(m.kernel.kernels[1].variance, False)\n","    set_trainable(m.inducing_variable, False)\n","    gp_models_dnnf.append(m)\n","  result_dict_dnnf = dict()\n","  # tensor_data = tuple(map(tf.convert_to_tensor, data_gp_dnnf))\n","  # training_loss = m.training_loss_closure(tensor_data, compile=True)\n","  # starting_elbo = -training_loss().numpy()\n","  # print(f'Starting ELBO {starting_elbo}')\n","  for mcount,m in enumerate(gp_models_dnnf):\n","      print(mcount)\n","      # opt = gpflow.optimizers.Scipy()\n","      # opt_logs = opt.minimize(\n","      #     m.training_loss_closure(data_gp_dnnf), m.trainable_variables, options=dict(maxiter=ci_niter(500))\n","      # )\n","      tensor_data = tuple(map(tf.convert_to_tensor, data_gp_dnnf))\n","      training_loss = m.training_loss_closure(tensor_data, compile=True)\n","      starting_elbo = -training_loss().numpy()\n","      print(f'Starting ELBO {starting_elbo}')\n","\n","      elbos = [training_loss().numpy()]\n","      optimizer = tf.optimizers.Adam()  \n","      # optimizer = tf.optimizers.RMSprop()\n","\n","      @tf.function\n","      def optim_here():\n","          optimizer.minimize(training_loss, m.trainable_variables)\n","\n","      early_stop_count = 100\n","      max_niters = 200000\n","      for itc in range(max_niters):\n","          optim_here()\n","          elbo_now = -training_loss().numpy()\n","          elbos.append(elbo_now)\n","          # if len(elbos) > 100:\n","          if (itc % 1000) == 0:\n","            print(f'ELBO {elbo_now}')\n","            if len(elbos) > (early_stop_count+1):\n","              if elbos[-early_stop_count] >= elbos[-1]*1.0001:\n","                  print(f'Early stopping at {itc}')\n","                  break          \n","          # needs at least a decent improvement\n","\n","      print(f'Ending ELBO {elbos[-1]}')\n","\n","      pY, pYv = m.predict_y(data_gp_test_dnnf_pca)  # Predict Y values at test locations\n","      result_dict_dnnf[f\"{gp_models_dnnf[0].kernel.name}_{mcount}_{str(int(min_var_explained*100))}\"] = dict({'model': m, 'X': data_gp_test_dnnf_pca, 'Y': data_gp_test_target_dnnf, 'pY': pY, 'pYv': pYv})\n","      np.save(f\"/content/drive/MyDrive/data_papers/{paper_name}/gp_collab/feature_collabUsingGP_dnn20_TestPCA_{gp_models_dnnf[mcount].kernel.name}_{mcount}_{str(int(min_var_explained*1000))}_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","          np.array(pY), \n","          allow_pickle=True, \n","          fix_imports=True)\n","      np.save(f\"/content/drive/MyDrive/data_papers/{paper_name}/gp_collab/feature_collabUsingGP_dnn20_TestPCA_Targets_{gp_models_dnnf[mcount].kernel.name}_{mcount}_{str(int(min_var_explained*1000))}_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","          np.array(data_gp_dnnf[1]), \n","          allow_pickle=True, \n","          fix_imports=True)\n","      tf.saved_model.save(m, f\"/content/drive/MyDrive/data_papers/{paper_name}/gp_collab/feature_collabUsingGP_dnn20_model_{gp_models_dnnf[mcount].kernel.name}_{mcount}_{str(int(min_var_explained*1000))}_{datetime.datetime.now():%Y%m%d%H%M%S}\")\n","      print_summary(m, fmt=\"notebook\")  \n"]},{"cell_type":"markdown","metadata":{"id":"A4hFzFmgUjSq"},"source":["Do a Collobarative on the X first PCAs for the CNN\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h9hhtojWUjSs"},"outputs":[],"source":["# trying to run it at different var% explained through PCA\n","vars_to_explain = [0.95]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lk7BkNymel7P"},"outputs":[],"source":["# len(cum_sum_eigenvalues_cnnf)\n","# min_var_explained\n","# cum_sum_eigenvalues_cnnf\n","# num_of_loadings\n","# data_gp_cnnf[0].shape\n","# # 73257/120"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v6U4WHJxUvjW"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os,sys\n","\n","# min_var_explained = 0.4\n","\n","# nth_inducing_ratio = 80\n","nth_inducing_ratio = 120\n","\n","for min_var_explained in vars_to_explain:\n","  num_of_loadings = np.min(np.where(cum_sum_eigenvalues_cnnf>min_var_explained))\n","  data_gp_train_cnnf_pca = data_gp_train_cnnf_pca[:,:(num_of_loadings)]\n","  data_gp_test_cnnf_pca = data_gp_test_cnnf_pca[:,:(num_of_loadings)]\n","  data_gp_cnnf = ( data_gp_train_cnnf_pca, data_gp_train_target_cnnf )\n","  num_of_independent_vars = data_gp_cnnf[0].shape[1]\n","  num_of_classes = np.unique(data_gp_cnnf[1]).size\n","  num_of_functions = num_of_classes\n","  possible_kernels_cnnf = construct_kernel_list(num_of_independent_vars, [0.1, 1.0])\n","  # Robustmax Multiclass Likelihood\n","  invlink = gpflow.likelihoods.RobustMax(num_of_functions)  # Robustmax inverse link function\n","  likelihood = gpflow.likelihoods.MultiClass(num_of_functions, invlink=invlink)  # Multiclass likelihood\n","  idxs_of_induced_cnnf = sorted(random.sample(range(data_gp_cnnf[0].shape[0]),int(data_gp_cnnf[0].shape[0]/nth_inducing_ratio)))\n","  inducing_inputs_cnnf = data_gp_cnnf[0][idxs_of_induced_cnnf,:].copy()  # inducing inputs\n","  gp_models_cnnf = []\n","  for kernel in possible_kernels_cnnf:\n","    m = gpflow.models.SVGP(\n","        kernel=kernel,\n","        likelihood=likelihood,\n","        inducing_variable=inducing_inputs_cnnf,\n","        num_latent_gps=num_of_functions,\n","        whiten=True,\n","        q_diag=True,\n","    )\n","    # Only train the variational parameters\n","    # set_trainable(m.kernel.kernels[1].variance, False)\n","    set_trainable(m.inducing_variable, False)\n","    gp_models_cnnf.append(m)\n","  result_dict_cnnf = dict()\n","  # tensor_data = tuple(map(tf.convert_to_tensor, data_gp_cnnf))\n","  # training_loss = m.training_loss_closure(tensor_data, compile=True)\n","  # starting_elbo = -training_loss().numpy()\n","  # print(f'Starting ELBO {starting_elbo}')\n","  for mcount,m in enumerate(gp_models_cnnf):\n","      print(mcount)\n","      # opt = gpflow.optimizers.Scipy()\n","      # opt_logs = opt.minimize(\n","      #     m.training_loss_closure(data_gp_cnnf), m.trainable_variables, options=dict(maxiter=ci_niter(500))\n","      # )\n","      tensor_data = tuple(map(tf.convert_to_tensor, data_gp_cnnf))\n","      training_loss = m.training_loss_closure(tensor_data, compile=True)\n","      starting_elbo = -training_loss().numpy()\n","      print(f'Starting ELBO {starting_elbo}')\n","\n","      elbos = [training_loss().numpy()]\n","      optimizer = tf.optimizers.Adam()  \n","      # optimizer = tf.optimizers.RMSprop()\n","\n","      @tf.function\n","      def optim_here():\n","          optimizer.minimize(training_loss, m.trainable_variables)\n","\n","      early_stop_count = 100\n","      max_niters = 200000\n","      for itc in range(max_niters):\n","          optim_here()\n","          elbo_now = -training_loss().numpy()\n","          elbos.append(elbo_now)\n","          # if len(elbos) > 100:\n","          if (itc % 1000) == 0:\n","            print(f'ELBO {elbo_now}')\n","            if len(elbos) > (early_stop_count+1):\n","              if elbos[-early_stop_count] >= elbos[-1]*1.0001:\n","                  print(f'Early stopping at {itc}')\n","                  break          \n","          # needs at least a decent improvement\n","\n","      print(f'Ending ELBO {elbos[-1]}')\n","\n","      pY, pYv = m.predict_y(data_gp_test_cnnf_pca)  # Predict Y values at test locations\n","      result_dict_cnnf[f\"{gp_models_cnnf[0].kernel.name}_{mcount}_{str(int(min_var_explained*100))}\"] = dict({'model': m, 'X': data_gp_test_cnnf_pca, 'Y': data_gp_test_target_cnnf, 'pY': pY, 'pYv': pYv})\n","      np.save(f\"/content/drive/MyDrive/data_papers/{paper_name}/gp_collab/feature_collabUsingGP_cnn20_TestPCA_{gp_models_cnnf[mcount].kernel.name}_{mcount}_{str(int(min_var_explained*1000))}_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","          np.array(pY), \n","          allow_pickle=True, \n","          fix_imports=True)\n","      np.save(f\"/content/drive/MyDrive/data_papers/{paper_name}/gp_collab/feature_collabUsingGP_cnn20_TestPCA_Targets_{gp_models_cnnf[mcount].kernel.name}_{mcount}_{str(int(min_var_explained*1000))}_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","          np.array(data_gp_cnnf[1]), \n","          allow_pickle=True, \n","          fix_imports=True)\n","      tf.saved_model.save(m, f\"/content/drive/MyDrive/data_papers/{paper_name}/gp_collab/feature_collabUsingGP_cnn20_model_{gp_models_cnnf[mcount].kernel.name}_{mcount}_{str(int(min_var_explained*1000))}_{datetime.datetime.now():%Y%m%d%H%M%S}\")\n","      print_summary(m, fmt=\"notebook\")  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xATuNcTfxZ5R"},"outputs":[],"source":["# gp_models_dnnf[0].kernel.name"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kKzFaZdmn5WS"},"outputs":[],"source":["# gp_models[0].kernel\n","# gp_models[1].kernel\n","\n","# data_gp_test_dnnf.shape\n","# data_gp_test_dnnf_pca.shape\n","# data_gp_test_target_dnnf.shape\n","\n","# pY, pYv = m.predict_y(data_gp_test_dnnf_pca)  # Predict Y values at test locations\n","# result_dict[f\"{gp_models_dnnf[0].kernel.name}_{mcount}\"] = dict({'model': m, 'X': data_gp_test_dnnf_pca, 'Y': data_gp_test_target_dnnf, 'pY': pY, 'pYv': pYv})\n","# print_summary(m, fmt=\"notebook\")\n","# print(pY.shape)\n","# np.apply_along_axis(np.argmax, 1, pY) "]},{"cell_type":"markdown","metadata":{"id":"DqVhyWuv26HD"},"source":["Set up a summary of all the GP collaborative"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"63Tz4R2t2zqj"},"outputs":[],"source":["import datetime\n","\n","# it (\"Targets\") is reverted!!!\n","predicted_y_files = [ i for i in os.listdir(f\"/content/drive/MyDrive/data_papers/{paper_name}/gp_collab/\") if os.path.isfile(f\"/content/drive/MyDrive/data_papers/{paper_name}/gp_collab/{i}\") and not \"_Targets_\" in i ]  \n","# used_x_files = [ i for i in os.listdir(\"D:/papers/gp_collab/gp_collab\") if os.path.isfile(i) and \"_Targets_\" in i ]\n","\n","collab_summaries = []\n","used_y_files = []\n","\n","# check that it picks up the full directory\n","for predicted_y_file in predicted_y_files:\n","    y_test_predicted = np.load(f\"/content/drive/MyDrive/data_papers/{paper_name}/gp_collab/{predicted_y_file}\")\n","    y_test_predicted = np.apply_along_axis(np.argmax, 1, y_test_predicted)\n","    y_test_predicted = np.expand_dims(y_test_predicted, axis=1)\n","    try:\n","        summary_here =  pr_rc_f1_acc_from_supplied(y_test_predicted, test_targets)\n","        used_y_files.append(predicted_y_file)\n","        collab_summaries.append(summary_here)\n","    except ValueError:\n","        print(f\"FAILED {predicted_y_file}\")            \n","        pass\n","\n","\n","collab_summaries = np.array(collab_summaries)\n","\n","gp_collab_summary = pd.DataFrame(collab_summaries, columns=['PR','RC','F1','ACC'])\n","gp_collab_summary['file'] = used_y_files\n","\n","gp_collab_summary.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/gp_collab_summary_with_file_{datetime.datetime.now():%Y%m%d%H%M%S}.csv\",index=False)\n","np.save(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/gp_collab_summary_with_file_{datetime.datetime.now():%Y%m%d%H%M%S}.npy\",collab_summaries)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jh3gG7xCrTxM"},"source":["# Summary analysis/display across all runs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OCIWfW-E_MKa"},"outputs":[],"source":["import os,sys\n","import numpy as np\n","import pandas as pd\n","\n","# os.chdir(\"D:/papers/gp_collab/summary_results\")\n","# os.listdir()\n","\n","# pr_rc_f1_acc_from_supplied\n","# feature_collabs = np.load(\"feature_collabUsingDnn_dnn20_cnn20_cnn10dnn10_wideresnet2810_summary_20211021233740.npy\")\n","# feature_collabs = np.load(\"feature_collabUsingDnn_dnn20_cnn20_cnn10dnn10_wideresnet2810_wrs5cnn4dnn1_wrs10cnn8dnn2_summary_20211027174827.npy\")\n","\n","feature_collabs = np.load(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/feature_collabUsingDnn_dnn20_cnn20_cnn10dnn10_wideresnet2810_wrs5cnn4dnn1_wrs10cnn8dnn2_wrs5cnn5_summary_20211028180009.npy\")\n","full_parrallel = np.load(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/full_parallel_dnn20_cnn20_cnn10dnn10_summary_20211003130317.npy\")\n","\n","tensemble_dnn20 = np.load(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/traditional_ensemble_dnn20_summary_20211117002646.npy\")\n","tensemble_cnn20 = np.load(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/traditional_ensemble_cnn20_summary_20211117002646.npy\")\n","tensemble_cnn10dnn10 = np.load(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/traditional_ensemble_cnn10dnn10_summary_20211117002646.npy\")\n","tensemble_resnets = np.load(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/traditional_ensemble_resnets_summary_20211117022755.npy\")\n","tensemble_wideresnets = np.load(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/traditional_ensemble_wideresnet28-10_summary_20211117022755.npy\")\n","\n","individual_dnns = np.load(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/individual_dnn_summary_20211014152626.npy\")\n","individual_cnns = np.load(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/individual_cnn_summary_20211014152108.npy\")\n","individual_resnets = np.load(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/individual_resnets_summary_20211014154755.npy\")\n","individual_wideresnets = np.load(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/individual_WideResNet2810_summary_20211017233136.npy\")\n","\n","\n","\n","gpresults = pd.read_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/gp_collab_summary_with_file_20211027112445.csv\")\n","\n","print(np.mean(tensemble_dnn20,0))\n","print(np.mean(tensemble_cnn20,0))\n","print(np.mean(tensemble_cnn10dnn10,0))\n","# individual_dnns[np.where(individual_dnns[:,0]>0.10)[0],:]\n","print(np.mean(individual_dnns,0))\n","print(np.mean(individual_dnns[np.where(individual_dnns[:,0]>0.10)[0],:],0))\n","print(np.where(individual_dnns[:,0]>0.10)[0].size)\n","print(len(individual_cnns))\n","print(np.mean(individual_cnns,0))\n","print(np.mean(individual_cnns[np.where(individual_cnns[:,0]>0.10)[0],:],0))\n","print(np.where(individual_cnns[:,0]>0.10)[0].size)\n","print(len(individual_resnets))\n","print(np.mean(individual_resnets,0))\n","print(len(individual_wideresnets))\n","print(np.mean(individual_wideresnets,0))\n","print(len(tensemble_wideresnets))\n","print(np.mean(tensemble_wideresnets,0))\n","print(len(tensemble_resnets))\n","print(np.mean(tensemble_resnets,0))\n","\n","print(feature_collabs)\n","\n","# import platform\n","# print(platform.python_implementation())\n","\n","# os.listdir()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8EjlpZ2ia5SN"},"outputs":[],"source":["# # [ i for i in os.listdir(f\"/content/drive/MyDrive/data_papers/{paper_name}/gp_collab/\") if not \"_Targets_\" in i and os.path.isfile(f\"/content/drive/MyDrive/data_papers/{paper_name}/gp_collab/{i}\")  ]\n","# collab_summaries\n","# # used_y_files\n","# np.save(collab_summaries,f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/gp_collab_summary_with_file_{datetime.datetime.now():%Y%m%d%H%M%S}.npy\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0dox3kuua3ff"},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oo_ykvqAencY"},"outputs":[],"source":["# (Y_test_GP_mean_dnnf, Y_test_GP_variance_dnnf) = gp_models[0].predict_y(data_gp_test_dnnf)\n","# Y_test_GP_mean_dnnf = Y_test_GP_mean_dnnf.numpy()\n","# Y_test_pred_dnnf = []\n","# for i in range(data_gp_test_dnnf.shape[0]):\n","#   Y_test_pred_dnnf.append(np.argmax(Y_test_GP_mean_dnnf[i,]))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qFnU7LqBrmBU"},"outputs":[],"source":["# Y_test_GP_mean_dnnf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M_-H1KDTojCa"},"outputs":[],"source":["# np_x_train_collab_dnn.shape\n","# np_x_train_collab_dnn[:1000,:].shape\n","# train_targets[:1000]\n","# train_targets.shape\n","# data_gp_train_target_hot\n","# np.apply_along_axis(np.argmax, 1, data_gp_train_target_hot)\n","# data_gp_train_target\n","# num_of_independent_vars\n","# np_x_train_collab_dnn.shape\n","# possible_kernels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PjMVfwrK5H2s"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sy1gmVR3GSrG"},"outputs":[],"source":["def unpacking_apply_along_axis(all_args):\n","    \"\"\"\n","    Like numpy.apply_along_axis(), but with arguments in a tuple\n","    instead.\n","\n","    This function is useful with multiprocessing.Pool().map(): (1)\n","    map() only handles functions that take a single argument, and (2)\n","    this function can generally be imported from a module, as required\n","    by map().\n","    \"\"\"\n","    (func1d, axis, arr, args, kwargs) = all_args\n","    # return np.apply_along_axis(func1d, axis, arr, *args, **kwargs)\n","\n","\n","def parallel_apply_along_axis(func1d, axis, arr, *args, **kwargs):\n","    \"\"\"\n","    Like numpy.apply_along_axis(), but takes advantage of multiple\n","    cores.\n","    \"\"\"\n","    # Effective axis where apply_along_axis() will be applied by each\n","    # worker (any non-zero axis number would work, so as to allow the use\n","    # of `np.array_split()`, which is only done on axis 0):\n","    effective_axis = 1 if axis == 0 else axis\n","    if effective_axis != axis:\n","        arr = arr.swapaxes(axis, effective_axis)\n","\n","    # Chunks for the mapping (only a few chunks):\n","    chunks = [(func1d, effective_axis, sub_arr, args, kwargs)\n","              for sub_arr in np.array_split(arr, multiprocessing.cpu_count())]\n","\n","    pool = multiprocessing.Pool()\n","    individual_results = pool.map(unpacking_apply_along_axis, chunks)\n","    # Freeing the workers:\n","    pool.close()\n","    pool.join()\n","\n","    return np.concatenate(individual_results)\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["av22_gwc_hOd","UrA1SY4CHTOS","NUnTfSfY2pZK","ib862JpbN7oI","d4NWpQ7wAS1u","N3pww0F8PDy4","05H-E-Hb21S9","7NfTm6C0efVR","zarZ-ubirq5w","Qr4uwylNzQl1","nlcSr-vhOENw","pVGqX79lZ3pw","5YEfUSsWG6bm","W8fThz_isxYI","3pW2evV5b2wJ","XtjoNp7qsxYd","8GvmAzHIp7te","kIiJPc3OqE-V","-nbwCVHSRRfM","1_Os-eyfeYfE","7IhTGp4wUo7T","mIJt0f9ZVIm5","tgih_EEjVMdQ","eDq_scmJVVLP","YlyIJ3CCVmCq","upPlyiuGnrbK","DaQvakL4rU88","TXy-xmor32UY","uLw5jkNprylq","Ul_SqJXjz_s9","Zv4uI-Rf_P4R","pfPdlux63Hqa","qSwgurq54dx7","BvyzEp0EkgBD","nKZebazXuPRf","sddcE82tOFDm","FWDC54QRQhyy","hx1fELMRQhyy","io3MAwHuQhyz","cxBmUlk5Qhy0","4Z9SnOmMTKbj","yHXkywirRpmM","jh3gG7xCrTxM"],"machine_shape":"hm","name":"collab_dsl_svhn_color.ipynb","provenance":[{"file_id":"10rMDUlL1J5GiuNM-KkWdqIIK162Aa5e9","timestamp":1649347961323}],"private_outputs":true,"background_execution":"on"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}