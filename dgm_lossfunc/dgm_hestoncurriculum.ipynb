{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dgm_hestoncurriculum.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "BErSeefeQwQi",
        "bvy0WvxDGCxk",
        "HrivvbmubiiY",
        "RNhAbZ727RC_",
        "wClW1g9rbm8o",
        "65nooklCbsdy"
      ],
      "machine_shape": "hm",
      "background_execution": "on"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BErSeefeQwQi"
      },
      "source": [
        "### Setup packages "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8DGCgVxR2AB",
        "outputId": "fc57aa8c-0a93-40d0-df11-ce2fd47b63c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xIx5C6UQn4u",
        "outputId": "c743aa15-aad3-41de-b15d-8d5167b52e97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting progressbar\n",
            "  Downloading progressbar-2.5.tar.gz (10 kB)\n",
            "Building wheels for collected packages: progressbar\n",
            "  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for progressbar: filename=progressbar-2.5-py3-none-any.whl size=12082 sha256=dd2651177c72bb74d028aaf18c29cbd8dc2dae124549c50e49ec4759bedfaea6\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/fd/1f/3e35ed57e94cd8ced38dd46771f1f0f94f65fec548659ed855\n",
            "Successfully built progressbar\n",
            "Installing collected packages: progressbar\n",
            "Successfully installed progressbar-2.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: plotnine in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: matplotlib>=3.1.1 in /usr/local/lib/python3.7/dist-packages (from plotnine) (3.2.2)\n",
            "Requirement already satisfied: descartes>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.1.0)\n",
            "Requirement already satisfied: pandas>=0.25.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.3.5)\n",
            "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from plotnine) (0.5.2)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (0.10.2)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.4.1)\n",
            "Requirement already satisfied: mizani>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (1.4.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.1.1->plotnine) (4.1.1)\n",
            "Requirement already satisfied: palettable in /usr/local/lib/python3.7/dist-packages (from mizani>=0.6.0->plotnine) (3.3.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->plotnine) (2022.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.4.1->plotnine) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ipython-autotime\n",
            "  Downloading ipython_autotime-0.3.1-py2.py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipython-autotime) (5.5.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (2.6.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (5.1.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (1.0.18)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (57.4.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (1.15.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipython-autotime) (0.7.0)\n",
            "Installing collected packages: ipython-autotime\n",
            "Successfully installed ipython-autotime-0.3.1\n",
            "time: 130 µs (started: 2022-06-20 10:59:00 +00:00)\n"
          ]
        }
      ],
      "source": [
        "%pip install progressbar\n",
        "%pip install plotnine\n",
        "%pip install torch\n",
        "%pip install ipython-autotime\n",
        "%load_ext autotime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfIU_eNp3Zio",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "887a8406-874b-4e84-9661-407b88ed5c7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.7 s (started: 2022-06-20 10:59:00 +00:00)\n"
          ]
        }
      ],
      "source": [
        "from plotnine import *\n",
        "from plotnine.themes import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmUjYbArAuQT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b41352e-6f0d-4c2c-fc5f-a40ddf140434"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.6 s (started: 2022-06-20 10:59:02 +00:00)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from scipy.io import loadmat\n",
        "import random\n",
        "import math\n",
        "import tensorflow_probability as tfp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PieVKPfHHYQ6"
      },
      "source": [
        "_paper_name_ establishes the reusable name of the paper, it represents the directory under data_papers on the google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BI4p7ZKb0Qz2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc0cbb59-aaf5-4aa1-d501-2cfe581071b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 834 µs (started: 2022-06-20 10:59:04 +00:00)\n"
          ]
        }
      ],
      "source": [
        "paper_name = \"dgm_hestoncurriculum\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "433z6V3T2rB2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfd9ee95-006d-4858-fe63-48206e8b2db1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 380 ms (started: 2022-06-20 10:59:04 +00:00)\n"
          ]
        }
      ],
      "source": [
        "import os, sys\n",
        "import errno\n",
        "\n",
        "# make a directory if it does not exist\n",
        "def make_dir_if_not_exist(used_path):\n",
        "    if not os.path.isdir(used_path):\n",
        "        try:\n",
        "            os.mkdir(used_path)\n",
        "        except OSError as exc:\n",
        "            if exc.errno != errno.EEXIST:\n",
        "                raise exc\n",
        "            else:\n",
        "                raise ValueError(f'{used_path} directoy cannot be created because its parent directory does not exist.')\n",
        "\n",
        "# make directories if they do not exist\n",
        "\n",
        "make_dir_if_not_exist(\"/content/drive/MyDrive/data_papers/\")\n",
        "make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}\")\n",
        "make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features/\")\n",
        "make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/\")\n",
        "make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_history/\")\n",
        "make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/\")\n",
        "make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_predictions/\")\n",
        "make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_ccs/\")\n",
        "make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/\")\n",
        "make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/temp/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uat0pG8aR3Rh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6a4a466-0dd8-483b-9356-c34a1867c925"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 7.39 ms (started: 2022-06-20 10:59:05 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Set up the imports\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import site\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import h5py as h5\n",
        "import matplotlib.pyplot as plt\n",
        "import errno\n",
        "import numpy as np\n",
        "import itertools\n",
        "import multiprocessing\n",
        "import json\n",
        "import datetime\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "pd.set_option('display.width', 400)\n",
        "pd.set_option('display.max_columns', 40)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpFjo3MkLus9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e593d835-155c-43e8-f73e-df1c84566889"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.79 s (started: 2022-06-20 10:59:05 +00:00)\n"
          ]
        }
      ],
      "source": [
        "import torch \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from scipy.stats import norm\n",
        "from matplotlib import cm\n",
        "import pdb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbfN42gpGZhC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a270e275-b6d7-4ce2-85b0-7b7abcd33ff4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.94 s (started: 2022-06-20 10:59:08 +00:00)\n"
          ]
        }
      ],
      "source": [
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvy0WvxDGCxk"
      },
      "source": [
        "### Shared functions across models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpVaz5dwXZNq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03fdd431-4107-426c-bddc-a934cec2fcbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 20.8 ms (started: 2022-06-20 10:59:09 +00:00)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def plot_report(train_instance):\n",
        "        \n",
        "    history_tl_cpu = [ x for x in train_instance.history_tl ]\n",
        "    history_internal_cpu = [ x.cpu().detach().numpy() for x in train_instance.history_internal_cpu ]\n",
        "    history_terminal_cpu = [ x.cpu().detach().numpy() for x in train_instance.history_terminal ]\n",
        "    history_initial_cpu = [ x.cpu().detach().numpy() for x in train_instance.history_initial ]\n",
        "\n",
        "    obs_data = pd.DataFrame({\"Epochs\" : [ (x+1)*train_instance.hook_interval for x in range(len(history_initial_cpu))], \n",
        "                             \"AvgLogLoss\": np.log(history_tl_cpu), \n",
        "                             \"TerminalLogLoss\" :  np.log(history_terminal_cpu),\n",
        "                             \"InternalLogLoss\" :  np.log(history_internal_cpu),\n",
        "                             \"InitialLogLoss\" : np.log(history_initial_cpu)\n",
        "                             })\n",
        "\n",
        "    return (ggplot(obs_data, aes(\"Epochs\",\"AvgLogLoss\")) + geom_line() + geom_point(),\n",
        "            ggplot(obs_data, aes(\"Epochs\",\"TerminalLogLoss\")) + geom_line() + geom_point(),\n",
        "            ggplot(obs_data, aes(\"Epochs\",\"InternalLogLoss\")) + geom_line() + geom_point(),\n",
        "            ggplot(obs_data, aes(\"Epochs\",\"InitialLogLoss\")) + geom_line() + geom_point())\n",
        "\n",
        "def plot_activation_mean(train_instance):\n",
        "    \n",
        "    if train_instance.debug == False:\n",
        "        print( 'error: debug is off , turn it on and train again ' )\n",
        "    else:\n",
        "        history = np.array(train_instance.history_mean_hooks)\n",
        "        jet= plt.get_cmap('jet')\n",
        "        colors = iter(jet(np.linspace(0,1,10)))\n",
        "        fig, ax = plt.subplots()\n",
        "        for i in range(history.shape[1]):\n",
        "            ax.plot(history[:,i], '--r', label= i , color=next(colors) )\n",
        "        fig.suptitle('Layers activation mean value', fontsize=10)\n",
        "        leg = ax.legend();\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMAuMqdgU9kL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65bee852-6a3f-40a0-c78d-b2171954e782"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 706 µs (started: 2022-06-20 10:59:10 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# plot_report(train)\n",
        "# plot_activation_mean(train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCV-yFDXUV4J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "927872d3-117c-4157-d0d0-77989ccd7531"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.19 ms (started: 2022-06-20 10:59:10 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# print( 'Value at 0' , net( torch.tensor( [ 0. , 1. , 1. , 1. ] ).cuda() ) )\n",
        "# #%% save\n",
        "# torch.save(net.state_dict(), './model3Assets')\n",
        "# #%%\n",
        "# net = TheModelClass(*args, **kwargs)\n",
        "# net.load_state_dict(torch.load('./modelmodel3Assets'))\n",
        "# net.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONB5NopRa3fD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfc8cf3e-2781-4694-b52d-e559fb1d8a53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 33.7 ms (started: 2022-06-20 10:59:10 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# a set up that just maximizes the loss s.t. loss < eps (maximizeloss_weights_st) using the weights on the losses\n",
        "from scipy.optimize import LinearConstraint, NonlinearConstraint\n",
        "from scipy.optimize import Bounds\n",
        "from functools import partial\n",
        "from scipy.optimize import minimize\n",
        "from functools import wraps\n",
        "\n",
        "def negative(f):\n",
        "    @wraps(f)\n",
        "    def g(*args,**kwargs):\n",
        "        return - f(*args,**kwargs)\n",
        "    # g.__name__ = f'negative({f.__name__})'\n",
        "    return g\n",
        "# kl_loss = nn.KLDivLoss(size_average=None, reduction=\"batchmean\")\n",
        "\n",
        "# we can add more minimization functions here later (e.g. SS diff)\n",
        "def KLDiffHere( varX, loss_terms, log_target = False, reduction = \"mean\"):  \n",
        "  target = torch.tensor([1./len(loss_terms)]*len(loss_terms))*torch.tensor(loss_terms)\n",
        "  input = torch.tensor(varX*loss_terms)\n",
        "  loss_pointwise = target * (torch.log(target) - torch.log(input))\n",
        "  if reduction == \"mean\":  # default\n",
        "      loss = loss_pointwise.mean()\n",
        "  elif reduction == \"batchmean\":  # mathematically correct\n",
        "      loss = loss_pointwise.sum() / input.size(0)\n",
        "  elif reduction == \"sum\":\n",
        "      loss = loss_pointwise.sum()\n",
        "  else:  # reduction == \"none\"\n",
        "      loss = loss_pointwise  \n",
        "  return loss\n",
        "\n",
        "  # return torch.nn.KLDivLoss(varX*loss_terms,np.array([1./len(loss_terms)]*len(loss_terms))*loss_terms)\n",
        "\n",
        "def minimize_weights_st(loss_terms, loss_func):\n",
        "  bounds = Bounds([0]*len(loss_terms), [1.0]*len(loss_terms))\n",
        "  linear_constraint = LinearConstraint([[1]*len(loss_terms)], [1.0], [1.0])\n",
        "  x0 = [0.25]*len(loss_terms)\n",
        "  res = minimize( partial(loss_func, loss_terms=loss_terms), \n",
        "                  x0, \n",
        "                  method='trust-constr', \n",
        "                  constraints=[linear_constraint],\n",
        "                  options={'verbose': 0}, \n",
        "                  bounds=bounds )\n",
        "  return res\n",
        "\n",
        "def maximizeloss_weights_st(loss_terms, loss_func, eps):\n",
        "  bounds = Bounds([0]*len(loss_terms), [1.0]*len(loss_terms))\n",
        "  linear_constraint = LinearConstraint([[1]*len(loss_terms)], [1.0], [1.0])\n",
        "  nonlinear_constraint  = NonlinearConstraint(negative(partial(loss_func, loss_terms=loss_terms)),1E-9,eps)\n",
        "  # even though zero is the KL minimum it helps to put a negative number here to explore\n",
        "\n",
        "  x0 = [1.0/len(loss_terms)]*len(loss_terms)\n",
        "  res = minimize( negative(partial(loss_func, loss_terms=loss_terms)), \n",
        "                  x0, \n",
        "                  method='trust-constr', \n",
        "                  constraints=[linear_constraint, nonlinear_constraint],\n",
        "                  options={'verbose': 0}, \n",
        "                  bounds=bounds )\n",
        "  return res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RM0IVdZ_TXW3",
        "outputId": "d74c649a-1104-4109-b430-bd5ab3722ef0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.33334757 0.33333761 0.33331482]\n",
            "time: 81.4 ms (started: 2022-06-20 10:59:10 +00:00)\n"
          ]
        }
      ],
      "source": [
        "r1 = maximizeloss_weights_st( [ 34.25, 100.12, 23.45] , KLDiffHere, 1E9)\n",
        "print(r1.x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyacROFeXgNp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5881528-eb9b-41f3-974b-04427a7311ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 29.1 ms (started: 2022-06-20 10:59:10 +00:00)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.distributions import Normal\n",
        "\n",
        "std_norm_cdf = Normal(0, 1).cdf\n",
        "std_norm_pdf = lambda x: torch.exp(Normal(0, 1).log_prob(x))\n",
        "\n",
        "def bs_price(right, K, S, T, sigma, r):\n",
        "    d_1 = (1 / (sigma * torch.sqrt(T))) * (torch.log(S / K) + (r + (torch.square(sigma) / 2)) * T)\n",
        "    d_2 = d_1 - sigma * torch.sqrt(T)\n",
        "    \n",
        "    if right == \"C\":\n",
        "        C = std_norm_cdf(d_1) * S - std_norm_cdf(d_2) * K * torch.exp(-r * T)\n",
        "        return C\n",
        "    elif right == \"P\":\n",
        "        P = std_norm_cdf(-d_2) * K * torch.exp(-r * T) - std_norm_cdf(-d_1) * S\n",
        "        return P\n",
        "      \n",
        "def bs_delta(right, K, S, T, sigma, r):\n",
        "    d_1 = (1 / (sigma * torch.sqrt(T))) * (torch.log(S / K) + (r + (torch.square(sigma) / 2)) * T)\n",
        "    if right == \"C\":\n",
        "        return std_norm_cdf(d_1) \n",
        "    elif right == \"P\":\n",
        "        return std_norm_cdf(d_1) - 1.0\n",
        "\n",
        "def bs_gamma(K, S, T, sigma, r):\n",
        "    d_1 = (1 / (sigma * torch.sqrt(T))) * (torch.log(S / K) + (r + (torch.square(sigma) / 2)) * T)\n",
        "    return std_norm_pdf(d_1)/(S*sigma*torch.sqrt(T)) \n",
        "\n",
        "def bs_theta(right, K, S, T, sigma, r):\n",
        "    d_1 = (1 / (sigma * torch.sqrt(T))) * (torch.log(S / K) + (r + (torch.square(sigma) / 2)) * T)\n",
        "    d_2 = d_1 - sigma * torch.sqrt(T)\n",
        "    if right == \"C\":\n",
        "        theta_call = (-S*std_norm_pdf(d_1)*sigma)/(2*torch.sqrt(T))-r*K*torch.exp(-r*T)*std_norm_cdf(d_2)\n",
        "        return theta_call \n",
        "    elif right == \"P\":\n",
        "        theta_put = (-S*std_norm_pdf(d_1)*sigma)/(2*torch.sqrt(T))+r*K*torch.exp(-r*T)*std_norm_cdf(-d_2)\n",
        "        return std_norm_cdf(d_1) - 1.0        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLsA5AvqpMM7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61dfab19-c792-4323-b59e-2508ddcc2f20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.73 ms (started: 2022-06-20 10:59:10 +00:00)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "def to_cpu_detach(x):\n",
        "  if isinstance(x, list):\n",
        "    return [ y.detach().cpu().item() for y in x ]\n",
        "  else:\n",
        "    return x.detach().cpu().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PC-E2SeX46A9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bd7b3e0-e0e2-4834-f060-ff1634c54e8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.28 ms (started: 2022-06-20 10:59:10 +00:00)\n"
          ]
        }
      ],
      "source": [
        "def huber_loss_zero_target(x, delta = 1.0):\n",
        "  loss_function = torch.nn.HuberLoss(delta=delta)\n",
        "  return loss_function(x, torch.zeros_like(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNYJyHWpeL66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bfb185b-89f6-4bee-81cb-8ef8f1542910"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 21.6 ms (started: 2022-06-20 10:59:10 +00:00)\n"
          ]
        }
      ],
      "source": [
        "def save_model_train(lr, net,  eqLossFn, sample_method, trainObj, eqType, eqObject = None ):\n",
        "\n",
        "  model_id_str =  f\"{eqType}_{datetime.datetime.now():%Y%m%d%H%M%S}_{eqLossFn}_{sample_method}_{trainObj.stop_epoch}_{str(lr).replace('.','p')}_{net.NL}_{net.NN}\"\n",
        "  \n",
        "  if eqObject is not None:\n",
        "    try:\n",
        "        beta = getattr(eqObject,\"beta\")\n",
        "        beta_str = str(beta).replace('.','p')\n",
        "        model_id_str = model_id_str + f\"_beta{beta_str}\"\n",
        "    except AttributeError:\n",
        "        pass\n",
        "    try:\n",
        "        gamma = getattr(eqObject,\"gamma\")\n",
        "        gamma_str = str(gamma).replace('.','p')\n",
        "        model_id_str = model_id_str + f\"_gamma{gamma_str}\"\n",
        "    except AttributeError:\n",
        "        pass\n",
        "  \n",
        "  torch.save(net.state_dict(), f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{model_id_str}\")\n",
        "  df_at_hookintervals = None\n",
        "  train_losses = None\n",
        "  validation_losses = None\n",
        "  try:\n",
        "      df_at_hookintervals = getattr(trainObj, \"history_surfaces_hooks\")\n",
        "      if df_at_hookintervals is not None:\n",
        "        df_at_hookintervals.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/validationHook_{trainObj.hook_interval}_{model_id_str}.csv\", index=False)\n",
        "  except AttributeError:\n",
        "      print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"history_surfaces_hooks\"))\n",
        "\n",
        "  try:\n",
        "      train_losses = getattr(trainObj,\"train_losses\")\n",
        "      if train_losses is not None:\n",
        "        train_losses.tofile(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/trainlosses_{model_id_str}.csv\", sep = ',')    \n",
        "  except AttributeError:\n",
        "      print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"train_losses\"))\n",
        "      # raise NotImplementedError(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"train_losses\"))\n",
        "\n",
        "  try:\n",
        "      validation_losses = getattr(trainObj,\"validation_losses\")\n",
        "      if validation_losses is not None:\n",
        "        validation_losses.tofile(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/validationlosses_{model_id_str}.csv\", sep = ',')    \n",
        "  except AttributeError:\n",
        "      print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"validation_losses\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipogSsVTbv0k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b8867a5-6af4-4dec-a7b6-c5850010ae39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 28.4 ms (started: 2022-06-20 10:59:10 +00:00)\n"
          ]
        }
      ],
      "source": [
        "def save_model_train_stratified(lr, net,  eqLossFn, sample_method, trainObj, eqType, eqObject = None ):\n",
        "\n",
        "  model_id_str =  f\"{eqType}_{datetime.datetime.now():%Y%m%d%H%M%S}_{eqLossFn}_{sample_method}_{trainObj.stop_epoch}_{str(lr).replace('.','p')}_{net.NL}_{net.NN}\"\n",
        "  \n",
        "  if eqObject is not None:\n",
        "    try:\n",
        "        beta = getattr(eqObject,\"beta\")\n",
        "        beta_str = str(beta).replace('.','p')\n",
        "        model_id_str = model_id_str + f\"_beta{beta_str}\"\n",
        "    except AttributeError:\n",
        "        pass\n",
        "    try:\n",
        "        gamma = getattr(eqObject,\"gamma\")\n",
        "        gamma_str = str(gamma).replace('.','p')\n",
        "        model_id_str = model_id_str + f\"_gamma{gamma_str}\"\n",
        "    except AttributeError:\n",
        "        pass\n",
        "    try:\n",
        "        xbreaks = getattr(eqObject,\"xbreaks\")\n",
        "        xbreaks_str = str(len(xbreaks))\n",
        "        model_id_str = model_id_str + f\"_StSaXbrks{xbreaks_str}\"\n",
        "    except AttributeError:\n",
        "        pass\n",
        "    try:\n",
        "        tbreaks = getattr(eqObject,\"tbreaks\")\n",
        "        tbreaks_str = str(len(tbreaks))\n",
        "        model_id_str = model_id_str + f\"_StSaTbrks{tbreaks_str}\"\n",
        "    except AttributeError:\n",
        "        pass\n",
        "  \n",
        "  torch.save(net.state_dict(), f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{model_id_str}\")\n",
        "  df_at_hookintervals = None\n",
        "  train_losses = None\n",
        "  validation_losses = None\n",
        "  try:\n",
        "      df_at_hookintervals = getattr(trainObj, \"history_surfaces_hooks\")\n",
        "      if df_at_hookintervals is not None:\n",
        "        df_at_hookintervals.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/validationHook_{trainObj.hook_interval}_{model_id_str}.csv\", index=False)\n",
        "  except AttributeError:\n",
        "      print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"history_surfaces_hooks\"))\n",
        "\n",
        "  try:\n",
        "      train_losses = getattr(trainObj,\"train_losses\")\n",
        "      if train_losses is not None:\n",
        "        train_losses.tofile(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/trainlosses_{model_id_str}.csv\", sep = ',')    \n",
        "  except AttributeError:\n",
        "      print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"train_losses\"))\n",
        "      # raise NotImplementedError(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"train_losses\"))\n",
        "\n",
        "  try:\n",
        "      validation_losses = getattr(trainObj,\"validation_losses\")\n",
        "      if validation_losses is not None:\n",
        "        validation_losses.tofile(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/validationlosses_{model_id_str}.csv\", sep = ',')    \n",
        "  except AttributeError:\n",
        "      print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"validation_losses\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tz5tUJuYaXKu"
      },
      "source": [
        "### Heston options - sampling methodology\n",
        "\n",
        "\n",
        "[Heston Model in terms of log price (not used)](https://www.frouah.com/finance%20notes/The%20Heston%20model%20short%20version.pdf)\n",
        "\n",
        "\n",
        "![HestonPDEInTermsOfLogPrice.png](data:image/png;base64,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)\n",
        "\n",
        "\n",
        "\n",
        "[Heston Model in terms of price ](https://www.frouah.com/finance%20notes/The%20Heston%20model%20short%20version.pdf)\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrivvbmubiiY"
      },
      "source": [
        "#### HestonOptionNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRraqOG4aXKx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17577129-3379-4fad-e075-a5a62a24d534"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 12.9 ms (started: 2022-06-20 10:59:10 +00:00)\n"
          ]
        }
      ],
      "source": [
        "class HestonOptionNet(nn.Module):\n",
        "    def __init__(self , NL  , NN, activation = torch.tanh  ):\n",
        "        super(HestonOptionNet, self).__init__()\n",
        "        self.NL = NL\n",
        "        self.NN = NN\n",
        "        ### time, nu (vol), log of the stock Px\n",
        "        ### ( t , xi)\n",
        "        self.Input = 2 + 1\n",
        "        self.fc_input = nn.Linear(self.Input,self.NN)\n",
        "        torch.nn.init.xavier_uniform_(self.fc_input.weight)\n",
        "        self.linears = nn.ModuleList([nn.Linear(self.NN, self.NN) for i in range(self.NL)])\n",
        "        for i, l in enumerate(self.linears):    \n",
        "            torch.nn.init.xavier_uniform_(l.weight)\n",
        "        self.fc_output = nn.Linear(self.NN,1)\n",
        "        torch.nn.init.xavier_uniform_(self.fc_output.weight)\n",
        "        self.act = activation\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h = self.act( self.fc_input(x)  )\n",
        "        for i, l in enumerate(self.linears):\n",
        "            h = self.act( l(h) )\n",
        "        out = self.fc_output(h)\n",
        "        return out \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNhAbZ727RC_"
      },
      "source": [
        "#### AlternativeNet\n",
        "\n",
        "[implement from github](https://github.com/Plemeur/DGM/blob/master/first_net.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_c-dZ5b37NwV",
        "outputId": "be3c1701-0a83-42d6-f57c-cb138b796ac4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 101 ms (started: 2022-06-20 10:59:10 +00:00)\n"
          ]
        }
      ],
      "source": [
        "class LinearWithXavier(nn.Module):\n",
        "    \"\"\" Copy of linear module from Pytorch, modified to have a Xavier init,\n",
        "        TODO : figure out what to do with the bias\"\"\"\n",
        "    def __init__(self, in_features, out_features, bias=True, batch_normalize=True):\n",
        "        super(LinearWithXavier, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.batch_normalize = batch_normalize\n",
        "        \n",
        "        if self.batch_normalize == True:\n",
        "          self.batch_norm = torch.nn.BatchNorm1d(out_features)\n",
        "        \n",
        "        if bias:\n",
        "            self.bias = torch.nn.Parameter(torch.Tensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "    \n",
        "    def reset_parameters(self):\n",
        "        torch.nn.init.xavier_uniform_(self.weight)\n",
        "        if self.bias is not None:\n",
        "            torch.nn.init.uniform_(self.bias, -1, 1) #boundary matter?\n",
        "    \n",
        "    def forward(self, input):\n",
        "        if self.batch_normalize == True:\n",
        "          return self.batch_norm(torch.nn.functional.linear(input, self.weight, self.bias))\n",
        "        return torch.nn.functional.linear(input, self.weight, self.bias)\n",
        "    \n",
        "    def extra_repr(self):\n",
        "        return 'in_features={}, out_features={}, bias={}'.format(\n",
        "            self.in_features, self.out_features, self.bias is not None\n",
        "        )\n",
        "\n",
        "\n",
        "class DGM_layer(nn.Module):\n",
        "    \"\"\" See readme for paper source\"\"\"\n",
        "    def __init__(self, in_features, out_feature, residual=False, batch_normalize=False):\n",
        "        super(DGM_layer, self).__init__()\n",
        "        self.residual = residual\n",
        "\n",
        "        self.Z = LinearWithXavier(out_feature, out_feature, batch_normalize=batch_normalize)\n",
        "        self.UZ = LinearWithXavier(in_features, out_feature, bias=False, batch_normalize=batch_normalize)\n",
        "        self.G = LinearWithXavier(out_feature, out_feature, batch_normalize=batch_normalize)\n",
        "        self.UG = LinearWithXavier(in_features, out_feature, bias=False, batch_normalize=batch_normalize)\n",
        "        self.R = LinearWithXavier(out_feature, out_feature, batch_normalize=batch_normalize)\n",
        "        self.UR = LinearWithXavier(in_features, out_feature, bias=False, batch_normalize=batch_normalize)\n",
        "        self.H = LinearWithXavier(out_feature, out_feature, batch_normalize=batch_normalize)\n",
        "        self.UH = LinearWithXavier(in_features, out_feature, bias=False, batch_normalize=batch_normalize)\n",
        "\n",
        "    def forward(self, x, s):\n",
        "        z = torch.tanh(self.UZ(x) + self.Z(s))\n",
        "        g = torch.tanh(self.UG(x) + self.G(s))\n",
        "        r = torch.tanh(self.UR(x) + self.R(s))\n",
        "        h = torch.tanh(self.UH(x) + self.H(s * r))\n",
        "        return (1 - g) * h + z * s\n",
        "\n",
        "\n",
        "class AlternativeNet(nn.Module):\n",
        "\n",
        "    def __init__(self, in_size, out_size, neurons, depth):\n",
        "        super(AlternativeNet, self).__init__()\n",
        "        self.dim = in_size\n",
        "        self.input_layer = LinearWithXavier(in_size, neurons)\n",
        "        self.middle_layer = nn.ModuleList([DGM_layer(in_size, neurons) for i in range(depth)])\n",
        "        # self.middle_layer_2 = nn.ModuleList([DGM_layer(in_size, neurons, batch_normalize=False) for i in range(2)])\n",
        "        self.final_layer = LinearWithXavier(neurons, out_size, batch_normalize=False)\n",
        "\n",
        "    def forward(self, X):\n",
        "        s = torch.tanh(self.input_layer(X))\n",
        "        for i, layer in enumerate(self.middle_layer):\n",
        "            s = torch.tanh(layer(X, s))\n",
        "        \n",
        "        # for i, layer in enumerate(self.middle_layer_2):\n",
        "        #     s = torch.tanh(layer(X, s))\n",
        "        \n",
        "        # s = torch.nn.functional.gelu(self.input_layer(X))\n",
        "        # for i, layer in enumerate(self.middle_layer):\n",
        "        #     s = torch.nn.functional.elu(layer(X, s))\n",
        "        # for i, layer in enumerate(self.middle_layer):\n",
        "        #     s = torch.nn.functional.gelu(layer(X, s))\n",
        "        # for i, layer in enumerate(self.middle_layer_2):\n",
        "        #     s = torch.nn.functional.gelu(layer(X, s))\n",
        "\n",
        "        return self.final_layer(s)\n",
        "        # return torch.pow(self.final_layer(s), 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wClW1g9rbm8o"
      },
      "source": [
        "#### EuropeanHestonSingleStockCurriculum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "LBMZYQSPaXKy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0484b35-02cc-44e5-8587-c06f6ecdfbbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 855 ms (started: 2022-06-20 10:59:10 +00:00)\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "class EuropeanHestonSingleStockCurriculum():\n",
        "    \n",
        "    def __init__(self , net, is_call = True):\n",
        "\n",
        "        self.C = 0.0           \n",
        "        self.R = 0.05         # Interest Rate (Yearly)\n",
        "\n",
        "        self.BS_SIGMA = 0.25  # BS Vol of Stock\n",
        "        self.SIGMA = 0.25     # Vol of Vol (Yearly)\n",
        "        self.RU = 1.0         # stock corrolation\n",
        "        self.KAPPA = 0.25     # OU reversion coefficient\n",
        "        self.THETA = 0.15     # long term mean of Vol of Vol\n",
        "        self.RHO  = 0.75      # correlation of Vol and S\n",
        "        self.LAMBDA = 0.5 # the price of risk\n",
        "        \n",
        "        self.K = 50.0              # Strike Price \n",
        "        self.T = 1.0               # Maturation time (in YEAR)\n",
        "        self.MAX_X = self.K*3.0   # MAX price\n",
        "        self.MAX_NU = 2.0\n",
        "        ## for accept reject purpose!\n",
        "        ## free boundry problems\n",
        "        self.net = net\n",
        "        \n",
        "        self.gamma = 0.0001\n",
        "        self.beta = 0.0001\n",
        "\n",
        "        self.is_call = is_call\n",
        "        self.log_normal_dist = torch.distributions.LogNormal(self.R-self.C, self.BS_SIGMA)\n",
        "        self.log_normal_dist_5 = torch.distributions.LogNormal(self.R-self.C, self.BS_SIGMA*5.0)\n",
        "\n",
        "        self.xbreaks = None\n",
        "        self.tbreaks = None\n",
        "\n",
        "    def g(self , x):\n",
        "        # pay off function - 1 is the stock dimension, 0 is the time dimension\n",
        "        if self.is_call:\n",
        "          return torch.max( x[:,1].reshape(-1,1) - self.K , torch.zeros([len(x),1]).cuda() ) \n",
        "        else:\n",
        "          return torch.max( self.K - x[:,1].reshape(-1,1) , torch.zeros([len(x),1]).cuda() ) \n",
        "\n",
        "    def mu(self, x):\n",
        "        ## should test it! output dimension is important !\n",
        "        return (self.R-self.C)*x.reshape(-1,1)\n",
        "\n",
        "    def sigma(self , x):\n",
        "        return self.BS_SIGMA*x.reshape(-1,1)\n",
        "\n",
        "    @staticmethod\n",
        "    def to_device(x, to_cpu):\n",
        "      if to_cpu:\n",
        "        return x.cpu()\n",
        "      else:\n",
        "        return x.cuda()\n",
        "\n",
        "    def sample(self , sample_method_X = \"U\", size = 2**8, to_cpu = False ):\n",
        "        '''\n",
        "        Sampling function\n",
        "        '''\n",
        "        # 2 samples returned: internal, terminal\n",
        "        # internal, boundary, initial, terminal\n",
        "\n",
        "        if sample_method_X in [\"U\",\"U3\"]:\n",
        "            range_multiplier = 3.0 if sample_method_X == \"U3\" else 1.0\n",
        "            # internal samples\n",
        "            # x = self.to_device(torch.cat(( torch.rand([size,1])*self.T , -self.MAX_X*range_multiplier*torch.rand([size, 1])+self.MAX_X*range_multiplier) , dim = 1 ),to_cpu)\n",
        "            x_internal_values = -self.MAX_X*range_multiplier*torch.rand([size, 1])+self.MAX_X*range_multiplier\n",
        "            nu_internal_values = -self.MAX_NU*range_multiplier*torch.rand([size, 1])+self.MAX_NU*range_multiplier\n",
        "            t_internal_values = torch.rand([size,1])*self.T\n",
        "            xvt_internal = self.to_device(torch.cat(( t_internal_values , nu_internal_values, x_internal_values ) , dim = 1 ),to_cpu)\n",
        "            ### Terminal time samples\n",
        "            x_terminal_values = -self.MAX_X*range_multiplier*torch.rand([size, 1])+self.MAX_X*range_multiplier\n",
        "            nu_terminal_values = -self.MAX_NU*range_multiplier*torch.rand([size, 1])+self.MAX_NU*range_multiplier\n",
        "            t_terminal_values = torch.zeros(size, 1) + self.T\n",
        "            xvt_terminal = self.to_device(torch.cat(( t_internal_values , nu_internal_values, x_internal_values ) , dim = 1 ),to_cpu)\n",
        "            # ### initial time samples\n",
        "            # x_initial_values = self.K*torch.ones( size, 1)\n",
        "            # nu_initial_values = -self.MAX_NU*range_multiplier*torch.rand([size, 1])+self.MAX_NU*range_multiplier\n",
        "            # t_initial_values = torch.zeros(size, 1)\n",
        "            # xvt_initial = self.to_device(torch.cat( ( t_initial_values , nu_initial_values, x_initial_values ) , dim = 1 ),to_cpu)\n",
        "            return xvt_internal , xvt_terminal #, xvt_initial\n",
        "\n",
        "\n",
        "        if sample_method_X in [\"ULP\",\"ULPE3\"] :\n",
        "            range_multiplier = 3.0 if sample_method_X == \"UE3\" else 1.0\n",
        "            # internal samples\n",
        "            x_internal_values = torch.log(-self.MAX_X*range_multiplier*torch.rand([size, 1])+self.MAX_X*range_multiplier)\n",
        "            nu_internal_values = -self.MAX_NU*range_multiplier*torch.rand([size, 1])+self.MAX_NU*range_multiplier\n",
        "            t_internal_values = torch.rand([size,1])*self.T\n",
        "            xvt_internal = self.to_device(torch.cat(( t_internal_values , nu_internal_values, x_internal_values ) , dim = 1 ),to_cpu)\n",
        "            ### Terminal time samples\n",
        "            x_terminal_values = torch.log(-self.MAX_X*range_multiplier*torch.rand([size, 1])+self.MAX_X*range_multiplier)\n",
        "            nu_terminal_values = -self.MAX_NU*range_multiplier*torch.rand([size, 1])+self.MAX_NU*range_multiplier\n",
        "            t_terminal_values = torch.zeros(size, 1) + self.T\n",
        "            xvt_terminal = self.to_device(torch.cat(( t_internal_values , nu_internal_values, x_internal_values ) , dim = 1 ),to_cpu)\n",
        "            ### initial time samples\n",
        "            x_initial_values = torch.log(self.K*torch.ones( size, 1))\n",
        "            nu_initial_values = -self.MAX_NU*range_multiplier*torch.rand([size, 1])+self.MAX_NU*range_multiplier\n",
        "            t_initial_values = torch.zeros(size, 1)\n",
        "            xvt_initial = self.to_device(torch.cat( ( t_initial_values , nu_initial_values, x_initial_values ) , dim = 1 ),to_cpu)\n",
        "\n",
        "            return xvt_internal , xvt_terminal , xvt_initial\n",
        "    \n",
        "        if sample_method_X in [\"LN\", \"LN5\"]:\n",
        "            ln_dist = self.log_normal_dist_5 if sample_method_X == \"LN5\" else self.log_normal_dist\n",
        "            # internal samples\n",
        "            x_internal_values = torch.log(torch.maximum(ln_dist.sample((size,)).reshape(-1,1)*self.K,torch.Tensor([0.0])))\n",
        "            nu_internal_values = -self.MAX_NU*torch.rand([size, 1])+self.MAX_NU\n",
        "            t_internal_values = torch.rand([size,1])*self.T\n",
        "            xvt_internal = self.to_device(torch.cat(( t_internal_values , nu_internal_values, x_internal_values ) , dim = 1 ),to_cpu)\n",
        "            ### Terminal time samples\n",
        "            x_terminal_values = torch.log(torch.maximum(ln_dist.sample((size,)).reshape(-1,1)*self.K,torch.Tensor([0.0])))\n",
        "            nu_terminal_values = -self.MAX_NU*torch.rand([size, 1])+self.MAX_NU\n",
        "            t_terminal_values = torch.zeros(size, 1) + self.T\n",
        "            xvt_terminal = self.to_device(torch.cat(( t_internal_values , nu_internal_values, x_internal_values ) , dim = 1 ),to_cpu)\n",
        "            ### initial time samples\n",
        "            x_initial_values = torch.log(self.K*torch.ones( size, 1))\n",
        "            nu_initial_values = -self.MAX_NU*torch.rand([size, 1])+self.MAX_NU\n",
        "            t_initial_values = torch.zeros(size, 1)\n",
        "            xvt_initial = self.to_device(torch.cat( ( t_initial_values , nu_initial_values, x_initial_values ) , dim = 1 ),to_cpu)\n",
        "            \n",
        "            return xvt_internal , xvt_terminal , xvt_initial\n",
        "\n",
        "        raise ValueError(f\"{sample_method_X} is not a supported sampling method\")\n",
        "        \n",
        "    def criterion(self, x_internal , x_terminal , loss_transforms = [torch.square]):\n",
        "        '''\n",
        "        Loss function that helps network find solution to equation\n",
        "        '''   \n",
        "        d = torch.autograd.grad(\n",
        "            self.net(x_internal), \n",
        "            x_internal, \n",
        "            grad_outputs=torch.ones_like(self.net(x_internal)) ,\n",
        "            create_graph=True )\n",
        "        dt  = d[0][:,0].reshape(-1,1)\n",
        "        dv1 = d[0][:,1].reshape(-1,1)\n",
        "        dx1 = d[0][:,2].reshape(-1,1)\n",
        "        \n",
        "        # d2u/dxdx\n",
        "        # pdb.set_trace()\n",
        "        dx1x1 = torch.autograd.grad(dx1, \n",
        "                                    x_internal, \n",
        "                                    grad_outputs=torch.ones_like(dx1) ,\n",
        "                                    create_graph = True)[0][:,2].reshape(-1,1)\n",
        "\n",
        "        # d2u/dvdv\n",
        "        dv1v1 = torch.autograd.grad(dv1, \n",
        "                                    x_internal, \n",
        "                                    grad_outputs=torch.ones_like(dv1) ,\n",
        "                                    create_graph = True, allow_unused=True)[0][:,1].reshape(-1,1)\n",
        "\n",
        "        # d2u/dxdv\n",
        "        dx1v1 = torch.autograd.grad(dx1, \n",
        "                                    x_internal, \n",
        "                                    grad_outputs=torch.ones_like(dx1) ,\n",
        "                                    create_graph = True, allow_unused=True)[0][:,1].reshape(-1,1)\n",
        "                            \n",
        "        \n",
        "        # dx1v1 = torch.tensor(torch.autograd.functional.hessian(self.net, tuple([x_internal[:,1], x_internal[:,2]])))\n",
        "        # env_loss = loss_fn(env_outputs, env_targets)\n",
        "        # total_loss += env_loss\n",
        "        # env_grads = torch.autograd.grad(env_loss, params, retain_graph=True, create_graph=True)\n",
        "        # print( env_grads[0] )\n",
        "        # hess_params = torch.zeros_like(env_grads[0])\n",
        "        # for i in range(env_grads[0].size(0)):\n",
        "        #     for j in range(env_grads[0].size(1)):\n",
        "        #         hess_params[i, j] = torch.autograd.grad(env_grads[0][i][j], params, retain_graph=True)[0][i, j] #  <--- error here\n",
        "        # print( hess_params )\n",
        "        # exit()\n",
        "\n",
        "        if loss_transforms is None:\n",
        "          loss_transforms = [torch.square]\n",
        "        intC = None\n",
        "        terC = None\n",
        "        iniC = None\n",
        "\n",
        "        if len(x_internal) == 0:\n",
        "          # print('zero batch size for domain!')\n",
        "          intC = [ torch.tensor(0).cuda().float() for loss_transform in loss_transforms ] \n",
        "        else:\n",
        "          # pdb.set_trace()\n",
        "          # x is above the free boundary ( so immediate pay-off is positive )\n",
        "\n",
        "          # def bs_gamma(K, S, T, sigma, r):\n",
        "          bs_gammas = bs_gamma( self.K*torch.ones( x_internal.shape[0], 1).to(x_internal.device), \n",
        "                                x_internal[:,2], \n",
        "                                x_internal[:,0],\n",
        "                                # torch.sqrt(x_internal[:,1]).to(x_internal.device), \n",
        "                                self.BS_SIGMA*torch.ones( x_internal.shape[0], 1).to(x_internal.device),                                 \n",
        "                                self.R*torch.ones( x_internal.shape[0], 1).to(x_internal.device)).to(x_internal.device)\n",
        "\n",
        "          # Equation 5 in https://www.frouah.com/finance%20notes/The%20Heston%20model%20short%20version.pdf but worked out for uo/u1\n",
        "          intC_loss_untransformed = dt + 0.5*x_internal[:,1]*(x_internal[:,2]**2)*dx1x1 + \\\n",
        "                                    self.RHO*self.SIGMA*x_internal[:,1]*x_internal[:,2]*dx1v1 + \\\n",
        "                                    0.5*(self.SIGMA**2)*x_internal[:,1]*dv1v1 - \\\n",
        "                                    self.R*self.net(x_internal) + \\\n",
        "                                    self.R*x_internal[:,2]*dx1 + \\\n",
        "                                    ( self.KAPPA * (self.THETA - x_internal[:,1]) - self.LAMBDA*x_internal[:,1] )*dv1 +\\\n",
        "                                    0.5*(x_internal[:,1]-self.BS_SIGMA**2)*(x_internal[:,2]**2)*bs_gammas\n",
        "                                    # 0.5*(x_internal[:,1]-self.BS_SIGMA**2)*(x_internal[:,2]**2)*bs_gammas  # if we use the same vol in BS we will end up with zero correction... \n",
        "          \n",
        "          intC = [ loss_transform(intC_loss_untransformed) for loss_transform in loss_transforms ]\n",
        "\n",
        "        # if torch.isnan(intC[0]):\n",
        "        #   pdb.set_trace()\n",
        "        #   pass\n",
        "\n",
        "\n",
        "        # pdb.set_trace()\n",
        "        # Terminal Condition - should be equal (both in- and out of the money)\n",
        "        # terC = [ loss_transform( self.g(x_terminal) - self.net(x_terminal) ) for loss_transform in loss_transforms ]\n",
        "        # g_terminal = self.g(x_terminal).to(x_terminal.device) # self.net(x_terminal)\n",
        "        # u0_terminal = bs_price( \"C\" if self.is_call else \"P\", \n",
        "        #                         self.K*torch.ones( x_terminal.shape[0], 1).to(x_terminal.device), \n",
        "        #                         torch.exp(x_terminal[:,2]).to(x_terminal.device), \n",
        "        #                         self.T*torch.ones( x_terminal.shape[0], 1).to(x_terminal.device), \n",
        "        #                         x_terminal[:,1], \n",
        "        #                         self.R*torch.ones( x_terminal.shape[0], 1).to(x_terminal.device)).to(g_terminal.device)\n",
        "        # terC = [ loss_transform( g_terminal - u0_terminal) for loss_transform in loss_transforms ]\n",
        "        terC = [ loss_transform(self.net(x_terminal)) for loss_transform in loss_transforms ]\n",
        "\n",
        "        # Initial Condition - should be equal (both in- and out of the money)\n",
        "        # Time is time to maturity \n",
        "        # initial_px_est = self.net(x_initial).to(x_initial.device)\n",
        "        # uT_initial = bs_price(\"C\" if self.is_call else \"P\", \n",
        "        #                         self.K*torch.ones( initial_px_est.shape[0], 1).to(x_initial.device), \n",
        "        #                         self.K*torch.ones( initial_px_est.shape[0], 1).to(x_initial.device), \n",
        "        #                         self.T*torch.ones( initial_px_est.shape[0], 1).to(x_initial.device), \n",
        "        #                         x_initial[:,1], \n",
        "        #                         self.R*torch.ones( initial_px_est.shape[0], 1).to(x_initial.device)).to(initial_px_est.device)\n",
        "        # iniC = [ loss_transform( initial_px_est - uT_initial) for loss_transform in loss_transforms ]\n",
        "        # iniC = [ loss_transform(initial_px_est) for loss_transform in loss_transforms ]\n",
        "        # closed_form_initial_pxs = bs_price(\"C\" if self.is_call else \"P\", self.K, x_initial[:,1], x_initial[:,0], torch.Tensor([self.SIGMA]).to(initial_px_est.device), self.R ).to(initial_px_est.device)\n",
        "        # iniC = loss_transform( initial_px_est - closed_form_initial_pxs )\n",
        "        return  intC , terC #, iniC\n",
        "\n",
        "    def calculateLoss(self, batch_x, train = True, loss_transforms = [ torch.square ], keep_batch = False):\n",
        "        '''\n",
        "        Helper function that Sample and Calculate loss,\n",
        "        '''        \n",
        "        # pdb.set_trace()\n",
        "        x , x_terminal   = batch_x # x_initial# \n",
        "        x = Variable( x , requires_grad=True)\n",
        "        Ls = self.criterion( x , x_terminal , loss_transforms = loss_transforms ) # , x_initial\n",
        "        intC , terC  = Ls # iniC\n",
        "\n",
        "        numActive = np.sum([1 if xb.numel()>0 else 0 for xb in batch_x ])\n",
        "        return_losses = []\n",
        "        for lc in range(len(loss_transforms)):\n",
        "          if not keep_batch:\n",
        "            loss_equalWeightedByType = (1./numActive*torch.mean(intC[lc]) + 1./numActive*torch.mean(terC[lc]))\n",
        "            return_losses.append( [ loss_equalWeightedByType , \n",
        "                                    1./numActive*torch.mean(intC[lc]) , 1./numActive*torch.mean(terC[lc]),  \n",
        "                                    loss_equalWeightedByType ] )       # 1./numActive*torch.mean(iniC[lc]     \n",
        "          else:\n",
        "            return_losses.append( [intC.numpy(), terC.numpy()] ) # , iniC.numpy()\n",
        "        return return_losses\n",
        "\n",
        "    def calculateLossUsingKLMinMax(self , batch_x , train = True, loss_transforms = [ torch.square ], keep_batch = False):\n",
        "        '''\n",
        "        Helper function that Samples and Calculate loss,\n",
        "        This is adapted in that it changes the weights on the losses\n",
        "        and the distribution of sampling to maximize the loss provided \n",
        "        the KL distance of the loss is within positive constraints\n",
        "        beta represents the constraints on the weights\n",
        "        gamma represents the constraints on the sampling distribution\n",
        "        (each representing an upper bound the KL distribution)\n",
        "        '''        \n",
        "        # x , x_terminal , x_initial = self.sample(sample_method_X, size)\n",
        "        x , x_terminal , x_initial = batch_x\n",
        "        x = Variable( x, requires_grad=True)\n",
        "        Ls = self.criterion( x , x_terminal , x_initial, loss_transforms = loss_transforms)\n",
        "        intC , terC , iniC = Ls\n",
        "\n",
        "        if self.weights is None:\n",
        "          self.weights = torch.ones(1,len(Ls)).to(intC[0].device)/len(Ls)\n",
        "        # print([DOt, TCt, BCt, torch.log(DOt + TCt + BCt), 1.0/self.gamma * torch.log(DOt + TCt + BCt)])\n",
        "\n",
        "        numActive = np.sum([1 if xb.numel()>0 else 0 for xb in batch_x ])\n",
        "\n",
        "        return_losses = []\n",
        "        for lc in range(len(loss_transforms)):\n",
        "          if not keep_batch:\n",
        "            intCt = self.weights[0,0] * torch.pow((1.0/intC[lc].numel() if intC[lc].numel() > 0 else 0.0) * torch.sum( torch.exp(self.beta * intC[lc])), self.gamma/self.beta) \n",
        "            terCt = self.weights[0,1] * torch.pow((1.0/terC[lc].numel() if terC[lc].numel() > 0 else 0.0) * torch.sum( torch.exp(self.beta * terC[lc])), self.gamma/self.beta) \n",
        "            iniCt = self.weights[0,2] * torch.pow((1.0/iniC[lc].numel() if iniC[lc].numel() > 0 else 0.0) * torch.sum( torch.exp(self.beta * iniC[lc])), self.gamma/self.beta) \n",
        "            loss_equalWeightedByType = (1./numActive*torch.mean(intC[lc]) + 1./numActive*torch.mean(terC[lc]) + 1./numActive*torch.mean(iniC[lc]) )\n",
        "            transformed_loss = 1.0/self.gamma * torch.log(intCt + terCt + iniCt)\n",
        "            return_losses.append( [ transformed_loss , \n",
        "                                    1./numActive*torch.mean(intC[lc]) , 1./numActive*torch.mean(terC[lc]) , 1./numActive*torch.mean(iniC[lc]), \n",
        "                                    loss_equalWeightedByType ] )            \n",
        "          else:\n",
        "            return_losses.append( [intC[lc].numpy(), terC[lc].numpy(), iniC[lc].numpy()] )\n",
        "        return return_losses\n",
        "\n",
        "    def calculateLossKLMinMaxGamma(self , batch_x , train = True, loss_transforms = [ torch.square ], keep_batch = False):\n",
        "        '''\n",
        "        Helper function that Samples and Calculate loss,\n",
        "        This is adapted in that it changes the weights on the losses\n",
        "        and the distribution of sampling to maximize the loss provided \n",
        "        the KL distance of the loss is within positive constraints\n",
        "        beta represents the constraints on the weights\n",
        "        gamma represents the constraints on the sampling distribution\n",
        "        (each representing an upper bound the KL distribution)\n",
        "        '''        \n",
        "        # x , x_terminal , x_initial = self.sample(sample_method_X, size)\n",
        "        x , x_terminal , x_initial, x_nonzero = batch_x\n",
        "        x = Variable( x, requires_grad=True)\n",
        "        Ls = self.criterion( x , x_terminal , x_initial, loss_transforms = loss_transforms)\n",
        "        intC , terC , iniC = Ls\n",
        "\n",
        "        if self.weights is None:\n",
        "          self.weights = torch.ones(1,len(Ls)).to(intC[0].device)/len(Ls)\n",
        "        \n",
        "        # print([DOt, TCt, BCt, torch.log(DOt + TCt + BCt), 1.0/self.gamma * torch.log(DOt + TCt + BCt)])\n",
        "        numActive = np.sum([1 if xb.numel()>0 else 0 for xb in batch_x ])\n",
        "\n",
        "        return_losses = []\n",
        "        for lc in range(len(loss_transforms)):\n",
        "          if not keep_batch:\n",
        "            intCt = self.weights[0,0] * (1.0/intC[lc].numel() if intC[lc].numel() > 0 else 0.0) * torch.sum( torch.exp(self.gamma * intC[lc])) \n",
        "            terCt = self.weights[0,1] * (1.0/terC[lc].numel() if terC[lc].numel() > 0 else 0.0) * torch.sum( torch.exp(self.gamma * terC[lc])) \n",
        "            iniCt = self.weights[0,2] * (1.0/iniC[lc].numel() if iniC[lc].numel() > 0 else 0.0) * torch.sum( torch.exp(self.gamma * iniC[lc])) \n",
        "            loss_equalWeightedByType = (1./numActive*torch.mean(intC[lc]) + 1./numActive*torch.mean(terC[lc]) + 1./numActive*torch.mean(iniC[lc]) )\n",
        "            transformed_loss = 1.0/self.gamma * torch.log(intCt + terCt + iniCt)\n",
        "            return_losses.append( [ transformed_loss , \n",
        "                                    1./numActive*torch.mean(intC[lc]) , 1./numActive*torch.mean(terC[lc]) , 1./numActive*torch.mean(iniC[lc]),  \n",
        "                                    loss_equalWeightedByType ] )            \n",
        "          else:\n",
        "            return_losses.append( [intC[lc].numpy(), terC[lc].numpy(), iniC[lc].numpy()] )\n",
        "        return return_losses\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65nooklCbsdy"
      },
      "source": [
        "#### TrainEuropeanHestonSingleStockCurriculum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtO8fV7oaXK2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35df7c94-76f1-4cad-b5d8-910c9689e61f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 542 ms (started: 2022-06-20 10:59:11 +00:00)\n"
          ]
        }
      ],
      "source": [
        "class TrainEuropeanHestonSingleStockCurriculum():\n",
        "    \n",
        "    def __init__(self , net , equation , BATCH_SIZE , debug = False):\n",
        "        self.history_mean_hooks = [] \n",
        "        self.history_surfaces_hooks = None       \n",
        "        self.history_tl = []\n",
        "        self.history_internal = []\n",
        "        self.history_terminal = []\n",
        "        self.history_initial = []              \n",
        "        self.history_nonzero = []\n",
        "        self.BATCH_SIZE = BATCH_SIZE\n",
        "        self.net = net\n",
        "        self.model = equation        \n",
        "        self.debug = debug  \n",
        "        self.hook_interval = 20      \n",
        "        if self.debug == True:\n",
        "            self.hooks = {}            \n",
        "            self.get_all_layers(self.net)\n",
        "\n",
        "        self.optimizer_used = optim.Adam\n",
        "\n",
        "        self.use_early_stop = False\n",
        "        self.early_stop_patience = 10\n",
        "        self.early_stop_delta = 0.0        \n",
        "        self.best_loss = np.Inf\n",
        "        self.monitored_loss_type = \"Train_L2\"\n",
        "        self.early_stop_counter = 0\n",
        "\n",
        "        self.stop_epoch = 0\n",
        "\n",
        "        self.validation_sample = None\n",
        "        self.validation_losses = None\n",
        "        self.train_losses = None        \n",
        "        \n",
        "    def train(self , epoch , lr, eqLossFn = 'calculateLoss', sample_method_X = \"U\", key_loss_func = torch.square, huber_delta = 0.5):\n",
        "        \n",
        "        self.validation_losses = np.ones((epoch, 4*3 ), dtype='float32') * np.nan\n",
        "        self.train_losses = np.ones((epoch, 4*2 - 1  ), dtype='float32') * np.nan\n",
        "\n",
        "        optimizer = self.optimizer_used(self.net.parameters(), lr)\n",
        "        optimizer = optim.SGD(self.net.parameters(), lr)\n",
        "        # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
        "\n",
        "        loss_avg = 0.0\n",
        "        loss_calc_method = None\n",
        "        try:\n",
        "            loss_calc_method = getattr(self.model, eqLossFn)\n",
        "        except AttributeError:\n",
        "            raise NotImplementedError(\"Class `{}` does not implement `{}`\".format(self.model.__class__.__name__, eqLossFn))\n",
        "        \n",
        "        for e in range(epoch):\n",
        "            optimizer.zero_grad()\n",
        "            # pdb.set_trace()\n",
        "            sample_batch = self.model.sample(sample_method_X = sample_method_X, size=self.BATCH_SIZE)\n",
        "\n",
        "            losses_L2, losses_ABS = loss_calc_method( sample_batch, loss_transforms = [ key_loss_func, torch.abs ], keep_batch = False )\n",
        "            # pdb.set_trace()\n",
        "            loss , internal , terminal, losses_equalWeightedByType = losses_L2 # initial\n",
        "            loss_abs , internal_abs , terminal_abs , losses_equalWeightedByType_abs = losses_ABS # initial_abs\n",
        "            max_loss_L2 = torch.max(torch.tensor([internal , terminal ])) # initial\n",
        "\n",
        "            self.train_losses[e,:] = [ to_cpu_detach(loss), \n",
        "                                       to_cpu_detach(internal), to_cpu_detach(terminal), #  to_cpu_detach(initial),  \n",
        "                                       to_cpu_detach(loss_abs), \n",
        "                                       to_cpu_detach(internal_abs), to_cpu_detach(terminal_abs), #to_cpu_detach(initial_abs), \n",
        "                                       to_cpu_detach(losses_equalWeightedByType_abs) ]\n",
        "\n",
        "            if self.debug == True and (self.validation_sample is not None):\n",
        "              losses_L2_validation, losses_ABS_validation, losses_Huber_validation = \\\n",
        "                loss_calc_method( self.validation_sample, \n",
        "                                  loss_transforms = [ torch.square, torch.abs, partial(huber_loss_zero_target, delta=huber_delta) ], \n",
        "                                  keep_batch = False )\n",
        "                \n",
        "              validation_loss_list = [*to_cpu_detach(losses_L2_validation),\n",
        "                                      *to_cpu_detach(losses_ABS_validation),\n",
        "                                      *to_cpu_detach(losses_Huber_validation)]\n",
        "              self.validation_losses[e,:] = validation_loss_list\n",
        "            \n",
        "            if self.use_early_stop:\n",
        "              loss_to_check = losses_equalWeightedByType\n",
        "              if self.monitored_loss_type == \"Train_L2\":\n",
        "                pass\n",
        "              elif self.monitored_loss_type == \"Train_L1\":             \n",
        "                loss_to_check = losses_equalWeightedByType_abs\n",
        "              elif self.monitored_loss_type == \"Train_MAXL2\":             \n",
        "                loss_to_check = max_loss_L2\n",
        "\n",
        "              if loss_to_check < (self.best_loss-self.early_stop_delta):\n",
        "                self.best_loss = loss_to_check\n",
        "                self.early_stop_counter = 0\n",
        "                # print(f\"New Loss to beat for early Stop at epoch {e}, original loss: {losses_equalWeightedByType} with patience {self.early_stop_patience}\")\n",
        "              else:\n",
        "                self.early_stop_counter += 1\n",
        "              if self.early_stop_counter>=self.early_stop_patience:\n",
        "                print(f\"Early Stop at epoch {e}, {self.monitored_loss_type}: {loss_to_check} with patience {self.early_stop_patience}\")\n",
        "                break\n",
        "            \n",
        "            loss_avg = loss_avg + float(loss.item())\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            # scheduler.step(loss)\n",
        "            if (e % self.hook_interval == (self.hook_interval-1)) or e == 0:\n",
        "\n",
        "                loss_avg = loss_avg/self.hook_interval\n",
        "                print(\"Epoch {} - lr {} -  key loss: {} - eqWeighted loss: {} - L1 loss {} - Max Loss {}\".format(e , lr , loss, losses_equalWeightedByType, loss_abs, max_loss_L2 ))\n",
        "\n",
        "                # loss_avg = 0\n",
        "                ## report detailed loss ## ## puting inside no grad??? for memory optimization!\n",
        "                # tl , dl , il , bl, _ = self.model.calculateLoss( 2**6 )  # note that this is the standard loss!!\n",
        "\n",
        "                self.history_tl.append( loss_avg )\n",
        "                self.history_internal.append( internal )\n",
        "                self.history_terminal.append( terminal )\n",
        "                # self.history_initial.append( initial )\n",
        "\n",
        "                if self.debug == True and (self.validation_sample is not None):\n",
        "                    mean = []\n",
        "                    for l in self.hooks:\n",
        "                        mean.append(torch.mean( self.hooks[l] ).item())\n",
        "                    self.history_mean_hooks.append( mean )\n",
        "                    xinternal, xterminal, xinitial = self.validation_sample\n",
        "                    xinternal_res = self.model.net(xinternal).detach()\n",
        "                    xterminal_res = self.model.net(xterminal).detach()\n",
        "                    # xinitial_res = self.model.net(xinitial).detach()\n",
        "\n",
        "                    df_internal = self.create_result_df(e, xinternal, xinternal_res, \"INTERNAL\")\n",
        "                    df_terminal = self.create_result_df(e, xterminal, xterminal_res, \"TERMINAL\")\n",
        "                    # df_initial = self.create_result_df(e, xinitial, xinitial_res, \"INITIAL\")\n",
        "                    \n",
        "                    if self.history_surfaces_hooks is None:\n",
        "                      self.history_surfaces_hooks = pd.concat([df_internal, df_terminal],axis=0) # df_initial\n",
        "                    else:\n",
        "                      self.history_surfaces_hooks = pd.concat([self.history_surfaces_hooks,pd.concat([df_internal, df_terminal],axis=0) ], axis=0) # df_initial\n",
        "\n",
        "        self.stop_epoch = e\n",
        "\n",
        "    def hook_fn(self, m, i, o):\n",
        "              self.hooks[m] = o.detach()\n",
        "            \n",
        "    def get_all_layers(self, net):\n",
        "      for name, layer in net._modules.items():\n",
        "          if isinstance(layer, nn.ModuleList):\n",
        "              for n , l in layer.named_children():\n",
        "                l.register_forward_hook(self.hook_fn)\n",
        "          else:\n",
        "              # it's a non sequential. Register a hook\n",
        "              layer.register_forward_hook(self.hook_fn)\n",
        "    \n",
        "    def create_result_df(self, e, xsample, xsample_res, sample_type):\n",
        "      df_xsample = pd.DataFrame(xsample.cpu().numpy(), columns = [\"Time\", \"Var\", \"S1\"])\n",
        "      df_xsample[\"Epoch\"] = e\n",
        "      df_xsample[\"Sample\"] = sample_type\n",
        "      df_xsample[\"Result\"] = xsample_res.cpu().numpy()\n",
        "      return df_xsample\n",
        "\n",
        "\n",
        "    # def train_stratified(self , epoch , lr, \n",
        "    #                      eqLossFn = 'calculateLoss', \n",
        "    #                      sample_method_X = \"U\", \n",
        "    #                      key_loss_func = torch.square, \n",
        "    #                      huber_delta = 0.5\n",
        "    #                      ):\n",
        "        \n",
        "    #     self.validation_losses = np.ones((epoch, 6*3 - 1), dtype='float32') * np.nan\n",
        "    #     self.train_losses = np.ones((epoch, 6*2), dtype='float32') * np.nan\n",
        "\n",
        "    #     optimizer = self.optimizer_used(self.net.parameters(), lr)\n",
        "    #     # optimizer = optim.SGD(self.net.parameters(), lr)\n",
        "    #     loss_avg = 0.0\n",
        "    #     loss_calc_method = None\n",
        "    #     try:\n",
        "    #         loss_calc_method = getattr(self.model, eqLossFn)\n",
        "    #     except AttributeError:\n",
        "    #         raise NotImplementedError(\"Class `{}` does not implement `{}`\".format(self.model.__class__.__name__, eqLossFn))\n",
        "        \n",
        "    #     for e in range(epoch):\n",
        "    #         optimizer.zero_grad()\n",
        "    #         internal_xts_bts, terminal_xts_bts, initial_xts_bts, nonzero_xts_bts = self.model.sample_stratified(sample_method_X = sample_method_X, size=self.BATCH_SIZE)\n",
        "    #         validation_stratum_losses = None #np.array([])#.reshape(1,self.validation_losses.shape[1])\n",
        "    #         training_stratum_losses = None # np.array([])#.reshape(1,self.train_losses.shape[1])  \n",
        "    #         training_value_to_optimize = torch.tensor(0.0,requires_grad=True)           \n",
        "            \n",
        "    #         # pdb.set_trace()\n",
        "    #         for stratum_count in range(len(internal_xts_bts)):              \n",
        "    #           # sample_batch = (internal_xts_bts[stratum_count,:,:], \n",
        "    #           #                 terminal_xts_bts[stratum_count,:,:], \n",
        "    #           #                 initial_xts_bts[stratum_count,:,:], \n",
        "    #           #                 nonzero_xts_bts[stratum_count,:,:])  \n",
        "    #           sample_batch = (internal_xts_bts[stratum_count], \n",
        "    #                           terminal_xts_bts[stratum_count], \n",
        "    #                           initial_xts_bts[stratum_count], \n",
        "    #                           nonzero_xts_bts[stratum_count])  \n",
        "\n",
        "    #           stratum_losses_L2, stratum_losses_ABS = loss_calc_method( sample_batch, loss_transforms = [ key_loss_func, torch.abs ], keep_batch = False )\n",
        "    #           if np.isnan(stratum_losses_L2[0].detach().cpu().item()):\n",
        "    #             pdb.set_trace()\n",
        "    #             pass\n",
        "            \n",
        "    #           if training_stratum_losses is not None:\n",
        "    #             training_stratum_losses = torch.vstack([training_stratum_losses, torch.tensor([*to_cpu_detach(stratum_losses_L2), *to_cpu_detach(stratum_losses_ABS)]) ]) \n",
        "    #           else:\n",
        "    #             training_stratum_losses = torch.tensor([*stratum_losses_L2, *stratum_losses_ABS], requires_grad=False) \n",
        "\n",
        "    #           # pdb.set_trace()  \n",
        "    #           training_value_to_optimize = training_value_to_optimize + stratum_losses_L2[0]\n",
        "\n",
        "    #         # pdb.set_trace()              \n",
        "    #         training_loss_for_epoch = torch.sum(training_stratum_losses,0)\n",
        "    #         loss = training_value_to_optimize\n",
        "\n",
        "    #         loss_optimized , internal , terminal , initial, nonzero, losses_equalWeightedByType, \\\n",
        "    #         loss_abs , internal_abs , terminal_abs , initial_abs, nonzero_abs, losses_equalWeightedByType_abs = training_loss_for_epoch            \n",
        "    #         max_loss_L2 = torch.max(torch.tensor([internal , terminal , initial, nonzero]))\n",
        "\n",
        "    #         self.train_losses[e,:] = training_loss_for_epoch.detach().numpy()\n",
        "\n",
        "    #         if self.debug == True and (self.validation_sample is not None):\n",
        "    #           losses_L2_validation, losses_ABS_validation, losses_Huber_validation = \\\n",
        "    #             loss_calc_method( self.validation_sample, \n",
        "    #                               loss_transforms = [ torch.square, torch.abs, partial(huber_loss_zero_target, delta=huber_delta) ], \n",
        "    #                               keep_batch = False )\n",
        "    #           validation_loss = [*to_cpu_detach(losses_L2_validation),\n",
        "    #                                           *to_cpu_detach(losses_ABS_validation),\n",
        "    #                                           *to_cpu_detach(losses_Huber_validation)]\n",
        "    #           validation_loss = validation_loss.pop(5) # the L2 loss is duplicated at index 1                \n",
        "    #           self.validation_losses[e,:] = validation_loss\n",
        "\n",
        "    #         if self.use_early_stop:\n",
        "    #           loss_to_check = losses_equalWeightedByType\n",
        "    #           if self.monitored_loss_type == \"Train_L2\":\n",
        "    #             pass\n",
        "    #           elif self.monitored_loss_type == \"Train_L1\":             \n",
        "    #             loss_to_check = losses_equalWeightedByType_abs\n",
        "    #           elif self.monitored_loss_type == \"Train_MAXL2\":             \n",
        "    #             loss_to_check = max_loss_L2\n",
        "    #           if loss_to_check < (self.best_loss-self.early_stop_delta):\n",
        "    #             self.best_loss = loss_to_check\n",
        "    #             self.early_stop_counter = 0\n",
        "    #             # print(f\"New Loss to beat for early Stop at epoch {e}, original loss: {losses_equalWeightedByType} with patience {self.early_stop_patience}\")\n",
        "    #           else:\n",
        "    #             self.early_stop_counter += 1\n",
        "    #           if self.early_stop_counter>=self.early_stop_patience:\n",
        "    #             print(f\"Early Stop at epoch {e}, {self.monitored_loss_type}: {loss_to_check} with patience {self.early_stop_patience}\")\n",
        "    #             break\n",
        "\n",
        "    #         loss_avg = loss_avg + float(loss.item())\n",
        "    #         loss.backward()\n",
        "\n",
        "    #         optimizer.step()\n",
        "    #         if (e % self.hook_interval == (self.hook_interval-1)) or e == 0:\n",
        "    #             loss_avg = loss_avg/self.hook_interval\n",
        "    #             print(\"Epoch {} - lr {} -  key loss: {} - eqWeighted loss: {} - L1 loss {} - Max Loss {}\".format(e , lr , loss, losses_equalWeightedByType, loss_abs, max_loss_L2 ))\n",
        "    #             # loss_avg = 0\n",
        "    #             ## report detailed loss ## ## puting inside no grad??? for memory optimization!\n",
        "    #             # tl , dl , il , bl, _ = self.model.calculateLoss( 2**6 )  # note that this is the standard loss!!\n",
        "    #             self.history_tl.append( loss_avg )\n",
        "    #             self.history_internal.append( internal )\n",
        "    #             self.history_terminal.append( terminal )\n",
        "    #             self.history_initial.append( initial )\n",
        "    #             self.history_nonzero.append( nonzero )\n",
        "    #             if self.debug == True and (self.validation_sample is not None):\n",
        "    #                 mean = []\n",
        "    #                 for l in self.hooks:\n",
        "    #                     mean.append(torch.mean( self.hooks[l] ).item())\n",
        "    #                 self.history_mean_hooks.append( mean )\n",
        "    #                 xinternal, xterminal, xinitial, xnonzero = self.validation_sample\n",
        "    #                 xinternal_res = self.model.net(xinternal).detach()\n",
        "    #                 xterminal_res = self.model.net(xterminal).detach()\n",
        "    #                 xinitial_res = self.model.net(xinitial).detach()\n",
        "    #                 xnonzero_res = self.model.net(xnonzero).detach()\n",
        "\n",
        "    #                 # pdb.set_trace()\n",
        "    #                 df_internal = self.create_result_df(e, xinternal, xinternal_res, \"INTERNAL\")\n",
        "    #                 df_terminal = self.create_result_df(e, xterminal, xterminal_res, \"TERMINAL\")\n",
        "    #                 df_initial = self.create_result_df(e, xinitial, xinitial_res, \"INITIAL\")\n",
        "    #                 df_nonzero = self.create_result_df(e, xnonzero, xnonzero_res, \"NONZERO\")\n",
        "                    \n",
        "    #                 if self.history_surfaces_hooks is None:\n",
        "    #                   self.history_surfaces_hooks = pd.concat([df_internal, df_terminal, df_initial, df_nonzero],axis=0)\n",
        "    #                 else:\n",
        "    #                   self.history_surfaces_hooks = pd.concat([self.history_surfaces_hooks,pd.concat([df_internal, df_terminal, df_initial,df_nonzero],axis=0) ], axis=0)\n",
        "\n",
        "    #     self.stop_epoch = e\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyaTS0KC-hmB"
      },
      "source": [
        "#### Test Case European Call - Standard Loss, Uniform sampling , NL=3, NN=30, lr = 0.01\n",
        "\n",
        "[neurodiffeq - reference](https://github.com/NeuroDiffGym/neurodiffeq)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0Z2unNvZctp"
      },
      "outputs": [],
      "source": [
        "# torch.autograd.grad(dx1, x_internal, grad_outputs=torch.ones_like(x_internal) , create_graph = True, allow_unused=True)\n",
        "# torch.autograd.grad(dx1, x_internal, grad_outputs=torch.ones_like(x_internal) , create_graph = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYJJWtCzaXK3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "884188b6-b78b-43ea-e160-035b46689436"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 - lr 0.001 -  key loss: 1212.52783203125 - eqWeighted loss: 1212.52783203125 - L1 loss 8.306964874267578 - Max Loss 1212.4638671875\n",
            "Epoch 99 - lr 0.001 -  key loss: 153.86178588867188 - eqWeighted loss: 153.86178588867188 - L1 loss 3.089627265930176 - Max Loss 153.80235290527344\n",
            "Epoch 199 - lr 0.001 -  key loss: 128.73268127441406 - eqWeighted loss: 128.73268127441406 - L1 loss 3.016728639602661 - Max Loss 128.64413452148438\n",
            "Epoch 299 - lr 0.001 -  key loss: 148.32257080078125 - eqWeighted loss: 148.32257080078125 - L1 loss 3.332744598388672 - Max Loss 148.24676513671875\n",
            "Epoch 399 - lr 0.001 -  key loss: 221.9593505859375 - eqWeighted loss: 221.9593505859375 - L1 loss 4.195493698120117 - Max Loss 221.89053344726562\n",
            "Epoch 499 - lr 0.001 -  key loss: 691.276123046875 - eqWeighted loss: 691.276123046875 - L1 loss 7.9931254386901855 - Max Loss 691.2214965820312\n",
            "Epoch 599 - lr 0.001 -  key loss: 212.6368865966797 - eqWeighted loss: 212.6368865966797 - L1 loss 5.079169273376465 - Max Loss 212.55923461914062\n",
            "Epoch 699 - lr 0.001 -  key loss: 137.8363800048828 - eqWeighted loss: 137.8363800048828 - L1 loss 3.8094229698181152 - Max Loss 137.7714080810547\n",
            "Epoch 799 - lr 0.001 -  key loss: 127.14875793457031 - eqWeighted loss: 127.14875793457031 - L1 loss 3.189800262451172 - Max Loss 127.11039733886719\n",
            "Epoch 899 - lr 0.001 -  key loss: 87.77690124511719 - eqWeighted loss: 87.77690124511719 - L1 loss 2.7012572288513184 - Max Loss 87.71102905273438\n",
            "Epoch 999 - lr 0.001 -  key loss: 54.21970748901367 - eqWeighted loss: 54.21970748901367 - L1 loss 1.9586448669433594 - Max Loss 54.1488151550293\n",
            "Epoch 1099 - lr 0.001 -  key loss: 151.0249786376953 - eqWeighted loss: 151.0249786376953 - L1 loss 3.5350708961486816 - Max Loss 150.94281005859375\n",
            "Epoch 1199 - lr 0.001 -  key loss: 176.97994995117188 - eqWeighted loss: 176.97994995117188 - L1 loss 4.83534574508667 - Max Loss 176.90298461914062\n",
            "Epoch 1299 - lr 0.001 -  key loss: 302.4665222167969 - eqWeighted loss: 302.4665222167969 - L1 loss 5.023682594299316 - Max Loss 302.3555603027344\n",
            "Epoch 1399 - lr 0.001 -  key loss: 204.8796844482422 - eqWeighted loss: 204.8796844482422 - L1 loss 4.816695690155029 - Max Loss 204.7860107421875\n",
            "Epoch 1499 - lr 0.001 -  key loss: 294.3558654785156 - eqWeighted loss: 294.3558654785156 - L1 loss 4.761364936828613 - Max Loss 294.2814025878906\n",
            "Epoch 1599 - lr 0.001 -  key loss: 150.16696166992188 - eqWeighted loss: 150.16696166992188 - L1 loss 3.5138232707977295 - Max Loss 150.0845489501953\n",
            "Epoch 1699 - lr 0.001 -  key loss: 189.56338500976562 - eqWeighted loss: 189.56338500976562 - L1 loss 4.268545150756836 - Max Loss 189.50094604492188\n",
            "Epoch 1799 - lr 0.001 -  key loss: 129.4283905029297 - eqWeighted loss: 129.4283905029297 - L1 loss 4.004239082336426 - Max Loss 129.35032653808594\n",
            "Epoch 1899 - lr 0.001 -  key loss: 226.23834228515625 - eqWeighted loss: 226.23834228515625 - L1 loss 5.814674377441406 - Max Loss 226.1553955078125\n",
            "Epoch 1999 - lr 0.001 -  key loss: 152.9254150390625 - eqWeighted loss: 152.9254150390625 - L1 loss 3.248854875564575 - Max Loss 152.8768310546875\n",
            "Epoch 2099 - lr 0.001 -  key loss: 85.63186645507812 - eqWeighted loss: 85.63186645507812 - L1 loss 2.2618653774261475 - Max Loss 85.5697250366211\n",
            "Epoch 2199 - lr 0.001 -  key loss: 599.7794799804688 - eqWeighted loss: 599.7794799804688 - L1 loss 6.175597190856934 - Max Loss 599.7042846679688\n",
            "Epoch 2299 - lr 0.001 -  key loss: 43.20489501953125 - eqWeighted loss: 43.20489501953125 - L1 loss 2.0069773197174072 - Max Loss 43.143524169921875\n",
            "Epoch 2399 - lr 0.001 -  key loss: 287.39422607421875 - eqWeighted loss: 287.39422607421875 - L1 loss 4.342308521270752 - Max Loss 287.3421325683594\n",
            "Epoch 2499 - lr 0.001 -  key loss: 176.75592041015625 - eqWeighted loss: 176.75592041015625 - L1 loss 3.819190263748169 - Max Loss 176.64743041992188\n",
            "Epoch 2599 - lr 0.001 -  key loss: 424.07415771484375 - eqWeighted loss: 424.07415771484375 - L1 loss 5.610414505004883 - Max Loss 424.0216369628906\n",
            "Epoch 2699 - lr 0.001 -  key loss: 83.33584594726562 - eqWeighted loss: 83.33584594726562 - L1 loss 2.7841646671295166 - Max Loss 83.27482604980469\n",
            "Epoch 2799 - lr 0.001 -  key loss: 186.31520080566406 - eqWeighted loss: 186.31520080566406 - L1 loss 2.826620101928711 - Max Loss 186.25997924804688\n",
            "Epoch 2899 - lr 0.001 -  key loss: 52.36433410644531 - eqWeighted loss: 52.36433410644531 - L1 loss 1.6515734195709229 - Max Loss 52.306766510009766\n",
            "Epoch 2999 - lr 0.001 -  key loss: 74.090087890625 - eqWeighted loss: 74.090087890625 - L1 loss 2.3069376945495605 - Max Loss 74.04371643066406\n",
            "Epoch 3099 - lr 0.001 -  key loss: 205.75718688964844 - eqWeighted loss: 205.75718688964844 - L1 loss 4.379575252532959 - Max Loss 205.68768310546875\n",
            "Epoch 3199 - lr 0.001 -  key loss: 985.4131469726562 - eqWeighted loss: 985.4131469726562 - L1 loss 7.337581634521484 - Max Loss 985.3516845703125\n",
            "Epoch 3299 - lr 0.001 -  key loss: 71.845703125 - eqWeighted loss: 71.845703125 - L1 loss 2.7497715950012207 - Max Loss 71.79338073730469\n",
            "Epoch 3399 - lr 0.001 -  key loss: 281.9794006347656 - eqWeighted loss: 281.9794006347656 - L1 loss 4.630603313446045 - Max Loss 281.8857421875\n",
            "Epoch 3499 - lr 0.001 -  key loss: 101.59859466552734 - eqWeighted loss: 101.59859466552734 - L1 loss 2.111299514770508 - Max Loss 101.54375457763672\n",
            "Epoch 3599 - lr 0.001 -  key loss: 328.23541259765625 - eqWeighted loss: 328.23541259765625 - L1 loss 4.793217182159424 - Max Loss 328.197265625\n",
            "Epoch 3699 - lr 0.001 -  key loss: 1123.51171875 - eqWeighted loss: 1123.51171875 - L1 loss 12.39488697052002 - Max Loss 1123.433837890625\n",
            "Epoch 3799 - lr 0.001 -  key loss: 92.65988159179688 - eqWeighted loss: 92.65988159179688 - L1 loss 3.120192289352417 - Max Loss 92.59832763671875\n",
            "Epoch 3899 - lr 0.001 -  key loss: 24.13892364501953 - eqWeighted loss: 24.13892364501953 - L1 loss 1.6669859886169434 - Max Loss 24.029157638549805\n",
            "Epoch 3999 - lr 0.001 -  key loss: 209.22879028320312 - eqWeighted loss: 209.22879028320312 - L1 loss 4.6203413009643555 - Max Loss 209.17056274414062\n",
            "time: 1min 4s (started: 2022-06-20 11:00:53 +00:00)\n"
          ]
        }
      ],
      "source": [
        "seed = 123\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "eqLossFn = 'calculateLoss'\n",
        "sample_method= \"U\"\n",
        "lr = 0.001\n",
        "net = HestonOptionNet( NL = 5 , NN = 100 )\n",
        "# net = AlternativeNet(in_size=3, out_size=1, neurons=100, depth = 10)\n",
        "net.to(torch.device(\"cuda:0\")) \n",
        "## providing sampler with net so it can accept/reject based on net and other criterions\n",
        "heston_equation = EuropeanHestonSingleStockCurriculum(net)\n",
        "# heston_equation.xbreaks = np.linspace(0, heston_equation.MAX_X, 3).tolist()\n",
        "# heston_equation.tbreaks = np.linspace(0, heston_equation.T, 3).tolist()\n",
        "trainEuss = TrainEuropeanHestonSingleStockCurriculum( net , heston_equation, BATCH_SIZE = 2**5 , debug = True )\n",
        "trainEuss.hook_interval = 100\n",
        "# trainEuss.use_early_stop = True\n",
        "# trainEuss.early_stop_patience = 100\n",
        "# trainEuss.validation_sample = heston_equation.sample(sample_method_X=\"U\", size=2**6)\n",
        "# trainEuss.train_stratified( epoch = 4000 , lr = lr, eqLossFn = eqLossFn , sample_method_X = sample_method)\n",
        "trainEuss.train(epoch = 4000 , lr = lr, eqLossFn = eqLossFn , sample_method_X = sample_method)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint as pp\n",
        "# def bs_price(right, K, S, T, sigma, r)\n",
        "pp ( bs_price(\"C\", \n",
        "         torch.tensor(heston_equation.K), \n",
        "         torch.tensor(heston_equation.K*1.0), \n",
        "         torch.tensor(0.5), \n",
        "         torch.tensor(heston_equation.BS_SIGMA), \n",
        "         torch.tensor(heston_equation.R) ) )\n",
        "\n",
        "net_cpu = trainEuss.model.net.cpu()\n",
        "net_cpu.eval()\n",
        "net_cpu(torch.tensor([[0.5, heston_equation.BS_SIGMA*heston_equation.BS_SIGMA, heston_equation.K*1.0   ]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7Q6TNVlsHt3",
        "outputId": "ccd986cf-0915-4f7f-c7b9-025ff1c08e73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(4.1300)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3514]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 33.5 ms (started: 2022-06-20 11:02:04 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pp(heston_equation.K)\n",
        "pp(heston_equation.BS_SIGMA)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8x_HU_-uirs",
        "outputId": "6919ccd2-970a-4f82-9153-78e6b530a5ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50.0\n",
            "0.25\n",
            "time: 973 µs (started: 2022-06-20 11:02:10 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hessian Test"
      ],
      "metadata": {
        "id": "lizuF5krQfDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import torch, torchvision\n",
        "# from torch.autograd import Variable, grad\n",
        "# import torch.distributions as td\n",
        "# import math\n",
        "# from torch.optim import Adam\n",
        "# import scipy.stats\n",
        "\n",
        "# x_data = torch.randn(100)+0.0 # observed data (here sampled under H0)\n",
        "# N = x_data.shape[0] # number of observations\n",
        "# mu_null = torch.zeros(1)\n",
        "# sigma_null_hat = Variable(torch.ones(1), requires_grad=True)\n",
        "\n",
        "# def log_lik(mu, sigma):\n",
        "#   return td.Normal(loc=mu, scale=sigma).log_prob(x_data).sum()\n",
        "\n",
        "# # Find theta_null_hat by some gradient descent algorithm (in this case an closed-form expression would be trivial to obtain (see below)):\n",
        "# opt = Adam([sigma_null_hat], lr=0.01)\n",
        "# for epoch in range(2000):\n",
        "#     opt.zero_grad() # reset gradient accumulator or optimizer\n",
        "#     loss = - log_lik(mu_null, sigma_null_hat) # compute log likelihood with current value of sigma_null_hat  (= Forward pass)\n",
        "#     loss.backward() # compute gradients (= Backward pass)\n",
        "#     opt.step()      # update sigma_null_hat\n",
        "    \n",
        "# print(f'parameter fitted under null: sigma: {sigma_null_hat}, expected: {torch.sqrt((x_data**2).mean())}')\n",
        "# #> parameter fitted under null: sigma: tensor([0.9260], requires_grad=True), expected: 0.9259940385818481\n",
        "\n",
        "# theta_null_hat = (mu_null, sigma_null_hat)\n",
        "\n",
        "# U = torch.tensor(torch.autograd.functional.jacobian(log_lik, theta_null_hat)) # Jacobian (= vector of partial derivatives of log likelihood w.r.t. the parameters (of the full/alternative model)) = score\n",
        "# I = -torch.tensor(torch.autograd.functional.hessian(log_lik, theta_null_hat)) / N # estimate of the Fisher information matrix\n",
        "# S = torch.t(U) @ torch.inverse(I) @ U / N # test statistic, often named \"LM\" (as in Lagrange multiplier), would be zero at the maximum likelihood estimate\n",
        "\n",
        "# pval_score_test = 1 - scipy.stats.chi2(df = 1).cdf(S) # S asymptocially follows a chi^2 distribution with degrees of freedom equal to the number of parameters fixed under H0\n",
        "# print(f'p-value Chi^2-based score test: {pval_score_test}')\n",
        "# #> p-value Chi^2-based score test: 0.9203232752568568\n",
        "\n",
        "# # comparison with Student's t-test:\n",
        "# pval_t_test = scipy.stats.ttest_1samp(x_data, popmean = 0).pvalue\n",
        "# print(f'p-value Student\\'s t-test: {pval_t_test}')\n",
        "# #> p-value Student's t-test: 0.9209265268946605\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmKSH6f0Qhdk",
        "outputId": "fb51e95f-13c8-432d-cd88-cf6a3beeb9e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "parameter fitted under null: sigma: tensor([1.0244], requires_grad=True), expected: 1.024373173713684\n",
            "p-value Chi^2-based score test: 0.7524456862836967\n",
            "p-value Student's t-test: 0.7544246752844566\n",
            "time: 831 ms (started: 2022-05-18 13:15:33 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cpu.empty_cache()"
      ],
      "metadata": {
        "id": "GjapoHRTsjIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnnWEBsXRqlC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b76fdac-533d-4193-d6f1-9172c64301d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.47 ms (started: 2022-05-18 13:15:37 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# nseed = 123\n",
        "# torch.manual_seed(seed)\n",
        "# torch.cuda.manual_seed(seed)\n",
        "# torch.cuda.manual_seed_all(seed)\n",
        "# np.random.seed(seed)\n",
        "# random.seed(seed)\n",
        "# torch.backends.cudnn.benchmark = False\n",
        "# torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# eqLossFn= 'calculateLoss'\n",
        "# sample_method= \"U\"\n",
        "# lr = 0.001\n",
        "# net = EuropeanOptionNet( NL = 3 , NN = 30 )\n",
        "# net.to(torch.device(\"cuda:0\")) \n",
        "# ## providing sampler with net so it can accept/reject based on net and other criterions\n",
        "# bsequation = EuropeanBlackScholesSingleStock(net)\n",
        "# bsequation.xbreaks = np.linspace(0, bsequation.MAX_X, 2).tolist()\n",
        "# bsequation.tbreaks = np.linspace(0, bsequation.T, 2).tolist()\n",
        "# trainEuss = TrainEuropeanBlackScholesSingleStock( net , bsequation, BATCH_SIZE = 2**8 , debug = True )\n",
        "# trainEuss.hook_interval = 500\n",
        "# trainEuss.use_early_stop = True\n",
        "# trainEuss.early_stop_patience = 1500\n",
        "# trainEuss.validation_sample = bsequation.sample(sample_method_X=\"U\", size=2**6)\n",
        "# trainEuss.train( epoch = 4000 , lr = lr, eqLossFn = eqLossFn , sample_method_X = sample_method)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbIttfIIfm3E"
      },
      "outputs": [],
      "source": [
        "# [*to_cpu_detach(stratum_losses_L2_validation),\n",
        "#                                                 *to_cpu_detach(stratum_losses_ABS_validation),\n",
        "#                                                 *to_cpu_detach(stratum_losses_Huber_validation)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gD3NSHbNaXK3"
      },
      "outputs": [],
      "source": [
        "# df = trainEuss.history_surfaces_hooks[trainEuss.history_surfaces_hooks.Epoch == max(trainEuss.history_surfaces_hooks.Epoch)]\n",
        "# fig = px.scatter_3d(df, x='Time', y='S1', z='Result',color='Sample', width=500, height=400)\n",
        "# fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlKcd04BaXK4"
      },
      "outputs": [],
      "source": [
        "# save_model_train(lr, net,  eqLossFn, sample_method, trainEuss, \"EuCallSs\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2860YMsaXK5",
        "outputId": "400f1fa7-ee52-4d7c-d738-51b5dd7e0cbd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# testmodel = AmericanOptionNet(NL = 3, NN = 30)\n",
        "# print(os.listdir(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals\"))\n",
        "# testmodel.load_state_dict(torch.load(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/AmCallSs_20220504153012_calculateLoss_U_5558_3_30\"))\n",
        "# testmodel.forward(bsequation.sample(\"U\", 2**4)[0].cpu().detach())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    # def sample_stratified(self , sample_method_X = \"U\", size = 2**8, to_cpu = False ):\n",
        "    #   if self.xbreaks is None and self.tbreaks is None:\n",
        "    #     return self.sample(sample_method_X, size, to_cpu)\n",
        "    #   internal_strata_xts = []\n",
        "    #   terminal_strata_xts = []\n",
        "    #   initial_strata_xts = []\n",
        "    #   nonzero_strata_xts = []\n",
        "    #   if sample_method_X in [\"U\",\"UE3\"]:\n",
        "    #       range_multiplier = 3.0 if sample_method_X == \"UE3\" else 1.0\n",
        "    #       xbreaks_used = self.xbreaks[:] if self.xbreaks is not None else [0,range_multiplier*self.MAX_X]\n",
        "    #       tbreaks_used = self.tbreaks[:] if self.tbreaks is not None else [0,self.T]\n",
        "    #       if xbreaks_used[-1] < range_multiplier*self.MAX_X:\n",
        "    #         xbreaks_used.append(range_multiplier*self.MAX_X)\n",
        "    #       while xbreaks_used[0] < 0.0:\n",
        "    #         xbreaks_used.pop(0)\n",
        "    #       if not xbreaks_used:\n",
        "    #         xbreaks_used = [0,range_multiplier*self.MAX_X]\n",
        "    #       if xbreaks_used[0] > 0.0:            \n",
        "    #         xbreaks_used.insert(0, 0.0)\n",
        "    #       if tbreaks_used[-1] < self.T:\n",
        "    #         tbreaks_used.append(self.T)\n",
        "    #       xbreaks_range = xbreaks_used[-1]-xbreaks_used[0]\n",
        "    #       tbreaks_range = tbreaks_used[-1]-tbreaks_used[0]\n",
        "    #       total_strat_processed = 0\n",
        "    #       for stratum_x_count in range(len(xbreaks_used)-1):\n",
        "    #         num_samples_in_stratum = 0\n",
        "    #         if len(xbreaks_used) > 2:  # x division takes priority so assign it if there is no T division\n",
        "    #           range_ratio_x_stratum = (xbreaks_used[stratum_x_count+1]-xbreaks_used[stratum_x_count])/xbreaks_range\n",
        "    #           num_samples_in_stratum = math.ceil(range_ratio_x_stratum*size)\n",
        "    #         for stratum_t_count in range(len(self.tbreaks)-1):\n",
        "    #           if num_samples_in_stratum == 0: # there is only a T division, so use it\n",
        "    #             range_ratio_t_stratum = (tbreaks_used[stratum_t_count+1]-tbreaks_used[stratum_t_count])/tbreaks_range\n",
        "    #             num_samples_in_stratum = math.ceil(range_ratio_t_stratum*size)\n",
        "    #           else:\n",
        "    #             # there is both an X and a T division, assign the number of samples uniformly, assuming same scale of X and T\n",
        "    #             stratum_coverage_on_unit_square = \\\n",
        "    #               ((xbreaks_used[stratum_x_count+1]-xbreaks_used[stratum_x_count])/xbreaks_range)*\\\n",
        "    #               ((tbreaks_used[stratum_t_count+1]-tbreaks_used[stratum_t_count])/tbreaks_range)\n",
        "    #             num_samples_in_stratum = math.ceil(stratum_coverage_on_unit_square * size)\n",
        "    #           ### internal samples\n",
        "    #           internal_stratum_t_sample = self.to_device(tbreaks_used[stratum_t_count] + torch.rand([num_samples_in_stratum,1])*(tbreaks_used[stratum_t_count+1]-tbreaks_used[stratum_t_count]), to_cpu)\n",
        "    #           internal_stratum_x_sample = self.to_device(xbreaks_used[stratum_x_count] + torch.rand([num_samples_in_stratum,1])*(xbreaks_used[stratum_x_count+1]-xbreaks_used[stratum_x_count]), to_cpu)\n",
        "    #           internal_stratum_xt = torch.cat(( internal_stratum_t_sample , internal_stratum_x_sample) , dim = 1 )\n",
        "    #           if internal_stratum_xt.numel()<1:\n",
        "    #             # pdb.set_trace()\n",
        "    #             pass\n",
        "    #           if not internal_strata_xts: #.numel()<1:\n",
        "    #             internal_strata_xts = [ internal_stratum_xt ] # internal_stratum_xt[None,:,:]\n",
        "    #           else:\n",
        "    #             internal_strata_xts.append(internal_stratum_xt)  # torch.vstack((internal_strata_xts,internal_stratum_xt[None,:,: ]))\n",
        "\n",
        "    #           ### Terminal time samples\n",
        "    #           terminal_stratum_x_sample = xbreaks_used[stratum_x_count] + torch.rand([num_samples_in_stratum,1])*(xbreaks_used[stratum_x_count+1]-xbreaks_used[stratum_x_count])\n",
        "    #           terminal_stratum_xt = self.to_device(torch.cat( ( torch.zeros(num_samples_in_stratum, 1) + self.T , terminal_stratum_x_sample ) , dim = 1 ),to_cpu)\n",
        "\n",
        "    #           if not terminal_strata_xts:\n",
        "    #             terminal_strata_xts = [ terminal_stratum_xt ] # terminal_stratum_xt[None,:,:]\n",
        "    #           else:\n",
        "    #             terminal_strata_xts.append(terminal_stratum_xt) # torch.vstack((terminal_strata_xts,terminal_stratum_xt[None,:,: ]))\n",
        "\n",
        "    #           ### initial time samples\n",
        "    #           # x_initial = torch.cat( ( torch.zeros(size, 1), -self.MAX_X*range_multiplier*torch.rand([size, 1])+self.MAX_X*range_multiplier ) , dim = 1 ).cuda()\n",
        "    #           ### initial time samples\n",
        "    #           initial_stratum_xt = self.to_device(torch.cat( ( torch.zeros(num_samples_in_stratum, 1), self.K*torch.ones( num_samples_in_stratum, 1)) , dim = 1 ),to_cpu)\n",
        "    #           if not initial_strata_xts: #.numel()<1:\n",
        "    #             initial_strata_xts = [initial_stratum_xt] # initial_stratum_xt[None,:,:]\n",
        "    #           else:\n",
        "    #             initial_strata_xts.append(initial_stratum_xt) # torch.vstack((initial_strata_xts,initial_stratum_xt[None,:,: ]))\n",
        "\n",
        "    #           ### non-zero value samples\n",
        "    #           stratum_mapped_stock_space = None\n",
        "    #           if self.is_call:\n",
        "    #             stratum_mapped_stock_space = self.K*(1.0/xbreaks_used[-1])* np.array([ xbreaks_used[stratum_x_count], xbreaks_used[stratum_x_count+1]])\n",
        "    #           else:\n",
        "    #             stratum_mapped_stock_space = self.K + self.K*(1.0/xbreaks_used[-1])*np.array([ xbreaks_used[stratum_x_count], xbreaks_used[stratum_x_count+1]])              \n",
        "    #           nonzero_stratum_x_sample = stratum_mapped_stock_space[0] + torch.rand([num_samples_in_stratum, 1])*(stratum_mapped_stock_space[1]-stratum_mapped_stock_space[0])\n",
        "    #           nonzero_stratum_xt = self.to_device(torch.cat( ( torch.rand([num_samples_in_stratum,1])*self.T, nonzero_stratum_x_sample ) , dim = 1 ),to_cpu)\n",
        "    #           compare = self.net(nonzero_stratum_xt) \n",
        "    #           mask = compare < 0\n",
        "    #           nonzero_stratum_xt = nonzero_stratum_xt[mask.reshape(-1),:]\n",
        "    #           if not nonzero_strata_xts: #.numel()<1:\n",
        "    #             nonzero_strata_xts = [nonzero_stratum_xt] #nonzero_stratum_xt[None,:,:]\n",
        "    #           else:\n",
        "    #             # pdb.set_trace()\n",
        "    #             nonzero_strata_xts.append(nonzero_stratum_xt) # torch.vstack((nonzero_strata_xts,nonzero_stratum_xt[None,:,: ]))\n",
        "    #           total_strat_processed += 1 \n",
        "    #       return internal_strata_xts , terminal_strata_xts , initial_strata_xts, nonzero_strata_xts\n",
        "    #   raise ValueError(f\"{sample_method_X} is not a supported sampling method\")"
      ],
      "metadata": {
        "id": "ZJC_j8bnQBq1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}